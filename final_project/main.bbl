\begin{thebibliography}{1}

\bibitem{dai2024racer}
Yinpei Dai, Jayjun Lee, Nima Fazeli, and Joyce Chai.
\newblock {RACER}: Rich language-guided failure recovery policies for imitation learning.
\newblock {\em arXiv preprint arXiv:2409.14674}, 2024.

\bibitem{duan2024aha}
Jiafei Duan, Wentao Pumacay, Nishanth Kumar, Yi~Ru Wang, Sichun Tian, Wenhao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, and Yuke Guo.
\newblock {AHA}: A vision-language-model for detecting and reasoning over failures in robotic manipulation.
\newblock {\em arXiv preprint arXiv:2410.00371}, 2024.

\bibitem{ncutpytorch}
Ningze~Zhong Huzheng~Yang.
\newblock Nystr{\"o}m normalized cuts pytorch documentation.
\newblock \url{https://ncut-pytorch.readthedocs.io/en/latest/}, 2025.
\newblock Accessed: December 2025.

\bibitem{kim2024openvla}
Moo~Jin Kim et~al.
\newblock {OpenVLA}: An open-source vision-language-action model.
\newblock {\em arXiv preprint arXiv:2406.09246}, 2024.

\bibitem{physicalintelligence2025pi05}
{Physical Intelligence} et~al.
\newblock pi0.5: a vision-language-action model with open-world generalization.
\newblock {\em arXiv preprint arXiv:2504.16054}, 2025.

\bibitem{shi2000normalized}
Jianbo Shi and Jitendra Malik.
\newblock Normalized cuts and image segmentation.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 22(8):888--905, 2000.

\bibitem{wang2019multimodal}
Tao Wang, Changliang Yang, Frank Kirchner, Peng Du, Fuchun Sun, and Bin Fang.
\newblock Multimodal grasp data set: A novel visual-tactile data set for robotic manipulation.
\newblock {\em International Journal of Advanced Robotic Systems}, 2019.

\end{thebibliography}
