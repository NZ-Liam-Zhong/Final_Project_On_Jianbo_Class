\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[english]{babel}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{natbib}

\title{\textbf{Unveiling the Feature Geometry of Robot Foundation Models\\via Nyström NCut}}

\author{
    Ningze Zhong, Kyle Zhang, Ily Rafaeli \\
    University of Pennsylvania \\
    \texttt{\{zhong666, kyle100, ilyr\}@seas.upenn.edu}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Vision-Language-Action (VLA) models have emerged as powerful robot foundation models, yet their internal representations remain largely opaque. We apply Nyström Normalized Cuts to visualize and analyze feature geometry across VLA hidden layers, revealing how these models process multimodal inputs during manipulation tasks. Our analysis uncovers strong object-centric semantic clustering and sensitivity to failure indicators, but also exposes limitations in spatial reasoning and language grounding. Through controlled experiments on OpenVLA, we demonstrate that visual features dominate over language by a factor of 10$\times$, and that spatial information fails to generalize despite perfect training-set memorization. These findings provide actionable insights for debugging and improving robot foundation models.
\end{abstract}

\section{Introduction}

Robot foundation models built on vision-language architectures \cite{kim2024openvla,physicalintelligence2025pi05} can follow natural language instructions for diverse manipulation tasks, from tool use to contact-rich assembly. Despite impressive empirical performance, these models remain black boxes. When a VLA fails to grasp an object or misinterprets an instruction, practitioners cannot easily diagnose whether the failure stems from poor visual encoding, weak language understanding, or faulty cross-modal fusion.

We propose using Nyström Normalized Cuts \cite{shi2000normalized} as a visual debugging tool for VLAs. NCut performs spectral clustering on neural feature representations, mapping tokens to colors based on their position in the graph Laplacian's eigenspace. Our key hypothesis is that well-separated clusters indicate learnable structure: if NCut easily partitions features by semantic categories, downstream layers can exploit this structure for action prediction.

We analyze OpenVLA \cite{kim2024openvla} across multiple dimensions. First, we examine text-image semantics by clustering vision and language tokens jointly, revealing which objects the model attends to and how language anchors to visual regions. Second, we track temporal dynamics through manipulation sequences, identifying cluster transitions that correlate with task phases. Third, we quantify input sensitivity by perturbing images versus text and measuring feature-space shifts. Finally, we probe spatial intelligence by attempting to predict camera poses from VLA embeddings.

Our findings are sobering. While VLAs recognize task-relevant objects and detect failure cues like tilting or slippage, they prioritize vision over language and encode spatial information in a non-generalizable manner. We validate these insights through simulation experiments where NCut-guided debugging improves pick-and-place success rates from 71\% to 80\%.

\section{Related Work}

\textbf{Vision-Language-Action Models.} Recent VLAs \cite{kim2024openvla,physicalintelligence2025pi05} fine-tune large vision-language models on robot trajectories, inheriting broad priors from internet-scale pretraining. These models can generalize to novel objects and scenes, but lack interpretability. Prior work on failure detection \cite{dai2024racer,duan2024aha} uses separate models to identify errors post-hoc; we instead analyze internal representations to understand failure modes.

\textbf{Neural Network Interpretability.} Attention visualization and gradient-based saliency are standard tools, but provide only coarse spatial attributions. Spectral methods like NCut offer complementary insights by clustering features based on global affinity structure rather than local gradients.

\section{Method}

\subsection{Nyström Normalized Cuts}

Given features $\mathbf{F} \in \mathbb{R}^{N \times d}$ extracted from a batch of $B$ images with $N = B \cdot h \cdot w$ total tokens, we construct an affinity matrix $\mathbf{W}$ where $W_{ij} = \exp(-\|\mathbf{f}_i - \mathbf{f}_j\|^2 / 2\sigma^2)$. NCut seeks a partition minimizing
\begin{equation}
\text{NCut}(\mathcal{A}, \mathcal{B}) = \frac{\text{cut}(\mathcal{A}, \mathcal{B})}{\text{vol}(\mathcal{A})} + \frac{\text{cut}(\mathcal{A}, \mathcal{B})}{\text{vol}(\mathcal{B})},
\end{equation}
where $\text{cut}(\mathcal{A}, \mathcal{B}) = \sum_{i \in \mathcal{A}, j \in \mathcal{B}} W_{ij}$ and $\text{vol}(\mathcal{A}) = \sum_{i \in \mathcal{A}, j} W_{ij}$. This relaxes to solving $(\mathbf{D} - \mathbf{W})\mathbf{v} = \lambda \mathbf{D}\mathbf{v}$ for the smallest eigenvectors of the normalized Laplacian.

For scalability, we use Nyström approximation: sample $m \ll N$ landmarks, compute the $m \times m$ affinity submatrix and $m \times N$ cross-affinities, then approximate the full eigendecomposition. We project the resulting eigenvectors to RGB via t-SNE for visualization.

\subsection{Debugging Protocol}

Our analysis follows three steps. \textbf{(1) Controlled batches:} We curate image sets that isolate specific factors (e.g., different finger counts to test counting ability). \textbf{(2) Layer-wise inspection:} We run NCut on each layer's activations. If colors shift for perturbed samples but remain stable for others, the model is sensitive to that perturbation at that layer. \textbf{(3) Submodule tracing:} For multi-stage architectures, we separately analyze the vision encoder and vision-language fusion layers to localize failures.

\subsection{Feature Extraction from VLAs}

VLA models consist of a vision encoder (typically a pretrained ViT) that outputs patch tokens, and a language model backbone with cross-attention layers where text tokens attend to image patches. We extract features after each transformer block using forward hooks. For temporal analysis, we process video frames sequentially and concatenate features across time. For language-vision alignment, we jointly cluster image patches and text tokens, then inspect whether semantically related tokens receive similar colors.

\section{Experiments}

\subsection{Dataset}

We use a pick-and-place dataset \cite{wang2019multimodal} containing both successful and failed manipulation attempts. While informal and noisy, this diversity helps us study edge cases. We manually label success/failure, extract keyframes (initial state, approach, grasp, transport, placement), and generate natural language prompts like ``pick up the red can and place it on the tray.''

\subsection{Text-Image Semantics}

\textbf{Object recognition.} NCut applied to OpenVLA's vision encoder (layer 4) produces distinct color clusters for target objects, the robot gripper, and background. Across lighting and viewpoint changes, objects maintain consistent colors, indicating viewpoint-invariant representations.

\textbf{Language grounding.} When we jointly cluster vision and language tokens from the fusion layers, noun phrases align spatially with their visual referents. For example, ``Pringles can'' tokens cluster near the corresponding image region. Verbs like ``pick'' and ``place'' also correlate with gripper and object regions, though more weakly.

\textbf{Failure sensitivity.} We observe color shifts when objects tilt or fall. The model's representation changes rapidly when the gripper slips, suggesting implicit awareness of failure cues even without explicit supervision.

\subsection{Temporal Dynamics}

Tracking cluster assignments across manipulation sequences reveals phase structure. During successful grasps, the object cluster remains stable. Failed attempts show higher volatility, with frequent reassignments indicating representational uncertainty. Plotting cluster distribution over time yields distinct signatures for approach, contact, and release phases.

\subsection{Input Sensitivity}

To quantify modality importance, we perturb images (changing lighting, object appearance) and text (swapping verbs or nouns), then measure cosine distance in feature space. Image perturbations induce changes $\Delta_\text{img}$, verb swaps induce $\Delta_\text{verb}$, and noun swaps induce $\Delta_\text{noun}$. We find $\Delta_\text{img} > 10 \cdot \Delta_\text{verb} > \Delta_\text{noun}$, indicating vision dominates.

We also compute mean Average Precision (mAP) for template-based retrieval. High-mAP nouns include ``room'' (0.36) and ``street'' (0.29); high-mAP verbs include ``park'' (0.45) and ``sit'' (0.38). Action verbs like ``fill'' (0.07) score poorly, suggesting weak action semantics.

\subsection{Spatial Intelligence}

We train a 3-layer MLP to regress 6-DOF camera poses from VLA features on NeRF datasets. Training loss reaches 0.003, but test loss remains at 0.998. Multi-dimensional scaling (MDS) on feature distances versus pose distances shows no correlation. This suggests VLAs memorize appearance-pose pairs without learning geometric priors.

\subsection{Gradient Flow}

Computing $\partial \mathcal{L} / \partial \mathbf{f}_i$ for action loss $\mathcal{L}$ reveals that gradients concentrate on object and gripper clusters, with near-zero attribution to background. Examining eigenvector importance shows that coarse partitions (low-frequency eigenvectors) drive action predictions more than fine-grained texture.

\subsection{Simulation Validation}

We deploy OpenVLA in a simulated tabletop environment with pick-and-place tasks. Baseline success rate is 71\%. Applying NCut to failure cases reveals fragmented object clusters (poor segmentation) and premature gripper-obstacle overlap. We augment training data to address these issues, raising success to 80\%.

\section{Bonus: NCut PyTorch Documentation}

As part of this project, we developed and maintain the official NCut PyTorch documentation at \url{https://ncut-pytorch.readthedocs.io} \cite{ncutpytorch}. The site provides tutorials, API references, and a gallery of applications spanning vision transformers, language models, and diffusion models. Built with Sphinx and hosted on ReadTheDocs, the documentation serves as both a research tool and an educational resource for interpretable machine learning.

\section{Discussion}

Our findings highlight both strengths and limitations of current VLAs. Strong object recognition and failure sensitivity suggest these models learn useful mid-level semantics. However, the 10$\times$ dominance of vision over language raises concerns about instruction following: can VLAs truly ground fine-grained linguistic distinctions, or do they mostly rely on visual priors?

The spatial intelligence failure is particularly troubling. VLAs memorize pose-appearance mappings but do not learn geometric relationships, limiting their ability to reason about occlusions, insertion, or multi-step assembly. Future work should explore geometric inductive biases or 3D-aware representations.

We plan to extend this analysis to OpenPi 0.5 \cite{physicalintelligence2025pi05} and investigate whether temporal NCut patterns align with instruction structure in multi-step tasks. Integrating NCut-based early warning signals with failure recovery systems \cite{dai2024racer,duan2024aha} could enable more robust deployment.

\section{Conclusion}

Nyström NCut provides a practical tool for debugging robot foundation models. By visualizing feature geometry, we uncover semantic structure, diagnose failure modes, and identify architectural weaknesses. Our analysis reveals that VLAs excel at object-centric reasoning but struggle with language grounding and spatial generalization. As these models scale, interpretability will be essential for safe and reliable real-world deployment.

\bibliographystyle{plain}
\bibliography{sample}

\end{document}
