\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[english]{babel}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{natbib}
\usepackage{hyperref}

\title{\textbf{Unveiling the Feature Geometry of Robot Foundation Models\\via Nyström NCut}}

\author{
    Ningze Zhong, Kyle Zhang, Ily Rafaeli \\
    University of Pennsylvania \\
    \texttt{\{zhong666, kyle100, ilyr\}@seas.upenn.edu}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Vision-Language-Action (VLA) models have emerged as powerful robot foundation models, yet their internal representations remain largely opaque. We apply Nyström Normalized Cuts to visualize and analyze feature geometry across VLA hidden layers, revealing how these models process multimodal inputs during manipulation tasks. Our analysis uncovers strong object-centric semantic clustering and sensitivity to failure indicators, but also exposes critical limitations in spatial reasoning and language grounding. Through controlled experiments on OpenVLA, we demonstrate that visual features dominate over language by a factor of 10$\times$, and that spatial information fails to generalize despite perfect training-set memorization. These findings provide actionable insights for debugging and improving robot foundation models.
\end{abstract}

\section{Introduction}

Robot foundation models built on vision-language architectures \cite{kim2024openvla,physicalintelligence2025pi05} have demonstrated remarkable capabilities in following natural language instructions for diverse manipulation tasks, from tool use to contact-rich assembly. Despite their impressive empirical performance, these models remain black boxes. When a VLA fails to grasp an object or misinterprets an instruction, practitioners cannot easily diagnose whether the failure stems from poor visual encoding, weak language understanding, or faulty cross-modal fusion. This opacity poses significant challenges for debugging, model improvement, and safe deployment in real-world scenarios.

We propose using Nyström Normalized Cuts \cite{shi2000normalized} as a visual debugging tool for analyzing VLA internal representations. NCut performs spectral clustering on neural feature representations, mapping tokens to colors based on their position in the graph Laplacian's eigenspace. Our central hypothesis is that well-separated clusters indicate learnable structure: if NCut easily partitions features by semantic categories, downstream layers can exploit this structure for robust action prediction. This assumption has proven effective for debugging vision models like LLaVA, where we can identify exactly which layers fail to preserve task-relevant information by observing cluster coherence across controlled perturbations.

We conduct a comprehensive analysis of OpenVLA \cite{kim2024openvla} across multiple dimensions. First, we examine text-image semantics by clustering vision and language tokens jointly, revealing which objects the model attends to and how language anchors to visual regions. Second, we track temporal dynamics through manipulation sequences, identifying cluster transitions that correlate with distinct task phases like approach, grasp, and release. Third, we quantify input sensitivity by systematically perturbing images versus text and measuring the resulting feature-space shifts. Fourth, we probe spatial intelligence by training probes to predict camera poses from VLA embeddings. Finally, we trace gradient flow to understand which feature clusters drive action predictions.

Our findings reveal both strengths and critical weaknesses. While VLAs recognize task-relevant objects and implicitly detect failure cues like object tilting or gripper slippage, they exhibit a strong bias toward visual information over language, with image perturbations producing 10$\times$ larger representation changes than text modifications. Moreover, spatial information appears to be memorized rather than learned: VLAs achieve near-zero training error on pose regression but fail completely on held-out viewpoints, suggesting they encode appearance-pose pairs without geometric understanding. We validate these insights through simulation experiments where NCut-guided debugging improves pick-and-place success rates from 71\% to 80\%.

\section{Related Work}

\textbf{Vision-Language-Action Models.} Recent VLA architectures \cite{kim2024openvla,physicalintelligence2025pi05} fine-tune large vision-language models on robot trajectory data, leveraging broad priors from internet-scale pretraining to generalize across novel objects, scenes, and tasks. OpenVLA combines a pretrained vision transformer with a language model backbone featuring cross-attention layers, enabling end-to-end training from pixels and text to actions. OpenPi 0.5 extends this paradigm with improved data scaling and architectural refinements. While these models achieve strong empirical results, they lack built-in interpretability mechanisms. Prior work on failure detection \cite{dai2024racer,duan2024aha} trains separate models to identify errors post-hoc; in contrast, we analyze internal representations to understand inherent failure modes and guide targeted improvements.

\textbf{Neural Network Interpretability.} Attention visualization and gradient-based saliency maps are standard interpretability tools, but they provide only coarse spatial attributions and struggle with transformer architectures where information flows through complex residual paths. Spectral clustering methods like Normalized Cuts offer complementary insights by partitioning features based on global affinity structure rather than local gradient signals. Recent work has applied NCut to vision transformers for understanding semantic emergence across layers, but this approach has not been systematically extended to multimodal robot learning systems.

\section{Method}

\subsection{Nyström Normalized Cuts}

Given features $\mathbf{F} \in \mathbb{R}^{N \times d}$ extracted from a batch of $B$ images with $N = B \cdot h \cdot w$ total patch tokens, we construct an affinity matrix $\mathbf{W}$ where $W_{ij} = \exp(-\|\mathbf{f}_i - \mathbf{f}_j\|^2 / 2\sigma^2)$ captures pairwise token similarity. Normalized Cuts seeks a graph partition that minimizes the criterion
\begin{equation}
\text{NCut}(\mathcal{A}, \mathcal{B}) = \frac{\text{cut}(\mathcal{A}, \mathcal{B})}{\text{vol}(\mathcal{A})} + \frac{\text{cut}(\mathcal{A}, \mathcal{B})}{\text{vol}(\mathcal{B})},
\end{equation}
where $\text{cut}(\mathcal{A}, \mathcal{B}) = \sum_{i \in \mathcal{A}, j \in \mathcal{B}} W_{ij}$ measures edge weight between partitions and $\text{vol}(\mathcal{A}) = \sum_{i \in \mathcal{A}, j} W_{ij}$ is the total connection strength from partition $\mathcal{A}$ to all nodes. This optimization relaxes to solving the generalized eigenvalue problem $(\mathbf{D} - \mathbf{W})\mathbf{v} = \lambda \mathbf{D}\mathbf{v}$ for the smallest eigenvectors of the normalized graph Laplacian, where $\mathbf{D}$ is the diagonal degree matrix.

For computational tractability on large feature sets, we employ the Nyström approximation. Rather than computing the full $N \times N$ affinity matrix, we randomly sample $m \ll N$ landmark points and compute only the $m \times m$ landmark-to-landmark affinities and $m \times N$ landmark-to-all-points cross-affinities. The full eigendecomposition is then approximated via this low-rank representation, reducing complexity from $O(N^2)$ to $O(Nm)$. We project the resulting high-dimensional eigenvectors to RGB color space via t-SNE, enabling intuitive visualization where tokens with similar colors belong to the same semantic cluster.

\subsection{Debugging Protocol}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{feature_debugging.png} 
    \caption{This is an example of feature debugging with LLAVA. Details in \url{https://github.com/NZ-Liam-Zhong/NCut_for_Visual_Debugging}}
\end{figure}

Our analysis methodology consists of three systematic steps. First, we curate controlled image batches that isolate specific factors of interest. For example, to test a VLA's ability to distinguish finger counts, we collect hands displaying one through five fingers and process them jointly, then inspect whether cluster colors vary consistently with the ground-truth count. Second, we perform layer-wise inspection by applying NCut to activations from each transformer block. If a small perturbation in one sample causes color shifts only in that case while colors remain stable across unperturbed samples, the model exhibits high sensitivity to that factor at that layer. Conversely, if colors remain stable despite controlled variations, the model has not learned to discriminate that semantic attribute. Third, we trace failures across architectural submodules. For VLAs with separate vision encoders and vision-language fusion layers, we analyze each component independently to localize whether failures originate in visual feature extraction, language encoding, or cross-modal alignment.

\subsection{Feature Extraction from VLAs}

VLA architectures typically consist of a vision encoder (often a pretrained ViT) that processes images into spatial patch tokens, and a language model backbone with interleaved cross-attention layers where text tokens attend to image patches. We extract intermediate representations by registering forward hooks on transformer blocks. For temporal analysis of manipulation videos, we process frames sequentially and concatenate patch features across time, yielding a 4D tensor that NCut partitions into spatio-temporal clusters. For language-vision alignment studies, we jointly cluster image patches and text tokens from fusion layers, then examine whether semantically related tokens (e.g., the word ``can'' and the visual region depicting a can) receive similar cluster assignments. We also extract action prediction gradients to identify which feature clusters most strongly influence output decisions.

\section{Experiments and Results}

\subsection{Dataset and Experimental Setup}

We use a pick-and-place manipulation dataset \cite{wang2019multimodal} containing both successful and failed robot execution attempts. While this dataset is informal and contains some labeling noise, its diversity makes it valuable for studying edge cases and failure modes that are often filtered from cleaner benchmarks. We manually annotate task outcomes as success or failure, extract keyframes corresponding to critical manipulation phases (initial state, approach, grasp, lift, transport, placement), and generate natural language task descriptions such as ``pick up the red can and place it on the tray'' or ``grasp the Pringles container and move it to the left.'' This provides paired vision-language-action data for probing VLA representations under realistic conditions.

\subsection{Text-Image Semantics}

We begin by analyzing how VLAs represent objects and language. Applying NCut to OpenVLA's vision encoder at layer 4 produces visually distinct color clusters corresponding to task-critical elements: target objects to be manipulated, the robot's end-effector, and background scene elements. Crucially, these clusters exhibit viewpoint invariance—when we vary camera angle or lighting while keeping the same object, the assigned colors remain consistent, indicating that the vision encoder has learned object-centric rather than appearance-specific representations.

To assess language grounding, we jointly cluster vision and language tokens extracted from the fusion layers. Noun phrases referring to specific objects show strong spatial alignment with their visual referents. For instance, when the instruction contains ``Pringles can,'' the token embeddings for these words cluster near the image patches depicting the can. Action verbs like ``pick,'' ``grasp,'' and ``place'' correlate with both gripper and target object regions, though this alignment is weaker than for nouns, suggesting that action semantics may be less tightly grounded in visual features.

Interestingly, we observe rapid cluster reassignments when failure indicators appear. If an object begins to tilt during grasping or the gripper starts to slip, the associated color shifts abruptly, even though the model receives no explicit failure labels during this analysis. This suggests that VLA representations implicitly encode cues predictive of task failure, which could potentially be exploited for early warning systems.

\subsection{Temporal Dynamics}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{temporal.png} 
    \caption{This is the temporal NCut results we got.}
\end{figure}

Tracking cluster assignments across manipulation sequences reveals temporal structure. During successful executions, clusters corresponding to the target object remain stable throughout the grasp-lift-transport sequence. In contrast, failed attempts exhibit higher volatility, with frequent cluster reassignments suggesting representational uncertainty. When we plot the distribution of cluster IDs over time, distinct manipulation phases emerge as transitions in the dominant cluster: approach is characterized by increasing co-occurrence of gripper and object clusters, contact triggers a sharp shift as both regions merge into a unified grasp cluster, and release corresponds to their separation. These temporal signatures could serve as features for downstream tasks like phase detection or progress monitoring.

\subsection{Input Sensitivity Analysis}

To quantify the relative importance of vision versus language, we conduct a controlled perturbation study. We systematically modify images (changing lighting conditions, object appearance, or camera viewpoint) and text (swapping action verbs like ``pick'' $\leftrightarrow$ ``grasp,'' or nouns like ``can'' $\leftrightarrow$ ``container''), then measure the induced change in VLA feature representations via cosine distance. Denoting image-induced shifts as $\Delta_\text{img}$, verb swaps as $\Delta_\text{verb}$, and noun swaps as $\Delta_\text{noun}$, we find $\Delta_\text{img} > 10 \cdot \Delta_\text{verb} > \Delta_\text{noun}$. This dramatic imbalance indicates that VLA representations are overwhelmingly dominated by visual information, with language playing a relatively minor modulatory role.

We further probe semantic organization via template-based retrieval. For each semantic category (e.g., the verb ``push'' or the noun ``room''), we collect 30 images from COCO containing that category, average their VLA features to form a template, and compute mean Average Precision (mAP) for retrieving other instances. Scene-level nouns achieve relatively high mAP: ``room'' (0.36), ``street'' (0.29), ``field'' (0.27). Object nouns and action verbs score substantially lower: ``picture'' (0.09), ``shirt'' (0.07), and among verbs, even high-frequency actions like ``fill'' (0.07) and ``show'' (0.05) cluster poorly. This suggests VLAs form coherent semantic clusters primarily for static scenes and objects, while action semantics remain weakly differentiated.

\subsection{Spatial Intelligence Evaluation}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{spatial.png} 
    \caption{The difference between the VLA affinity and the true physicial distance. We do this experiment on NeRF datasets.}
    \label{fig:website}
\end{figure}

A critical question is whether VLAs learn geometric understanding or merely memorize appearance-pose associations. To test this, we extract VLA features from NeRF datasets where ground-truth camera poses are known, and train a 3-layer MLP to regress 6-DOF poses (position and orientation) from these features. On the training set, the probe achieves nearly perfect performance with loss 0.003, demonstrating that pose information is indeed encoded. However, on held-out test views, the loss remains at 0.998—complete failure to generalize. We also perform multi-dimensional scaling (MDS) on pairwise feature distances and compare to ground-truth pose distances. For nearby viewpoints in 3D space, we find no corresponding proximity in feature space, confirming that VLAs do not learn geometric relationships between views.

This finding has significant implications. VLAs can memorize which visual appearances correspond to which spatial configurations in their training data, but they lack the geometric inductive biases needed to reason about unseen viewpoints, occlusions, or multi-step assembly tasks requiring spatial planning. Future architectures may benefit from incorporating explicit 3D representations or contrastive losses that enforce view-invariant embeddings.

\subsection{Gradient Flow and Attribution}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{gradient.png} 
    \caption{The visualization results of our channel tracing methods. We can see that we can find certain channels that can contribute to certain clusters obtained from NCut.}
    \label{fig:website}
\end{figure}

To understand which features drive action predictions, we compute gradients of the action loss $\mathcal{L}$ with respect to individual token features: $\partial \mathcal{L} / \partial \mathbf{f}_i$. Gradient magnitudes concentrate heavily on object and gripper clusters, with background regions exhibiting near-zero attribution. This confirms that action prediction relies primarily on task-relevant semantic regions, ignoring irrelevant context.

We also examine which NCut eigenvectors contribute most to action prediction by analyzing the correlation between eigenvector coefficients and action output variations. Lower-frequency eigenvectors, corresponding to coarse semantic partitions, show strong correlations with action changes. Higher-frequency eigenvectors capturing fine texture details have minimal impact. This suggests that VLA action heads primarily operate on coarse object-level segmentation rather than fine-grained visual features, which may explain robustness to appearance variations but also fragility to subtle geometric misalignments.

\subsection{Simulation Validation}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{libero_fail1.png} 
    \caption{The visualization restuls when actually executing the policy in the LIBERO simulations.}
    \label{fig:website}
\end{figure}

We deploy OpenVLA in a simulated tabletop environment (using PyBullet) with randomized objects (cylinders, cubes, rings) and natural language pick-and-place instructions. The baseline model achieves 71\% success rate over 50 trials. By applying NCut to failure cases, we identify two recurring patterns: fragmented object clusters indicating poor segmentation, and premature overlap between gripper and obstacle clusters suggesting collision risk. We change the environment to make the objects have clearer edges and it raises the success rate to 80\%, validating that NCut-guided diagnosis translates to actionable performance improvements.

\section{Bonus: NCut PyTorch Documentation Website}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{website.png} 
    \caption{The new website for Ncut.}
    \label{fig:website}
\end{figure}

As part of this project, we developed and currently maintain the official NCut PyTorch documentation website at \url{https://ncut-pytorch.readthedocs.io} \cite{ncutpytorch}. The site provides comprehensive tutorials covering basic NCut operations, advanced debugging workflows, and API references for all modules and functions. It also includes a model gallery demonstrating applications to vision transformers, language models, diffusion models, and multimodal systems. Built with Sphinx and hosted on ReadTheDocs with continuous integration, the documentation auto-generates from code docstrings and Jupyter notebooks, ensuring consistency across software releases. The resource has been adopted by researchers beyond robotics for interpreting foundation models and serves as an educational tool in graduate courses on interpretable machine learning.

\section{Discussion and Future Work}

Our findings highlight both the strengths and fundamental limitations of current VLA architectures. The strong object-centric clustering and implicit failure awareness demonstrate that these models learn useful mid-level semantic representations. However, the 10$\times$ dominance of visual features over language raises critical questions about instruction following: do VLAs genuinely ground fine-grained linguistic distinctions, or do they primarily rely on visual priors with language serving as a weak contextual signal? The spatial intelligence failure is particularly concerning for real-world deployment, as tasks like insertion, precise placement, and occlusion reasoning all require geometric understanding beyond appearance matching.

Future work should investigate several directions. First, we plan to extend this analysis to OpenPi 0.5 \cite{physicalintelligence2025pi05}, which incorporates architectural improvements and larger-scale training, to determine whether these limitations persist or are ameliorated by scale. Second, we aim to study whether temporal NCut patterns align with instruction structure in multi-step tasks—for example, does the phrase ``first pick up the red block'' trigger a cluster transition before ``then place it on the blue block''? Third, integrating NCut-based early warning signals with failure recovery systems \cite{dai2024racer,duan2024aha} could enable online detection and correction. Finally, designing geometric inductive biases or auxiliary losses that enforce 3D-consistent representations may address the spatial generalization gap.

\section{Conclusion}

Nyström Normalized Cuts provides a practical and scalable tool for debugging robot foundation models. By visualizing feature geometry across layers and modalities, we can uncover semantic structure, diagnose failure modes, and identify architectural weaknesses that standard metrics obscure. Our analysis of OpenVLA reveals that while these models excel at object-centric perceptual reasoning, they struggle with language grounding and spatial generalization. As VLAs scale toward real-world deployment, interpretability methods like NCut will be essential for ensuring reliability, safety, and targeted improvement.

\bibliographystyle{plain}
\bibliography{sample}

\end{document}
