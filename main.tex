\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[english]{babel}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{natbib}

\title{\textbf{Unveiling the Feature Geometry of Robot Foundation Models via Nyström NCut}}

\author{
    Ningze Zhong, Kyle Zhang, Ily Rafaeli \\
    University of Pennsylvania \\
    \texttt{\{zhong666, kyle100, ilyr\}@seas.upenn.edu}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Vision-Language-Action (VLA) models have emerged as powerful robot foundation models, yet their internal representations remain largely opaque. We apply Nyström Normalized Cuts to visualize and analyze feature geometry across VLA hidden layers, revealing how these models process multimodal inputs during manipulation tasks. Our analysis uncovers strong object-centric semantic clustering and sensitivity to failure indicators, but also exposes critical limitations in spatial reasoning and language grounding. Through controlled experiments on OpenVLA, we demonstrate that visual features dominate over language by a factor of 10$\times$, and that spatial information fails to generalize despite perfect training-set memorization. These findings provide actionable insights for debugging and improving robot foundation models.
\end{abstract}

\section{Introduction}

Robot foundation models built on vision-language architectures \cite{kim2024openvla,physicalintelligence2025pi05} have demonstrated remarkable capabilities in following natural language instructions for diverse manipulation tasks, from tool use to contact-rich assembly. Despite their impressive empirical performance, these models remain black boxes. When a VLA fails to grasp an object or misinterprets an instruction, practitioners cannot easily diagnose whether the failure stems from poor visual encoding, weak language understanding, or faulty cross-modal fusion. This opacity poses significant challenges for debugging, model improvement, and safe deployment in real-world scenarios.

We propose using Nyström Normalized Cuts \cite{shi2000normalized} as a visual debugging tool for analyzing VLA internal representations. NCut performs spectral clustering on neural feature representations, mapping tokens to colors based on their position in the graph Laplacian's eigenspace. Our central hypothesis is that well-separated clusters indicate learnable structure: if NCut easily partitions features by semantic categories, downstream layers can exploit this structure for robust action prediction. In our exploration, this assumption has proven effective for debugging vision models like LLaVA, where we can identify exactly which layers fail to preserve task-relevant information by observing cluster coherence across controlled perturbations.

We conduct a comprehensive analysis of OpenVLA \cite{kim2024openvla} across multiple dimensions. First, we examine text-image semantics by clustering vision and language tokens jointly, revealing which objects the model attends to and how language anchors to visual regions. Second, we track temporal dynamics through manipulation sequences, identifying cluster transitions that correlate with distinct task phases like approach, grasp, and release. Third, we quantify input sensitivity by systematically perturbing images versus text and measuring the resulting feature-space shifts. Fourth, we probe spatial intelligence by training probes to predict camera poses from VLA embeddings. Finally, we trace gradient flow to understand which feature clusters drive action predictions.

Our findings reveal both strengths and critical weaknesses. While VLAs recognize task-relevant objects and implicitly detect failure cues like object tilting or gripper slippage, they exhibit a strong bias toward visual information over language, with image perturbations producing 10$\times$ larger representation changes than text modifications. Moreover, spatial information appears to be memorized rather than learned: VLAs achieve near-zero training error on pose regression but fail completely on held-out viewpoints, suggesting they encode appearance-pose pairs without geometric understanding. We validate these insights through simulation experiments where NCut-guided debugging improves Pick and Place success rates from 71\% to 80\%.

\section{Related Work}

\textbf{Vision-Language-Action Models.} Recent VLA architectures \cite{kim2024openvla,physicalintelligence2025pi05} fine-tune large vision-language models on robot trajectory data, leveraging broad priors from internet-scale pretraining to generalize across novel objects, scenes, and tasks. OpenVLA combines a pretrained vision transformer with a language model backbone featuring cross-attention layers, enabling end-to-end training from pixels and text to actions. OpenPi 0.5 extends this paradigm with improved data scaling and architectural refinements. While these models achieve strong empirical results, they lack built-in interpretability mechanisms. Prior work on failure detection \cite{dai2024racer,duan2024aha} trains separate models to identify errors post-hoc; in contrast, we analyze internal representations to understand inherent failure modes and guide targeted improvements.

\textbf{Neural Network Interpretability.} Attention visualization and gradient-based saliency maps are standard interpretability tools, but they provide only coarse spatial attributions and struggle with transformer architectures where information flows through complex residual paths. Spectral clustering methods like Normalized Cuts offer complementary insights by partitioning features based on global affinity structure rather than local gradient signals. Recent work has applied NCut to vision transformers for understanding semantic emergence across layers, but this approach has not been systematically extended to multimodal robot learning systems.

\section{Method}

\subsection{Nyström Normalized Cuts}

Given features $\mathbf{F} \in \mathbb{R}^{N \times d}$ extracted from a batch of $B$ images with $N = B \cdot h \cdot w$ total patch tokens, we construct an affinity matrix $\mathbf{W}$ where $W_{ij} = \exp(-\|\mathbf{f}_i - \mathbf{f}_j\|^2 / 2\sigma^2)$ captures pairwise token similarity. Normalized Cuts seeks a graph partition that minimizes the criterion
\begin{equation}
\text{NCut}(\mathcal{A}, \mathcal{B}) = \frac{\text{cut}(\mathcal{A}, \mathcal{B})}{\text{vol}(\mathcal{A})} + \frac{\text{cut}(\mathcal{A}, \mathcal{B})}{\text{vol}(\mathcal{B})},
\end{equation}
where $\text{cut}(\mathcal{A}, \mathcal{B}) = \sum_{i \in \mathcal{A}, j \in \mathcal{B}} W_{ij}$ measures edge weight between partitions and $\text{vol}(\mathcal{A}) = \sum_{i \in \mathcal{A}, j} W_{ij}$ is the total connection strength from partition $\mathcal{A}$ to all nodes. This optimization relaxes to solving the generalized eigenvalue problem $(\mathbf{D} - \mathbf{W})\mathbf{v} = \lambda \mathbf{D}\mathbf{v}$ for the smallest eigenvectors of the normalized graph Laplacian, where $\mathbf{D}$ is the diagonal degree matrix.

For computational tractability on large feature sets, we employ the Nyström approximation. Rather than computing the full $N \times N$ affinity matrix, we randomly sample $m \ll N$ landmark points and compute only the $m \times m$ landmark-to-landmark affinities and $m \times N$ landmark-to-all-points cross-affinities. The full eigendecomposition is then approximated via this low-rank representation, reducing complexity from $O(N^2)$ to $O(Nm)$. We project the resulting high-dimensional eigenvectors to RGB color space via t-SNE, enabling intuitive visualization where tokens with similar colors belong to the same semantic cluster.

\subsection{Debugging Protocol}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{feature_debuggin_left.png}
    
    \vspace{0.5em}
    \resizebox{\textwidth}{!}{%
    \renewcommand{\arraystretch}{1.15}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Row $\backslash$ Col} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
    \hline
    01--08 & 5 Fingers & 4 Extended & 5 Fingers & 5 Fingers & 5 Fingers & 5 Fingers & 5 Fingers & 5 Fingers \\
    \hline
    09--16 & Unknown & 16 Fingers & 5 Fingers & 4 Upwards & 10 (2 Hands) & Unknown & 5 Fingers & 10 Total \\
    \hline
    17--24 & 10 Total & 12 Visible & 10 Fingers & 10 Fingers & 4 Visible & 10 Visible & 4 Extended & 10 (2 Hands) \\
    \hline
    25--32 & 5 Fingers & 5 Fingers & 5 Fingers & 5 Fingers & 4 Fingers & 5 Fingers & 5 Fingers & 4 Extended \\
    \hline
    33--40 & 5 Fingers & 4 Fingers & 10 (Clasped) & 5 Fingers & 5 Fingers & 5 Fingers & 5 Fingers & 5 Fingers \\
    \hline
    \end{tabular}%
    }
    \caption{NCut visualization results showing feature clustering patterns across different image patches, with finger count classifications from LLaVA \cite{ncutpytorch}.}
    \label{fig:feature_debugging}
\end{figure*}

Our analysis methodology consists of three systematic steps, illustrated in Figure~\ref{fig:feature_debugging}.

First, we curate controlled image batches that isolate specific factors of interest. For example, to test a vision-language model's ability to distinguish finger counts, we collect images of hands displaying varying numbers of fingers and process them jointly. Figure~\ref{fig:feature_debugging} shows NCut applied to 40 such images, with the corresponding table indicating LLaVA's finger count predictions for each patch. We then inspect whether cluster colors align consistently with the true finger count---inconsistencies reveal cases where the model fails to discriminate semantic attributes.

Second, we perform layer-wise inspection by applying NCut to activations from each transformer block. If a perturbation in one sample causes color shifts, while colors remain unchanged across unperturbed samples, the model exhibits high sensitivity to that factor at that layer. Conversely, if colors remain stable despite controlled variations, the model has not learned to discriminate these semantic attributes.

Third, we perform comparative analysis across model configurations and failure cases. By visualizing NCut clusters for both successful and failed task executions side-by-side, we identify systematic differences in feature organization that correlate with performance. For instance, fragmented clusters on target objects often precede grasp failures, while coherent object-gripper cluster overlap typically indicates successful manipulation. This comparative approach enables us to formulate hypotheses about model weaknesses and guide targeted interventions, such as data augmentation or architectural modifications to address specific failure modes.

\subsection{Feature Extraction from VLAs}

VLA architectures typically consist of a vision encoder (often a pretrained ViT) that processes images into spatial patch tokens, and a language model backbone with interleaved cross-attention layers where image patches attend to text tokens. We extract intermediate representations by registering forward hooks on transformer blocks. For temporal analysis of manipulation videos, we process frames sequentially and concatenate patch features across time, yielding a 4D tensor that NCut partitions into spatio-temporal clusters. For language-vision alignment studies, we jointly cluster image patches and text tokens from fusion layers, then examine whether semantically related tokens (e.g., the word ``can'' and the visual region depicting a can) receive similar cluster assignments.

\section{Experiments and Results}

\subsection{Dataset and Experimental Setup}

To apply NCut techniques on OpenVLA, a dataset of robot actions and task completions is necessary. Our criteria for a dataset included:

\begin{itemize}
    \item Visual data from a real-world environment as opposed to simulations,
    \item Inclusion of action failure cases as opposed to only successful executions,
    \item Action diversity to enable exploration of diverse VLA characteristics, 
    \item Repetition of action executions - allowing for fine-grained perturbation selections,
    \item Labelling of the goals and degree of success of executed tasks.
\end{itemize}

Unfortunately, in preliminary exploration, we found that most open-source datasets cannot meet all of these requirements, often failing more than one of the criteria. This held true even for Pick and Place actions, which make up the majority of robot action datasets.

We decided to select a dataset which met most of these criteria, short of detailed labelling, and instead manually label it \cite{wang2019multimodal}. The selected dataset is informally curated by a research team for their specific explorations, and thus contains irregularities and non-uniformities in data collection (with some instances of media errors). However, its diversity makes it valuable for studying edge cases and failure modes that are often filtered from cleaner benchmarks. Given manual labelling, extraction of erroneous datapoints, and descriptions of each of the attempted tasks - this dataset enabled our analysis of a VLA's comprehension of text-image semantics.

For other explorations, we also utilize other datasets or simulations where we seek specific characteristics.

\subsection{Text-Image Semantics}

We begin by performing batch NCut on various frames of a simple successful case of successful robot action. Specifically in Figure \ref{fig:text2}, the sequential reach, grasp and lift of a red can. This figure performs NCut on the last layer of OpenVLA, with 20 vision clusters. Where similar colors represent close clustering (semantic alignment within the VLA's language-based latent space), several characteristics of the VLA may be recognised.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{text2.jpg} 
    \caption{Batch NCut results of successful action.}
    \label{fig:text2}
\end{figure}

Firstly, the model shows the ability differentiate between foreground and background, segmenting the robot arm and the red can from the table. Furthermore, the robot arm and the red can both display a similar olive-green hue throughout every frame of the interaction. They retain a green hue throughout frames of interaction, including instances where most of the arm and can are out of frame. These indicate that the vision encoder has learned object-centric rather than position-centric representation. Beyond that, the model associates the objects with one another - an indicator of semantic understanding that they are interacting. Minimal hue differences are observed between cases of the robot arm holding the can with an open or closed grip. This may be indicative of semantic distinction between various levels or stages of interaction.

Interestingly, we observe rapid cluster/color reassignments at points of transition between different stages of robot interactions. Specifically, notable color shifts occur when the gripper transitions from approach to contact with the object, when the grip closes around the target, and when the lifting motion begins. These transition points correspond to meaningful changes in the physical state of the manipulation task. The model's sensitivity to these phase boundaries suggests that the VLA has implicitly learned to recognize the discrete stages that comprise structured manipulation tasks like Pick and Place, rather than treating the action sequence as a continuous undifferentiated trajectory. This phase-aware representation could be particularly valuable for task monitoring and progress estimation in robotic systems.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{text3.jpg}
  \caption{Batch NCut results of frames of failed actions: rapid dropping (left), toppling (right).}
  \label{fig:text3}
\end{figure*}

To assess the degree to which the VLA's semantic comprehension is grounded in language, we jointly cluster vision and language tokens extracted from the fusion layers. Typically, this is used to map the self-attention between an object and its verbal description within a transformer. We aim to see if NCut can be used to discern more complex text-vision associations. Specifically, we seek to determine whether the trained VLA has a generalized sense of ``success'' or ``failure'' among robot-object interactions. Figure \ref{fig:text3} presents examples of this mapping on select frames of failed interactions with the red can shown earlier.

OpenVLA showed extreme sensitivity to instances of rapid movement, reorientation, and collisions of the foreground object being interacted with - resulting in more significant cluster/color reassignments than those discussed previously. These instances correlated closely with typical indicators of a failed robot interaction, such as loss of grip or control. Furthermore, upon frames indicative of failure, the new clusters show close alignment with the word ``failure''. This correlation also held for interactions with different objects, and with longer text token arrays. Regardless of the success or failure of a robot task, it was also observed that the attention of these text tokens typically focused on the table upon which the Pick and Place actions were taken.

These results suggest that OpenVLA representation semantically comprehends the various features, sequences, and commonalities of robot actions. They also indicate that the VLA's representations implicitly encode cues predictive of task success or failure. These basic observations already present extensive applications of NCut for VLA design. For instance, NCut may be used as triggers for early warning signals of failed or unsafe robot interactions. Such warnings may be applied online by leveraging the efficiency of Nyström NCut.


\subsection{Temporal Dynamics}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{temporal_left.png}
    
    \vspace{0.5em}
    
    \includegraphics[width=0.45\textwidth]{temporal_right.png}
    \caption{Temporal NCut results of robot Pick and Place of a toy carrot.}
    \label{fig:temporal}
\end{figure}

Tracking cluster assignments across manipulation sequences reveals temporal structure. During successful executions, clusters corresponding to the target object remain stable throughout the grasp-lift-transport sequence. In contrast, failed attempts exhibit higher volatility, with frequent cluster reassignments suggesting representational uncertainty. When we plot the distribution of cluster IDs over time, distinct manipulation phases emerge as transitions in the dominant cluster: approach is characterized by increasing co-occurrence of gripper and object clusters, contact triggers a sharp shift as both regions merge into a unified grasp cluster, and release corresponds to their separation. These temporal signatures could serve as features for downstream tasks like phase detection or progress monitoring.

\subsection{Input Sensitivity Analysis}

To quantify the relative importance of vision versus language, we conduct a controlled perturbation study. We systematically modify images (changing lighting conditions, object appearance, or camera viewpoint) and text (swapping action verbs like ``pick'' $\leftrightarrow$ ``grasp,'' or nouns like ``can'' $\leftrightarrow$ ``container''), then measure the induced change in VLA feature representations via cosine distance. Denoting image-induced shifts as $\Delta_\text{img}$, verb swaps as $\Delta_\text{verb}$, and noun swaps as $\Delta_\text{noun}$, we find $\Delta_\text{img} > 10 \cdot \Delta_\text{verb} > \Delta_\text{noun}$. This dramatic imbalance indicates that VLA representations are overwhelmingly dominated by visual information, with language playing a relatively minor modulatory role.

We further probe semantic organization via template-based retrieval. For each semantic category (e.g., the verb ``push'' or the noun ``room''), we collect 30 images from COCO containing that category, average their VLA features to form a template, and compute mean Average Precision (mAP) for retrieving other instances. Scene-level nouns achieve relatively high mAP: ``room'' (0.36), ``street'' (0.29), ``field'' (0.27). Object nouns and action verbs score substantially lower: ``picture'' (0.09), ``shirt'' (0.07), and among verbs, even high-frequency actions like ``fill'' (0.07) and ``show'' (0.05) cluster poorly. This suggests VLAs form coherent semantic clusters primarily for static scenes and objects, while action semantics remain weakly differentiated.

\subsection{Spatial Intelligence Evaluation}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{spatial_left.png}
    
    \vspace{0.5em}
    
    \includegraphics[width=0.45\textwidth]{spatial_right.png}
    \caption{The difference between the VLA affinity and the true physical distance. We do this experiment on NeRF datasets.}
    \label{fig:spatial}
\end{figure}

A critical question is whether VLAs learn geometric understanding or merely memorize appearance-pose associations. To test this, we extract VLA features from NeRF datasets where ground-truth camera poses are known, and train a 3-layer MLP to regress 6-DOF poses (position and orientation) from these features. On the training set, the probe achieves nearly perfect performance with loss 0.003, demonstrating that pose information is indeed encoded. However, on held-out test views, the loss remains at 0.998—complete failure to generalize. We also perform multi-dimensional scaling (MDS) on pairwise feature distances and compare to ground-truth pose distances. For nearby viewpoints in 3D space, we find no corresponding proximity in feature space, confirming that VLAs do not learn geometric relationships between views.

This finding has significant implications. VLAs can memorize which visual appearances correspond to which spatial configurations in their training data, but they lack the geometric inductive biases needed to reason about unseen viewpoints, occlusions, or multi-step assembly tasks requiring spatial planning. Future architectures may benefit from incorporating explicit 3D representations or contrastive losses that enforce view-invariant embeddings.

\subsection{Gradient Flow and Attribution}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{gradient.png} 
    \caption{The visualization results of our channel tracing methods. We can see that we can find certain channels that can contribute to certain clusters obtained from NCut.}
    \label{fig:website}
\end{figure}

To understand which features drive action predictions, we compute gradients of the action loss $\mathcal{L}$ with respect to individual token features: $\partial \mathcal{L} / \partial \mathbf{f}_i$. Gradient magnitudes concentrate heavily on object and gripper clusters, with background regions exhibiting near-zero attribution. This confirms that action prediction relies primarily on task-relevant semantic regions, ignoring irrelevant context.

We also examine which NCut eigenvectors contribute most to action prediction by analyzing the correlation between eigenvector coefficients and action output variations. Lower-frequency eigenvectors, corresponding to coarse semantic partitions, show strong correlations with action changes. Higher-frequency eigenvectors capturing fine texture details have minimal impact. This suggests that VLA action heads primarily operate on coarse object-level segmentation rather than fine-grained visual features, which may explain robustness to appearance variations but also fragility to subtle geometric misalignments.

\subsection{Simulation Validation}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{libero_fail1.png} 
    \caption{The visualization results when actually executing the policy in the LIBERO simulations.}
    \label{fig:libero}
\end{figure}

We deploy OpenVLA in a simulated tabletop environment (using PyBullet) with randomized objects (cylinders, cubes, rings) and natural language pick-and-place instructions. The baseline model achieves 71\% success rate over 50 trials. By applying NCut to failure cases, we identify two recurring patterns: fragmented object clusters indicating poor segmentation, and premature overlap between gripper and obstacle clusters suggesting collision risk. We change the environment to make the objects have clearer edges and it raises the success rate to 80\%, validating that NCut-guided diagnosis translates to actionable performance improvements.

\section{NCut PyTorch Documentation}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{website.png} 
    \caption{The new website for NCut.}
    \label{fig:website}
\end{figure}

As part of this project, we developed and currently maintain the official NCut PyTorch documentation website \cite{ncutpytorch}. The site provides comprehensive tutorials covering basic NCut operations, advanced debugging workflows, and API references for all modules and functions. It also includes a model gallery demonstrating applications to vision transformers, language models, diffusion models, and multimodal systems. Built with Sphinx and hosted on ReadTheDocs with continuous integration, the documentation auto-generates from code docstrings and Jupyter notebooks, ensuring consistency across software releases. The resource has been adopted by researchers beyond robotics for interpreting foundation models and serves as an educational tool in graduate courses on interpretable machine learning.

\section{Discussion and Future Work}

Our exploration shows that NCut offers insight into the black box architectures of VLAs. While much of the decision-making process may remain obscured, this tool has enabled us to better understand and characterize a VLA model - from low-level insights on token attention, to higher-level concepts of spatial/geometric comprehension and semantic understanding of text tokens related to task instructions. 

Our findings highlight both the strengths and fundamental limitations of current VLA architectures. The strong object-centric clustering and implicit failure awareness demonstrate that these models learn useful mid-level semantic representations. However, the 10$\times$ dominance of visual features over language raises critical questions about instruction following: do VLAs genuinely ground fine-grained linguistic distinctions, or do they primarily rely on visual priors with language serving as a weak contextual signal? The spatial intelligence failure is particularly concerning for real-world deployment, as tasks like insertion, precise placement, and occlusion reasoning all require geometric understanding beyond appearance matching.

We believe that NCut may offer many avenues for continued exploration of VLAs. First, we plan to extend this analysis to OpenPi 0.5 \cite{physicalintelligence2025pi05}, which incorporates architectural improvements to OpenVLA and larger-scale training, to determine how the capabilities and limitations of VLAs may vary across architectures and training approaches. Second, we aim to study whether temporal NCut patterns align with instruction structure in multi-step tasks. For example, does the phrase ``first pick up the red block'' trigger a cluster transition before ``then place it on the blue block''? Third, integrating NCut-based early warning signals with failure recovery systems \cite{dai2024racer,duan2024aha} could enable online detection and correction.

Additional future work may also look to tracing failures across architectural submodules. For VLAs with separate vision encoders and vision-language fusion layers, NCut may allow for analysis of component independently to localize whether failures originate in visual feature extraction, language encoding, or cross-modal alignment. It may also aid as an analysis tool in development of new or improved architectural submodules. For instance, it may help characterise geometric inductive biases or auxiliary losses that enforce 3D-consistent representations - which may address the spatial generalization gap.

\section{Conclusion}

Nyström Normalized Cuts provides a practical and scalable tool for debugging robot foundation models. By visualizing feature geometry across layers and modalities, we can uncover semantic structure, diagnose failure modes, and identify architectural weaknesses that standard metrics obscure. Our analysis of OpenVLA reveals that while these models excel at object-centric perceptual reasoning, they struggle with language grounding and spatial generalization. NCut may be applied in various additional ways to those explored, and as VLAs scale toward real-world deployment, interpretability methods like NCut may prove crucial for ensuring reliability, safety, and targeted improvement.

\bibliographystyle{plain}
\bibliography{sample}

\end{document}
