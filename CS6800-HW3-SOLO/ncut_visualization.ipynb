{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52987a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from ncut_pytorch import ncut_fn, tsne_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f832c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "IMAGE_RESOLUTION = 224\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dino-vitb16\")\n",
    "processor.crop_size = {'height': IMAGE_RESOLUTION, 'width': IMAGE_RESOLUTION}\n",
    "model = AutoModel.from_pretrained(\"facebook/dino-vitb16\").to(DEVICE).to(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef81ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "data_dir = Path(\"hw3 solo datasets\")\n",
    "data_paths = {fname.split(\"_\")[2]: f\"{data_dir}/{fname}\" for fname in os.listdir(data_dir)}\n",
    "with h5py.File(data_paths[\"img\"], \"r\") as f:\n",
    "    images_raw = torch.tensor(np.array(f[\"data\"])).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and crop the non-padded region for each image individually\n",
    "non_padded_images = []\n",
    "for img in images_raw:\n",
    "    # Identify the non-padded region\n",
    "    non_zero_mask = img.abs().sum(dim=0) > 0  # Sum across channels to find non-zero regions\n",
    "    non_zero_rows = non_zero_mask.any(dim=1)  # Find rows with non-zero values\n",
    "    non_zero_cols = non_zero_mask.any(dim=0)  # Find columns with non-zero values\n",
    "\n",
    "    # Find the bounding box of the non-padded region\n",
    "    top, bottom = torch.where(non_zero_rows)[0][[0, -1]]\n",
    "    left, right = torch.where(non_zero_cols)[0][[0, -1]]\n",
    "\n",
    "    # Crop the image to the non-padded region\n",
    "    cropped_img = img[:, top:bottom + 1, left:right + 1]\n",
    "    non_padded_images.append(cropped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da862d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific indices along the first dimension of the 4D tensor\n",
    "selected_indices = [0, 3, 4, 6, 24, 26, 27, 29, 37, 44, 51, 52, 53, 66, 71, 72, 73, 76, 85, 92, 93, 95, 97]\n",
    "images_cat = [non_padded_images[i] for i in selected_indices[:20]]\n",
    "fig, axs = plt.subplots(4, 5, figsize=(15, 15))\n",
    "for i, img in enumerate(images_cat):\n",
    "    ax = axs[i // 5, i % 5]\n",
    "    ax.imshow(img.permute(1, 2, 0).numpy().astype(np.uint8))\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e207cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [img.permute(1, 2, 0).numpy().astype(np.uint8) for img in images_cat]\n",
    "\n",
    "# Process the batch of images\n",
    "batch_inputs = processor(images=images, return_tensors=\"pt\", do_rescale=True).to(DEVICE).to(DTYPE)\n",
    "print(f\"Batch shape: {batch_inputs['pixel_values'].shape}\")\n",
    "images = [img.astype(np.float32) / 255.0 for img in images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9396cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**batch_inputs, output_hidden_states=True, interpolate_pos_encoding=True)\n",
    "\n",
    "# The last hidden state contains the patch features (and CLS token)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "print(f\"Shape of last hidden state: {last_hidden_state.shape}\")\n",
    "\n",
    "# The hidden states are a tuple, one for each layer\n",
    "hidden_states = outputs.hidden_states\n",
    "print(f\"Number of hidden layers: {len(hidden_states)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87113943",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_layer in [-5, -4, -3, -2, -1]:\n",
    "# target_layer = -2 # Last layer\n",
    "# We are interested in the patch features, so we skip the first token (CLS token)\n",
    "    patch_features = hidden_states[target_layer][:, 1:, :]\n",
    "\n",
    "    # NCut and t-SNE coloring\n",
    "    b, n, d = patch_features.shape\n",
    "    patch_features = rearrange(patch_features, 'b n d -> (b n) d')  # Combine batch and patches\n",
    "    eigvecs, _ = ncut_fn(patch_features, n_eig=100, d_gamma=0.05)\n",
    "    colors_rgb = tsne_color(eigvecs)\n",
    "\n",
    "    # The processor resizes the image to a square, so we can find the patch grid size\n",
    "    h = w = int(np.sqrt(n))\n",
    "    print(f\"Patch grid size: {h}x{w}; Layer: {target_layer}\")\n",
    "\n",
    "    # Reshape the colors to match the patch grid\n",
    "    color_grid = rearrange(colors_rgb.cpu(), '(b h w) c -> b h w c', b=b, h=h, w=w)\n",
    "\n",
    "    # Visualize the original images and colored image patches\n",
    "    num_images_to_show = min(8, b)\n",
    "    fig, axs = plt.subplots(2, num_images_to_show, figsize=(15, 10))\n",
    "    for i in range(num_images_to_show):\n",
    "        # Show original image\n",
    "        axs[0, i].imshow(images[i])\n",
    "        axs[0, i].set_title(f'Original Image {i+1}')\n",
    "        axs[0, i].axis('off')\n",
    "        \n",
    "        # Show NCut visualization\n",
    "        axs[1, i].imshow(color_grid[i].numpy())\n",
    "        axs[1, i].set_title(f'NCut Visualization {i+1}')\n",
    "        axs[1, i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6976fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./src')\n",
    "from model import SOLO\n",
    "\n",
    "################################################################################################################\n",
    "# TODO: Load the model checkpoint you want to visualize here\n",
    "################################################################################################################\n",
    "solo_model: SOLO = SOLO.load_from_checkpoint(\"checkpoints/best_resnet_epoch=35.ckpt\", strict=False).cuda()\n",
    "################################################################################################################\n",
    "\n",
    "backbone_outputs = []\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "normalized_images = [(torch.tensor(img).to(torch.float32) - mean) / std for img in images]\n",
    "normalized_images = [img.permute(2, 0, 1) for img in normalized_images]  # Change to (C, H, W) format\n",
    "\n",
    "with torch.no_grad():\n",
    "    for non_padded_batched_img in normalized_images:\n",
    "        backbone_output = solo_model.backbone(non_padded_batched_img.unsqueeze(0).to(DEVICE))\n",
    "        backbone_outputs.append(backbone_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d888e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_layer in backbone_outputs[0].keys():\n",
    "    print(f\"Backbone layer: {target_layer}, shape: {backbone_outputs[0][target_layer].shape}\")\n",
    "    patch_features_all = []\n",
    "    for backbone_output in backbone_outputs:\n",
    "        resnet_feats = backbone_output[target_layer].permute(0,2,3,1)\n",
    "        h, w = resnet_feats.shape[1], resnet_feats.shape[2]\n",
    "        b, d = resnet_feats.shape[0], resnet_feats.shape[3]\n",
    "        n = h * w\n",
    "        patch_features = rearrange(resnet_feats, 'b h w d -> (b h w) d')  # Combine batch and patches\n",
    "        patch_features_all.append(patch_features)\n",
    "    patch_features_all = torch.cat(patch_features_all, dim=0)\n",
    "    b = len(backbone_outputs)\n",
    "    eigvecs, _ = ncut_fn(patch_features_all, n_eig=100, d_gamma=0.05)\n",
    "    colors_rgb = tsne_color(eigvecs)\n",
    "    num_images_to_show = min(8, b)\n",
    "\n",
    "    fig, axs = plt.subplots(2, num_images_to_show, figsize=(15, 10))\n",
    "    idx = 0\n",
    "    for i in range(num_images_to_show):\n",
    "        h, w = backbone_outputs[i][target_layer].shape[2], backbone_outputs[i][target_layer].shape[3]\n",
    "        colors_rgb_i = colors_rgb[idx:idx + h * w]\n",
    "        color_grid = rearrange(colors_rgb_i.cpu(), '(b h w) c -> b h w c', b=1, h=h, w=w)\n",
    "        idx += h * w\n",
    "        # Show original image\n",
    "        axs[0, i].imshow(images[i])\n",
    "        axs[0, i].set_title(f'Original Image {i+1}')\n",
    "        axs[0, i].axis('off')\n",
    "        \n",
    "        # Show NCut visualization\n",
    "        axs[1, i].imshow(color_grid[0].numpy())\n",
    "        axs[1, i].set_title(f'NCut Visualization {i+1}')\n",
    "        axs[1, i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis6800",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}