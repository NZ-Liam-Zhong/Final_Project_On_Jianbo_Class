\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[english]{babel}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{natbib}
\usepackage{listings}
\usepackage{xcolor}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\scriptsize,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{orange},
    numbers=left,
    numberstyle=\tiny\color{gray},
    breaklines=true,
    frame=single,
    captionpos=b,
    showstringspaces=false
}

\title{\textbf{Unveiling the Feature Geometry of Robot Foundation Models via Nyström NCut}}

\author{
    Ningze Zhong, Kyle Zhang, Ily Rafaeli \\
    University of Pennsylvania \\
    \texttt{\{zhong666, kyle100, ilyr\}@seas.upenn.edu}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Vision-Language-Action (VLA) models have emerged as powerful robot foundation models, yet their internal representations remain largely opaque. We apply Nyström Normalized Cuts to visualize and analyze feature geometry across VLA hidden layers, revealing how these models process multimodal inputs during manipulation tasks. Our analysis uncovers strong object-centric semantic clustering and sensitivity to failure indicators, but also exposes critical limitations in spatial reasoning and language grounding. Through controlled experiments on OpenVLA, we demonstrate that visual features dominate over language by a factor of 10$\times$, and that spatial information fails to generalize despite perfect training-set memorization. These findings provide actionable insights for debugging and improving robot foundation models.
\end{abstract}

\section{Introduction}

Robot foundation models built on vision-language architectures \cite{kim2024openvla,physicalintelligence2025pi05} have demonstrated remarkable capabilities in following natural language instructions for diverse manipulation tasks, from tool use to contact-rich assembly. Despite their impressive empirical performance, these models remain black boxes. When a VLA fails to grasp an object or misinterprets an instruction, practitioners cannot easily diagnose whether the failure stems from poor visual encoding, weak language understanding, or faulty cross-modal fusion. This opacity poses significant challenges for debugging, model improvement, and safe deployment in real-world scenarios.

We propose using Nyström Normalized Cuts \cite{shi2000normalized} as a visual debugging tool for analyzing VLA internal representations. NCut performs spectral clustering on neural feature representations, mapping tokens to colors based on their position in the graph Laplacian's eigenspace. Our central hypothesis is that well-separated clusters indicate learnable structure: if NCut easily partitions features by semantic categories, downstream layers can exploit this structure for robust action prediction. In our exploration, this assumption has proven effective for debugging vision models like LLaVA, where we can identify exactly which layers fail to preserve task-relevant information by observing cluster coherence across controlled perturbations.

We conduct a comprehensive analysis of OpenVLA \cite{kim2024openvla} across multiple dimensions. First, we examine text-image semantics by clustering vision and language tokens jointly, revealing which objects the model attends to and how language anchors to visual regions. Second, we track temporal dynamics through manipulation sequences, identifying cluster transitions that correlate with distinct task phases like approach, grasp, and release. Third, we quantify input sensitivity by systematically perturbing images versus text and measuring the resulting feature-space shifts. Fourth, we probe spatial intelligence by training probes to predict camera poses from VLA embeddings. Finally, we trace gradient flow to understand which feature clusters drive action predictions.

Our findings reveal both strengths and critical weaknesses. While VLAs recognize task-relevant objects and implicitly detect failure cues like object tilting or gripper slippage, they exhibit a strong bias toward visual information over language, with image perturbations producing 10$\times$ larger representation changes than text modifications. Moreover, spatial information appears to be memorized rather than learned: VLAs achieve near-zero training error on pose regression but fail completely on held-out viewpoints, suggesting they encode appearance-pose pairs without geometric understanding. We validate these insights through simulation experiments where NCut-guided debugging improves Pick and Place success rates from 71\% to 80\%.

\section{Related Work}

\textbf{Vision-Language-Action Models.} Recent VLA architectures \cite{kim2024openvla,physicalintelligence2025pi05} fine-tune large vision-language models on robot trajectory data, leveraging broad priors from internet-scale pretraining to generalize across novel objects, scenes, and tasks. OpenVLA combines a pretrained vision transformer with a language model backbone featuring cross-attention layers, enabling end-to-end training from pixels and text to actions. OpenPi 0.5 extends this paradigm with improved data scaling and architectural refinements. While these models achieve strong empirical results, they lack built-in interpretability mechanisms. Prior work on failure detection \cite{dai2024racer,duan2024aha} trains separate models to identify errors post-hoc; in contrast, we analyze internal representations to understand inherent failure modes and guide targeted improvements.

\textbf{Neural Network Interpretability.} Attention visualization and gradient-based saliency maps are standard interpretability tools, but they provide only coarse spatial attributions and struggle with transformer architectures where information flows through complex residual paths. Spectral clustering methods like Normalized Cuts offer complementary insights by partitioning features based on global affinity structure rather than local gradient signals. Recent work has applied NCut to vision transformers for understanding semantic emergence across layers, but this approach has not been systematically extended to multimodal robot learning systems.

\section{Method}

\subsection{Nyström Normalized Cuts}

Given features $\mathbf{F} \in \mathbb{R}^{N \times d}$ extracted from a batch of $B$ images with $N = B \cdot h \cdot w$ total patch tokens, we construct an affinity matrix $\mathbf{W}$ where $W_{ij} = \exp(-\|\mathbf{f}_i - \mathbf{f}_j\|^2 / 2\sigma^2)$ captures pairwise token similarity. Normalized Cuts seeks a graph partition that minimizes the criterion
\begin{equation}
\text{NCut}(\mathcal{A}, \mathcal{B}) = \frac{\text{cut}(\mathcal{A}, \mathcal{B})}{\text{vol}(\mathcal{A})} + \frac{\text{cut}(\mathcal{A}, \mathcal{B})}{\text{vol}(\mathcal{B})},
\end{equation}
where $\text{cut}(\mathcal{A}, \mathcal{B}) = \sum_{i \in \mathcal{A}, j \in \mathcal{B}} W_{ij}$ measures edge weight between partitions and $\text{vol}(\mathcal{A}) = \sum_{i \in \mathcal{A}, j} W_{ij}$ is the total connection strength from partition $\mathcal{A}$ to all nodes. This optimization relaxes to solving the generalized eigenvalue problem $(\mathbf{D} - \mathbf{W})\mathbf{v} = \lambda \mathbf{D}\mathbf{v}$ for the smallest eigenvectors of the normalized graph Laplacian, where $\mathbf{D}$ is the diagonal degree matrix.

For computational tractability on large feature sets, we employ the Nyström approximation. Rather than computing the full $N \times N$ affinity matrix, we randomly sample $m \ll N$ landmark points and compute only the $m \times m$ landmark-to-landmark affinities and $m \times N$ landmark-to-all-points cross-affinities. The full eigendecomposition is then approximated via this low-rank representation, reducing complexity from $O(N^2)$ to $O(Nm)$. We project the resulting high-dimensional eigenvectors to RGB color space via t-SNE, enabling intuitive visualization where tokens with similar colors belong to the same semantic cluster. See Appendix~\ref{app:ncut_implementation} for implementation details.

\subsection{Debugging Protocol}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{feature_debuggin_left.png}
    
    \vspace{0.5em}
    \resizebox{\textwidth}{!}{%
    \renewcommand{\arraystretch}{1.15}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Row $\backslash$ Col} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
    \hline
    01--08 & 5 Fingers & 4 Extended & 5 Fingers & 5 Fingers & 5 Fingers & 5 Fingers & 5 Fingers & 5 Fingers \\
    \hline
    09--16 & Unknown & 16 Fingers & 5 Fingers & 4 Upwards & 10 (2 Hands) & Unknown & 5 Fingers & 10 Total \\
    \hline
    17--24 & 10 Total & 12 Visible & 10 Fingers & 10 Fingers & 4 Visible & 10 Visible & 4 Extended & 10 (2 Hands) \\
    \hline
    25--32 & 5 Fingers & 5 Fingers & 5 Fingers & 5 Fingers & 4 Fingers & 5 Fingers & 5 Fingers & 4 Extended \\
    \hline
    33--40 & 5 Fingers & 4 Fingers & 10 (Clasped) & 5 Fingers & 5 Fingers & 5 Fingers & 5 Fingers & 5 Fingers \\
    \hline
    \end{tabular}%
    }
    \caption{NCut visualization results showing feature clustering patterns across different image patches, with finger count classifications from LLaVA \cite{ncutpytorch}.}
    \label{fig:feature_debugging}
\end{figure*}

Our analysis methodology consists of three systematic steps, illustrated in Figure~\ref{fig:feature_debugging}.

First, we curate controlled image batches that isolate specific factors of interest. For example, to test a vision-language model's ability to distinguish finger counts, we collect images of hands displaying varying numbers of fingers and process them jointly. Figure~\ref{fig:feature_debugging} shows NCut applied to 40 such images, with the corresponding table indicating LLaVA's finger count predictions for each patch. We then inspect whether cluster colors align consistently with the true finger count---inconsistencies reveal cases where the model fails to discriminate semantic attributes.

Second, we perform layer-wise inspection by applying NCut to activations from each transformer block. If a perturbation in one sample causes color shifts, while colors remain unchanged across unperturbed samples, the model exhibits high sensitivity to that factor at that layer. Conversely, if colors remain stable despite controlled variations, the model has not learned to discriminate these semantic attributes.

Third, we perform comparative analysis across model configurations and failure cases. By visualizing NCut clusters for both successful and failed task executions side-by-side, we identify systematic differences in feature organization that correlate with performance. For instance, fragmented clusters on target objects often precede grasp failures, while coherent object-gripper cluster overlap typically indicates successful manipulation. This comparative approach enables us to formulate hypotheses about model weaknesses and guide targeted interventions, such as data augmentation or architectural modifications to address specific failure modes.

\subsection{Feature Extraction from VLAs}

VLA architectures typically consist of a vision encoder (often a pretrained ViT) that processes images into spatial patch tokens, and a language model backbone with interleaved cross-attention layers where image patches attend to text tokens. We extract intermediate representations by registering forward hooks on transformer blocks. For temporal analysis of manipulation videos, we process frames sequentially and concatenate patch features across time, yielding a 4D tensor that NCut partitions into spatio-temporal clusters. For language-vision alignment studies, we jointly cluster image patches and text tokens from fusion layers, then examine whether semantically related tokens (e.g., the word ``can'' and the visual region depicting a can) receive similar cluster assignments. Appendix~\ref{app:feature_extraction} provides code for extracting vision and language features from OpenVLA.

\section{Experiments and Results}

\subsection{Dataset and Experimental Setup}

To apply NCut techniques on OpenVLA, a dataset of robot actions and task completions is necessary. Our criteria for a dataset included:

\begin{itemize}
    \item Visual data from a real-world environment as opposed to simulations,
    \item Inclusion of action failure cases as opposed to only successful executions,
    \item Action diversity to enable exploration of diverse VLA characteristics, 
    \item Repetition of action executions - allowing for fine-grained perturbation selections,
    \item Labelling of the goals and degree of success of executed tasks.
\end{itemize}

Unfortunately, in preliminary exploration, we found that most open-source datasets cannot meet all of these requirements, often failing more than one of the criteria. This held true even for Pick and Place actions, which make up the majority of robot action datasets.

We decided to select a dataset which met most of these criteria, short of detailed labelling, and instead manually label it \cite{wang2019multimodal}. The selected dataset is informally curated by a research team for their specific explorations, and thus contains irregularities and non-uniformities in data collection (with some instances of media errors). However, its diversity makes it valuable for studying edge cases and failure modes that are often filtered from cleaner benchmarks. Given manual labelling, extraction of erroneous datapoints, and descriptions of each of the attempted tasks - this dataset enabled our analysis of a VLA's comprehension of text-image semantics.

For other explorations, we also utilize other datasets or simulations where we seek specific characteristics.

\subsection{Text-Image Semantics}

We begin by performing batch NCut on various frames of a simple successful case of successful robot action. Specifically in Figure \ref{fig:text2}, the sequential reach, grasp and lift of a red can. This figure performs NCut on the last layer of OpenVLA, with 20 vision clusters. Where similar colors represent close clustering (semantic alignment within the VLA's language-based latent space), several characteristics of the VLA may be recognised.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{text2.jpg} 
    \caption{Batch NCut results of successful action.}
    \label{fig:text2}
\end{figure}

Firstly, the model shows the ability differentiate between foreground and background, segmenting the robot arm and the red can from the table. Furthermore, the robot arm and the red can both display a similar olive-green hue throughout every frame of the interaction. They retain a green hue throughout frames of interaction, including instances where most of the arm and can are out of frame. These indicate that the vision encoder has learned object-centric rather than position-centric representation. Beyond that, the model associates the objects with one another - an indicator of semantic understanding that they are interacting. Minimal hue differences are observed between cases of the robot arm holding the can with an open or closed grip. This may be indicative of semantic distinction between various levels or stages of interaction.

Interestingly, we observe rapid cluster/color reassignments at points of transition between different stages of robot interactions. Specifically, notable color shifts occur when the gripper transitions from approach to contact with the object, when the grip closes around the target, and when the lifting motion begins. These transition points correspond to meaningful changes in the physical state of the manipulation task. The model's sensitivity to these phase boundaries suggests that the VLA has implicitly learned to recognize the discrete stages that comprise structured manipulation tasks like Pick and Place, rather than treating the action sequence as a continuous undifferentiated trajectory. This phase-aware representation could be particularly valuable for task monitoring and progress estimation in robotic systems.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{text3.jpg}
  \caption{Batch NCut results of frames of failed actions: rapid dropping (left), toppling (right).}
  \label{fig:text3}
\end{figure*}

To assess the degree to which the VLA's semantic comprehension is grounded in language, we jointly cluster vision and language tokens extracted from the fusion layers. Typically, this is used to map the self-attention between an object and its verbal description within a transformer. We aim to see if NCut can be used to discern more complex text-vision associations. Specifically, we seek to determine whether the trained VLA has a generalized sense of ``success'' or ``failure'' among robot-object interactions. Figure \ref{fig:text3} presents examples of this mapping on select frames of failed interactions with the red can shown earlier.

OpenVLA showed extreme sensitivity to instances of rapid movement, reorientation, and collisions of the foreground object being interacted with - resulting in more significant cluster/color reassignments than those discussed previously. These instances correlated closely with typical indicators of a failed robot interaction, such as loss of grip or control. Furthermore, upon frames indicative of failure, the new clusters show close alignment with the word ``failure''. This correlation also held for interactions with different objects, and with longer text token arrays. Regardless of the success or failure of a robot task, it was also observed that the attention of these text tokens typically focused on the table upon which the Pick and Place actions were taken.

These results suggest that OpenVLA representation semantically comprehends the various features, sequences, and commonalities of robot actions. They also indicate that the VLA's representations implicitly encode cues predictive of task success or failure. These basic observations already present extensive applications of NCut for VLA design. For instance, NCut may be used as triggers for early warning signals of failed or unsafe robot interactions. Such warnings may be applied online by leveraging the efficiency of Nyström NCut.


\subsection{Temporal Dynamics}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{temporal_left.png}
    
    \vspace{0.5em}
    
    \includegraphics[width=0.45\textwidth]{temporal_right.png}
    \caption{Temporal NCut results of robot Pick and Place of a toy carrot.}
    \label{fig:temporal}
\end{figure}

Tracking cluster assignments across manipulation sequences reveals temporal structure. During successful executions, clusters corresponding to the target object remain stable throughout the grasp-lift-transport sequence. In contrast, failed attempts exhibit higher volatility, with frequent cluster reassignments suggesting representational uncertainty. When we plot the distribution of cluster IDs over time, distinct manipulation phases emerge as transitions in the dominant cluster: approach is characterized by increasing co-occurrence of gripper and object clusters, contact triggers a sharp shift as both regions merge into a unified grasp cluster, and release corresponds to their separation. These temporal signatures could serve as features for downstream tasks like phase detection or progress monitoring. See Appendix~\ref{app:temporal_alignment} for our temporal eigenvector alignment implementation.

\subsection{Input Sensitivity Analysis}

To quantify the relative importance of vision versus language, we conduct a controlled perturbation study. We systematically modify images (changing lighting conditions, object appearance, or camera viewpoint) and text (swapping action verbs like ``pick'' $\leftrightarrow$ ``grasp,'' or nouns like ``can'' $\leftrightarrow$ ``container''), then measure the induced change in VLA feature representations via cosine distance. Denoting image-induced shifts as $\Delta_\text{img}$, verb swaps as $\Delta_\text{verb}$, and noun swaps as $\Delta_\text{noun}$, we find $\Delta_\text{img} > 10 \cdot \Delta_\text{verb} > \Delta_\text{noun}$. This dramatic imbalance indicates that VLA representations are overwhelmingly dominated by visual information, with language playing a relatively minor modulatory role.

We further probe semantic organization via template-based retrieval. For each semantic category (e.g., the verb ``push'' or the noun ``room''), we collect 30 images from COCO containing that category, average their VLA features to form a template, and compute mean Average Precision (mAP) for retrieving other instances (see Appendix~\ref{app:sensitivity_analysis} for perturbation methodology and Appendix~\ref{app:semantic_retrieval} for retrieval metrics). Scene-level nouns achieve relatively high mAP: ``room'' (0.36), ``street'' (0.29), ``field'' (0.27). Object nouns and action verbs score substantially lower: ``picture'' (0.09), ``shirt'' (0.07), and among verbs, even high-frequency actions like ``fill'' (0.07) and ``show'' (0.05) cluster poorly. This suggests VLAs form coherent semantic clusters primarily for static scenes and objects, while action semantics remain weakly differentiated.

\subsection{Spatial Intelligence Evaluation}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{spatial_left.png}
    
    \vspace{0.5em}
    
    \includegraphics[width=0.45\textwidth]{spatial_right.png}
    \caption{The difference between the VLA affinity and the true physical distance. We do this experiment on NeRF datasets.}
    \label{fig:spatial}
\end{figure}

A critical question is whether VLAs learn geometric understanding or merely memorize appearance-pose associations. To test this, we extract VLA features from NeRF datasets where ground-truth camera poses are known, and train a 3-layer MLP to regress 6-DOF poses (position and orientation) from these features (see Appendix~\ref{app:pose_regression} for architecture and training details). On the training set, the probe achieves nearly perfect performance with loss 0.003, demonstrating that pose information is indeed encoded. However, on held-out test views, the loss remains at 0.998—complete failure to generalize. We also perform multi-dimensional scaling (MDS) on pairwise feature distances and compare to ground-truth pose distances (Appendix~\ref{app:affinity_analysis}). For nearby viewpoints in 3D space, we find no corresponding proximity in feature space, confirming that VLAs do not learn geometric relationships between views.

This finding has significant implications. VLAs can memorize which visual appearances correspond to which spatial configurations in their training data, but they lack the geometric inductive biases needed to reason about unseen viewpoints, occlusions, or multi-step assembly tasks requiring spatial planning. Future architectures may benefit from incorporating explicit 3D representations or contrastive losses that enforce view-invariant embeddings.

\subsection{Gradient Flow and Attribution}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{gradient.png} 
    \caption{The visualization results of our channel tracing methods. We can see that we can find certain channels that can contribute to certain clusters obtained from NCut.}
    \label{fig:website}
\end{figure}

To understand which features drive action predictions, we compute gradients of the action loss $\mathcal{L}$ with respect to individual token features: $\partial \mathcal{L} / \partial \mathbf{f}_i$ (see Appendix~\ref{app:gradient_attribution} for implementation). Gradient magnitudes concentrate heavily on object and gripper clusters, with background regions exhibiting near-zero attribution. This confirms that action prediction relies primarily on task-relevant semantic regions, ignoring irrelevant context.

We also examine which NCut eigenvectors contribute most to action prediction by analyzing the correlation between eigenvector coefficients and action output variations. Lower-frequency eigenvectors, corresponding to coarse semantic partitions, show strong correlations with action changes. Higher-frequency eigenvectors capturing fine texture details have minimal impact. This suggests that VLA action heads primarily operate on coarse object-level segmentation rather than fine-grained visual features, which may explain robustness to appearance variations but also fragility to subtle geometric misalignments.

\subsection{Simulation Validation}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{libero_fail1.png} 
    \caption{The visualization results when actually executing the policy in the LIBERO simulations.}
    \label{fig:libero}
\end{figure}

We deploy OpenVLA in a simulated tabletop environment (using PyBullet) with randomized objects (cylinders, cubes, rings) and natural language pick-and-place instructions. The baseline model achieves 71\% success rate over 50 trials. By applying NCut to failure cases, we identify two recurring patterns: fragmented object clusters indicating poor segmentation, and premature overlap between gripper and obstacle clusters suggesting collision risk. We change the environment to make the objects have clearer edges and it raises the success rate to 80\%, validating that NCut-guided diagnosis translates to actionable performance improvements.

\section{NCut PyTorch Documentation}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{website.png} 
    \caption{The new website for NCut.}
    \label{fig:website}
\end{figure}

As part of this project, we developed and currently maintain the official NCut PyTorch documentation website \cite{ncutpytorch}. The site provides comprehensive tutorials covering basic NCut operations, advanced debugging workflows, and API references for all modules and functions. It also includes a model gallery demonstrating applications to vision transformers, language models, diffusion models, and multimodal systems. Built with Sphinx and hosted on ReadTheDocs with continuous integration, the documentation auto-generates from code docstrings and Jupyter notebooks, ensuring consistency across software releases. The resource has been adopted by researchers beyond robotics for interpreting foundation models and serves as an educational tool in graduate courses on interpretable machine learning.

\section{Discussion and Future Work}

Our exploration shows that NCut offers insight into the black box architectures of VLAs. While much of the decision-making process may remain obscured, this tool has enabled us to better understand and characterize a VLA model - from low-level insights on token attention, to higher-level concepts of spatial/geometric comprehension and semantic understanding of text tokens related to task instructions. 

Our findings highlight both the strengths and fundamental limitations of current VLA architectures. The strong object-centric clustering and implicit failure awareness demonstrate that these models learn useful mid-level semantic representations. However, the 10$\times$ dominance of visual features over language raises critical questions about instruction following: do VLAs genuinely ground fine-grained linguistic distinctions, or do they primarily rely on visual priors with language serving as a weak contextual signal? The spatial intelligence failure is particularly concerning for real-world deployment, as tasks like insertion, precise placement, and occlusion reasoning all require geometric understanding beyond appearance matching.

We believe that NCut may offer many avenues for continued exploration of VLAs. First, we plan to extend this analysis to OpenPi 0.5 \cite{physicalintelligence2025pi05}, which incorporates architectural improvements to OpenVLA and larger-scale training, to determine how the capabilities and limitations of VLAs may vary across architectures and training approaches. Second, we aim to study whether temporal NCut patterns align with instruction structure in multi-step tasks. For example, does the phrase ``first pick up the red block'' trigger a cluster transition before ``then place it on the blue block''? Third, integrating NCut-based early warning signals with failure recovery systems \cite{dai2024racer,duan2024aha} could enable online detection and correction.

Additional future work may also look to tracing failures across architectural submodules. For VLAs with separate vision encoders and vision-language fusion layers, NCut may allow for analysis of component independently to localize whether failures originate in visual feature extraction, language encoding, or cross-modal alignment. It may also aid as an analysis tool in development of new or improved architectural submodules. For instance, it may help characterise geometric inductive biases or auxiliary losses that enforce 3D-consistent representations - which may address the spatial generalization gap.

\section{Conclusion}

Nyström Normalized Cuts provides a practical and scalable tool for debugging robot foundation models. By visualizing feature geometry across layers and modalities, we can uncover semantic structure, diagnose failure modes, and identify architectural weaknesses that standard metrics obscure. Our analysis of OpenVLA reveals that while these models excel at object-centric perceptual reasoning, they struggle with language grounding and spatial generalization. NCut may be applied in various additional ways to those explored, and as VLAs scale toward real-world deployment, interpretability methods like NCut may prove crucial for ensuring reliability, safety, and targeted improvement.

\bibliographystyle{unsrt}
\bibliography{sample}

\onecolumn
\appendix

\section{Appendix: Implementation Details}

This appendix provides implementation details for the key analyses presented in the paper. All code is available in our repository.

\subsection{NCut Implementation}
\label{app:ncut_implementation}

The core NCut coloring function extracts features from a transformer layer and projects them to RGB space via eigendecomposition and t-SNE. We cast all operations to float32 on CPU to ensure numerical stability.

\begin{lstlisting}[caption={Core NCut coloring function for feature visualization.}]
def color_from_ncut(features: torch.Tensor, n_eig: int = 10) -> np.ndarray:
    """Run NCut on features [num_tokens, hidden] and return RGB colors.
    
    Always casts to float32 on CPU to avoid half-precision 
    CUDA ops (e.g., cdist) not implemented.
    """
    from ncut_pytorch import ncut_fn, tsne_color

    with torch.no_grad():
        f32 = features.detach().to(device="cpu", dtype=torch.float32)
        eigvecs, _ = ncut_fn(f32, n_eig=n_eig)
    colors = tsne_color(eigvecs)
    return colors.cpu().numpy()
\end{lstlisting}

For k-way partitioning with gradient tracking (used in Section~\ref{app:gradient_attribution}), we employ a differentiable variant:

\begin{lstlisting}[caption={Differentiable k-way NCut for gradient-based analysis.}]
def differentiable_kway_ncut(features_flat: torch.Tensor, n_segment: int):
    """Run NCut with gradient tracking on CPU float32.

    features_flat: [N, C] requires_grad=True will be honored.
    Returns (eigvec, kway_eigvec) both tensors on CPU.
    """
    from ncut_pytorch import ncut_fn, kway_ncut
    f32 = features_flat.to(device="cpu", dtype=torch.float32)
    eigvec, _ = ncut_fn(f32, n_eig=n_segment, track_grad=True)
    kway_eigvec = kway_ncut(eigvec)
    return eigvec, kway_eigvec
\end{lstlisting}

For batch NCut across multiple frames (co-segmentation), we concatenate features and process jointly:

\begin{lstlisting}[caption={Batched NCut for co-segmentation across frames.}]
def compute_batch_ncut(features_list, n_eig=3, device='cuda'):
    """Compute batched NCut on a list of features for co-segmentation.
    
    Args:
        features_list: list of tensors, each (N, D)
        n_eig: number of eigenvectors
    Returns:
        colors_list: list of RGB colors, each (N, 3)
    """
    all_features = []
    frame_lengths = []
    
    for feat in features_list:
        if feat.dim() == 3:
            feat = feat.squeeze(0)
        all_features.append(feat.float())
        frame_lengths.append(len(feat))
    
    # Concatenate along sequence dimension
    batched_features = torch.cat(all_features, dim=0).to(device)
    
    # Compute NCut
    ncut = Ncut(n_eig=n_eig, device=device)
    eigvecs = ncut.fit_transform(batched_features)
    
    # Normalize to 0-1 for RGB visualization
    eigvecs_norm = (eigvecs - eigvecs.min(0, keepdim=True)[0]) / \
                   (eigvecs.max(0, keepdim=True)[0] - eigvecs.min(0, keepdim=True)[0] + 1e-8)
    
    # Split colors back to individual frames
    colors_list = []
    start_idx = 0
    for length in frame_lengths:
        colors_list.append(eigvecs_norm[start_idx:start_idx + length].cpu())
        start_idx += length
    
    return colors_list
\end{lstlisting}

\subsection{Feature Extraction from OpenVLA}
\label{app:feature_extraction}

We extract vision tokens from OpenVLA's vision backbone by accessing the intermediate representations after the visual encoder processes the input images.

\begin{lstlisting}[caption={Extracting vision tokens from OpenVLA.}]
def openvla_get_vision_tokens(model, inputs, dtype, device) -> torch.Tensor:
    with torch.no_grad():
        pixel_values = inputs["pixel_values"]
        pixel_values = pixel_values.to(device, dtype=dtype)
        
        if hasattr(model, "vision_backbone"):
            vision_features = model.vision_backbone(pixel_values)
        else:
            raise ValueError("OpenVLA vision component not found")

        if hasattr(vision_features, "last_hidden_state"):
            tokens = vision_features.last_hidden_state  # [B, N, C]
        elif isinstance(vision_features, torch.Tensor):
            tokens = vision_features
        elif isinstance(vision_features, (tuple, list)):
            tokens = vision_features[0]
        else:
            raise ValueError(f"Unexpected type: {type(vision_features)}")
    return tokens
\end{lstlisting}

For language features, we retrieve hidden states from the language model backbone with pooling options:

\begin{lstlisting}[caption={Extracting and pooling language hidden states from OpenVLA.}]
def pool_hidden_states(hidden: torch.Tensor, strategy: str) -> torch.Tensor:
    """Pool hidden states for downstream analysis."""
    if hidden.ndim != 2:
        raise ValueError("Expected hidden state tensor of shape (seq_len, hidden_dim)")
    if strategy == "mean":
        return hidden.mean(dim=0)
    if strategy == "mean_std":
        mean = hidden.mean(dim=0)
        std = hidden.std(dim=0, unbiased=False)
        return torch.cat([mean, std], dim=0)
    raise ValueError(f"Unsupported pooling strategy: {strategy}")

def collect_pooled_features(model, processor, image_paths, prompt, 
                            device, autocast_dtype, pooling) -> torch.Tensor:
    """Collect pooled features from OpenVLA for a batch of images."""
    pooled_features = []
    autocast_ctx = torch.autocast(device_type="cuda", dtype=autocast_dtype) \
                   if device.type == "cuda" else nullcontext()
    
    with torch.no_grad():
        for image_path in image_paths:
            with Image.open(image_path) as img:
                image = img.convert("RGB")
            inputs = processor(text=[prompt], images=image, return_tensors="pt")
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                      for k, v in inputs.items()}
            with autocast_ctx:
                outputs = model(**inputs, output_hidden_states=True, return_dict=True)
            last_hidden = outputs.hidden_states[-1][0].to(torch.float32).cpu()
            pooled_features.append(pool_hidden_states(last_hidden, pooling))
    return torch.stack(pooled_features)
\end{lstlisting}

We also use forward hooks to capture intermediate vision encoder features:

\begin{lstlisting}[caption={Vision feature hook for capturing encoder outputs.}]
class VisionFeatureHook:
    """Hook to capture vision encoder features at any layer."""
    def __init__(self):
        self.features = None
        
    def __call__(self, module, input, output):
        if isinstance(output, tuple):
            self.features = output[0].detach()
        else:
            self.features = output.detach()

# Usage: Register hook on vision backbone's last block
hook = VisionFeatureHook()
if hasattr(model, 'vision_backbone'):
    vision = model.vision_backbone
    if hasattr(vision, 'featurizer') and hasattr(vision.featurizer, 'blocks'):
        last_block = vision.featurizer.blocks[-1]
        handle = last_block.register_forward_hook(hook)
\end{lstlisting}

\subsection{Temporal Dynamics and Eigenvector Alignment}
\label{app:temporal_alignment}

When analyzing manipulation sequences, eigenvector sign ambiguity can cause color flipping between frames. We apply temporal alignment to ensure consistent cluster colors:

\begin{lstlisting}[caption={Temporal eigenvector alignment to prevent color flipping.}]
def align_eigenvectors(curr_eig, ref_eig):
    """Align current eigenvectors to reference to ensure temporal consistency."""
    if ref_eig is None:
        return curr_eig
        
    K = curr_eig.shape[1]
    aligned_eig = curr_eig.clone()
    
    for k in range(K):
        dot_prod = torch.dot(curr_eig[:, k], ref_eig[:, k])
        if dot_prod < 0:
            aligned_eig[:, k] = -aligned_eig[:, k]
            
    return aligned_eig

def compute_aligned_ncut(features_list, n_eig=3, device='cuda'):
    """Compute NCut with temporal alignment via correlation-based sign correction."""
    # First do batch NCut
    colors_list = compute_batch_ncut(features_list, n_eig, device)
    if colors_list is None:
        return None
    
    # Apply temporal alignment
    aligned_colors = []
    prev_colors = None
    
    for colors in colors_list:
        if prev_colors is not None:
            aligned = colors.clone()
            for c in range(colors.shape[1]):
                curr_channel = colors[:, c]
                prev_channel = prev_colors[:, c]
                # Check correlation and flip if negative
                corr = torch.corrcoef(torch.stack([curr_channel, prev_channel]))[0, 1]
                if corr < 0:
                    aligned[:, c] = 1.0 - aligned[:, c]
            aligned_colors.append(aligned)
            prev_colors = aligned
        else:
            aligned_colors.append(colors)
            prev_colors = colors
    
    return aligned_colors
\end{lstlisting}

\subsection{Input Sensitivity Analysis}
\label{app:sensitivity_analysis}

To quantify vision-vs-language sensitivity (Section 4.4), we define systematic prompt perturbations and measure feature-space shifts:

\begin{lstlisting}[caption={Prompt perturbation variants for sensitivity analysis.}]
BASE_PROMPT = "In: What action should the robot take to push the trees away from the wall?\nOut:"

PROMPT_VARIANTS = {
    "base": BASE_PROMPT,
    "verb_swap": BASE_PROMPT.replace("push the trees away from the wall", 
                                      "pull the trees towards the robot"),
    "noun_swap": BASE_PROMPT.replace("trees", "boxes"),
    "neutral": "In: Describe the scene in detail.\nOut:",
    "short": "In: Action?\nOut:",
}

def summarize_differences(base: np.ndarray, variant: np.ndarray) -> Dict[str, float]:
    """Compute feature-space distance metrics between baseline and variant."""
    diff = variant - base
    base_norm = np.linalg.norm(base, axis=1) + 1e-6
    variant_norm = np.linalg.norm(variant, axis=1) + 1e-6
    l2 = np.linalg.norm(diff, axis=1)
    rel = l2 / base_norm
    cosine = np.sum(base * variant, axis=1) / (base_norm * variant_norm)
    return {
        "mean_l2": float(np.mean(l2)),
        "median_l2": float(np.median(l2)),
        "p90_l2": float(np.percentile(l2, 90)),
        "mean_relative_l2": float(np.mean(rel)),
        "mean_cosine": float(np.mean(cosine)),
    }

def cross_image_baseline(features: np.ndarray, num_pairs: int = 1000) -> Dict[str, float]:
    """Compute baseline distances between random image pairs for reference."""
    rng = np.random.default_rng(seed=0)
    n = features.shape[0]
    idx_a = rng.integers(0, n, size=num_pairs)
    idx_b = rng.integers(0, n, size=num_pairs)
    diff = features[idx_a] - features[idx_b]
    distances = np.linalg.norm(diff, axis=1)
    return {
        "mean_cross_image_l2": float(np.mean(distances)),
        "median_cross_image_l2": float(np.median(distances)),
    }
\end{lstlisting}

\subsection{Semantic Retrieval Metrics}
\label{app:semantic_retrieval}

For the template-based semantic retrieval analysis (Section 4.4), we compute Average Precision and Precision/Recall@K:

\begin{lstlisting}[caption={Semantic retrieval metrics for verb/noun alignment analysis.}]
def average_precision(y_true: np.ndarray, scores: np.ndarray) -> float:
    """Compute average precision for retrieval evaluation."""
    positives = y_true.sum()
    if positives == 0:
        return float("nan")
    order = np.argsort(-scores)
    sorted_true = y_true[order]
    cumulative = np.cumsum(sorted_true)
    precision = cumulative / (np.arange(len(sorted_true)) + 1)
    ap = (precision * sorted_true).sum() / positives
    return float(ap)

def precision_at_k(y_true: np.ndarray, scores: np.ndarray, k: int) -> float:
    order = np.argsort(-scores)[:k]
    return float(y_true[order].sum() / max(k, 1))

def recall_at_k(y_true: np.ndarray, scores: np.ndarray, k: int) -> float:
    positives = y_true.sum()
    if positives == 0:
        return float("nan")
    order = np.argsort(-scores)[:k]
    return float(y_true[order].sum() / positives)

def evaluate_word_retrieval(word, normalized_features, per_image_word_sets):
    """Evaluate prototype-based retrieval for a semantic category."""
    labels = np.array([1 if word in word_set else 0 
                       for word_set in per_image_word_sets], dtype=np.float32)
    if labels.sum() == 0:
        return None
    
    # Build prototype from positive examples
    mask = labels.astype(bool)
    prototype = normalized_features[mask].mean(axis=0)
    prototype /= np.linalg.norm(prototype).clip(min=1e-6)
    
    # Compute similarity scores
    scores = normalized_features @ prototype
    
    return {
        "word": word,
        "positives": int(labels.sum()),
        "average_precision": average_precision(labels, scores),
        "precision_at_10": precision_at_k(labels, scores, k=10),
        "recall_at_10": recall_at_k(labels, scores, k=10),
    }
\end{lstlisting}

\subsection{Spatial Intelligence: Pose Regression}
\label{app:pose_regression}

For the spatial intelligence evaluation (Section 4.5), we train a lightweight MLP to regress camera poses from VLA features:

\begin{lstlisting}[caption={Pose regression MLP architecture.}]
class PoseRegressor(nn.Module):
    """MLP for regressing 6-DOF camera poses from VLA features."""
    def __init__(self, input_dim: int, target_dim: int, hidden_dim: int,
                 num_hidden_layers: int, dropout: float):
        super().__init__()
        layers = [nn.LayerNorm(input_dim)]
        in_dim = input_dim
        for _ in range(max(num_hidden_layers, 0)):
            layers.extend([
                nn.Linear(in_dim, hidden_dim),
                nn.GELU(),
            ])
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
            in_dim = hidden_dim
        layers.append(nn.Linear(in_dim, target_dim))
        self.net = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)

def flatten_pose_matrices(pose_matrices: torch.Tensor) -> torch.Tensor:
    """Flatten (N, 3, 4) pose matrices to (N, 12) for regression."""
    if pose_matrices.ndim != 3 or pose_matrices.shape[1:] != (3, 4):
        raise ValueError("Pose matrices must have shape (N, 3, 4)")
    return pose_matrices.reshape(pose_matrices.shape[0], -1)
\end{lstlisting}

\subsection{Feature-Pose Affinity Analysis}
\label{app:affinity_analysis}

To compare feature similarity structure with ground-truth pose distances:

\begin{lstlisting}[caption={RBF affinity and MDS for feature-pose comparison.}]
def rbf_affinity_from_features(features: torch.Tensor, gamma: float = 1.0) -> torch.Tensor:
    """Compute RBF (Gaussian) affinity matrix from features."""
    if features.ndim != 2:
        raise ValueError("`features` must be a 2D tensor [n_samples, dim].")
    d = torch.cdist(features, features, p=2)
    squared = torch.pow(d, 2)
    sigma = 2 * gamma * features.var(dim=0, unbiased=False).sum()
    sigma = torch.clamp(sigma, min=1e-6)
    return torch.exp(-squared / sigma)

def classical_mds(distances: torch.Tensor, output_dim: int = 2) -> np.ndarray:
    """Multi-dimensional scaling for embedding visualization."""
    dist_np = distances.cpu().numpy()
    sq_dist = dist_np**2
    n = sq_dist.shape[0]
    centering = np.eye(n) - np.ones((n, n)) / n
    gram = -0.5 * centering @ sq_dist @ centering
    eigvals, eigvecs = np.linalg.eigh(gram)
    idx = np.argsort(eigvals)[::-1]
    eigvals = eigvals[idx]
    eigvecs = eigvecs[:, idx]
    positive = eigvals > 0
    eigvals = eigvals[positive][:output_dim]
    eigvecs = eigvecs[:, positive][:, :output_dim]
    coords = eigvecs * np.sqrt(np.maximum(eigvals, 0))
    return coords
\end{lstlisting}

\subsection{Gradient Attribution for Clusters}
\label{app:gradient_attribution}

To compute which feature channels contribute most to a given NCut cluster (Section 4.6), we backpropagate through the k-way eigenvector assignments:

\begin{lstlisting}[caption={Computing channel gradients for cluster attribution.}]
def channel_gradient_from_cluster(features_flat: torch.Tensor, 
                                   cluster_mask_flat: torch.Tensor, 
                                   kway_eigvec: torch.Tensor, 
                                   cluster_idx: int) -> torch.Tensor:
    """Compute average channel gradient for a specific cluster.

    features_flat: [N, C] with requires_grad=True (CPU float32)
    cluster_mask_flat: [N] bool tensor (CPU)
    returns grad: [C] (CPU)
    """
    assert features_flat.requires_grad is True
    if features_flat.grad is not None:
        features_flat.grad.zero_()
    
    cluster_mask_flat = cluster_mask_flat.to(device=features_flat.device)
    loss = -kway_eigvec[cluster_mask_flat, cluster_idx].abs().mean()
    loss.backward(retain_graph=True)
    grad = features_flat.grad[cluster_mask_flat].mean(0)
    return grad.detach()
\end{lstlisting}

\subsection{Entropy Metrics for Layer Analysis}
\label{app:entropy_metrics}

To quantify information flow between layers, we compute entropy-based metrics on NCut cluster assignments:

\begin{lstlisting}[caption={Entropy and mutual information for cross-layer analysis.}]
def compute_entropy_metrics(features1, features2, n_eig=20, mode='discrete'):
    """Compute conditional entropy and mutual information between two layers.
    
    All entropy values are measured in bits (using log2).
    """
    from ncut_pytorch import Ncut, kway_ncut
    from ncut_pytorch.utils.math import rbf_affinity
    from scipy.optimize import linear_sum_assignment
    
    # Process features through NCut pipeline
    h1, w1, d1 = features1.shape
    flattened1 = features1.reshape(h1 * w1, d1)
    aligned_features1 = rbf_affinity(flattened1)
    eigvecs1 = Ncut(n_eig=n_eig).fit_transform(aligned_features1)
    kway_eigvecs1 = kway_ncut(eigvecs1)
    
    h2, w2, d2 = features2.shape
    flattened2 = features2.reshape(h2 * w2, d2)
    aligned_features2 = rbf_affinity(flattened2)
    eigvecs2 = Ncut(n_eig=n_eig).fit_transform(aligned_features2)
    kway_eigvecs2 = kway_ncut(eigvecs2)
    
    # Convert to numpy and compute joint distribution
    kway1 = kway_eigvecs1.numpy()
    kway2 = kway_eigvecs2.numpy()
    N, K = kway1.shape
    eps = 1e-10
    
    def H(p):  # Entropy in bits
        p = np.clip(p, eps, 1.0)
        return -np.sum(p * np.log2(p))
    
    labels1 = np.argmax(kway1, axis=1)
    labels2 = np.argmax(kway2, axis=1)
    
    # Build confusion matrix -> joint distribution
    confusion = np.zeros((K, K), dtype=np.float64)
    for a, b in zip(labels1, labels2):
        confusion[a, b] += 1.0
    P_joint = confusion / float(N)
    P1 = P_joint.sum(axis=1)
    P2 = P_joint.sum(axis=0)
    
    H1 = H(P1)
    H2 = H(P2)
    H_joint = H(P_joint.reshape(-1))
    MI = H1 + H2 - H_joint
    
    return {
        'entropy_layer1': H1,
        'entropy_layer2': H2,
        'conditional_entropy_1_given_2': H_joint - H2,
        'conditional_entropy_2_given_1': H_joint - H1,
        'mutual_information': MI,
    }
\end{lstlisting}

This gradient computation enables us to identify which feature dimensions are most associated with each semantic cluster, revealing that lower-frequency eigenvectors (capturing coarse semantic partitions) show stronger correlations with action predictions than higher-frequency components capturing fine texture details.

\end{document}
