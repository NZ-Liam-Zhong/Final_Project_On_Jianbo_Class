#!/usr/bin/env python
"""Export large-scale noun/verb alignment rankings on COCO captions."""

from __future__ import annotations

import argparse
import json
from collections import Counter
from pathlib import Path
from typing import Dict, List, Sequence, Set, Tuple

import numpy as np
import spacy
import torch


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Rank noun/verb alignment scores on COCO captions")
    parser.add_argument(
        "--coco-root",
        type=Path,
        default=Path("/mnt/disk1/ilykyleliam/public/datasets/coco2017"),
        help="Path to COCO 2017 directory containing val2017/ and annotations/",
    )
    parser.add_argument(
        "--feature-cache",
        type=Path,
        default=Path("experiments/coco_attribute_probe/val_openvla_features.pt"),
        help="Feature cache generated by coco_attribute_probe.py",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("experiments/coco_word_alignment"),
        help="Directory to save ranked alignment tables",
    )
    parser.add_argument(
        "--max-words",
        type=int,
        default=200,
        help="Maximum number of nouns/verbs to keep in the ranking (per POS category)",
    )
    parser.add_argument(
        "--min-count",
        type=int,
        default=20,
        help="Minimum number of positive images required to include a word",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=10,
        help="k used for precision@k / recall@k metrics",
    )
    parser.add_argument(
        "--pos-filter",
        choices=["nouns", "verbs", "both"],
        default="both",
        help="Whether to export nouns, verbs, or both",
    )
    parser.add_argument("--seed", type=int, default=17, help="Random seed for reproducibility")
    return parser.parse_args()


def load_captions(annotation_path: Path) -> Dict[int, List[str]]:
    with annotation_path.open("r", encoding="utf-8") as f:
        data = json.load(f)
    id_to_captions: Dict[int, List[str]] = {}
    for ann in data["annotations"]:
        id_to_captions.setdefault(ann["image_id"], []).append(ann["caption"])
    return id_to_captions


def gather_word_sets(
    image_ids: Sequence[int],
    id_to_captions: Dict[int, List[str]],
    nlp,
) -> Tuple[List[Set[str]], List[Set[str]], Counter, Counter]:
    noun_sets: List[Set[str]] = []
    verb_sets: List[Set[str]] = []
    noun_counter: Counter[str] = Counter()
    verb_counter: Counter[str] = Counter()

    for image_id in image_ids:
        captions = id_to_captions.get(image_id, [])
        doc = nlp(" ".join(captions))
        nouns: Set[str] = set()
        verbs: Set[str] = set()
        for token in doc:
            if not token.text.isalpha() or token.is_stop:
                continue
            lemma = token.lemma_.lower()
            if token.pos_ in {"NOUN", "PROPN"}:
                nouns.add(lemma)
            elif token.pos_ == "VERB":
                verbs.add(lemma)
        noun_sets.append(nouns)
        verb_sets.append(verbs)
        noun_counter.update(nouns)
        verb_counter.update(verbs)

    return noun_sets, verb_sets, noun_counter, verb_counter


def average_precision(y_true: np.ndarray, scores: np.ndarray) -> float:
    positives = y_true.sum()
    if positives == 0:
        return float("nan")
    order = np.argsort(-scores)
    sorted_true = y_true[order]
    cumulative = np.cumsum(sorted_true)
    precision = cumulative / (np.arange(len(sorted_true)) + 1)
    ap = (precision * sorted_true).sum() / positives
    return float(ap)


def precision_at_k(y_true: np.ndarray, scores: np.ndarray, k: int) -> float:
    if k <= 0:
        return float("nan")
    order = np.argsort(-scores)[:k]
    return float(y_true[order].sum() / k)


def recall_at_k(y_true: np.ndarray, scores: np.ndarray, k: int) -> float:
    positives = y_true.sum()
    if positives == 0:
        return float("nan")
    order = np.argsort(-scores)[:k]
    return float(y_true[order].sum() / positives)


def evaluate_words(
    word_list: List[str],
    per_image_sets: List[Set[str]],
    normalized_features: np.ndarray,
    top_k: int,
) -> List[dict]:
    results = []
    for word in word_list:
        labels = np.array([1 if word in s else 0 for s in per_image_sets], dtype=np.float32)
        if labels.sum() == 0:
            continue
        prototype = normalized_features[labels.astype(bool)].mean(axis=0)
        norm = np.linalg.norm(prototype)
        if norm < 1e-6:
            continue
        prototype /= norm
        scores = normalized_features @ prototype
        ap = average_precision(labels, scores)
        p_at_k = precision_at_k(labels, scores, top_k)
        r_at_k = recall_at_k(labels, scores, top_k)
        results.append(
            {
                "word": word,
                "positives": int(labels.sum()),
                "average_precision": ap,
                "precision_at_k": p_at_k,
                "recall_at_k": r_at_k,
            }
        )
    return results


def sort_and_truncate(results: List[dict], max_words: int) -> List[dict]:
    def key_fn(entry: dict) -> float:
        ap = entry.get("average_precision", float("nan"))
        if np.isnan(ap):
            return -float("inf")
        return ap

    ranked = sorted(results, key=key_fn, reverse=True)
    if max_words is not None:
        ranked = ranked[:max_words]
    return ranked


def main() -> None:
    args = parse_args()
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)

    if not args.feature_cache.exists():
        raise FileNotFoundError(f"Feature cache not found: {args.feature_cache}")
    payload = torch.load(args.feature_cache)
    features = payload["features"].clone().to(torch.float32)
    image_ids: List[int] = payload["image_ids"]

    norms = torch.norm(features, dim=1, keepdim=True).clamp_min(1e-6)
    normalized = (features / norms).cpu().numpy()

    coco_root = args.coco_root
    captions_path = coco_root / "annotations" / "captions_val2017.json"
    if not captions_path.exists():
        raise FileNotFoundError(f"Missing COCO captions file: {captions_path}")
    id_to_captions = load_captions(captions_path)

    nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])

    noun_sets, verb_sets, noun_counter, verb_counter = gather_word_sets(image_ids, id_to_captions, nlp)

    noun_words = [
        word for word, count in noun_counter.most_common() if count >= args.min_count
    ]
    verb_words = [
        word for word, count in verb_counter.most_common() if count >= args.min_count
    ]

    output: Dict[str, List[dict]] = {}

    if args.pos_filter in {"nouns", "both"}:
        noun_metrics = evaluate_words(noun_words, noun_sets, normalized, args.top_k)
        output["nouns"] = sort_and_truncate(noun_metrics, args.max_words)

    if args.pos_filter in {"verbs", "both"}:
        verb_metrics = evaluate_words(verb_words, verb_sets, normalized, args.top_k)
        output["verbs"] = sort_and_truncate(verb_metrics, args.max_words)

    output_dir = args.output_dir
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / "word_alignment_rankings.json"
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(
            {
                "rankings": output,
                "config": {
                    "feature_cache": str(args.feature_cache),
                    "max_words": args.max_words,
                    "min_count": args.min_count,
                    "top_k": args.top_k,
                    "pos_filter": args.pos_filter,
                },
            },
            f,
            indent=2,
        )
    print(f"Saved rankings to {output_path}")


if __name__ == "__main__":
    main()



