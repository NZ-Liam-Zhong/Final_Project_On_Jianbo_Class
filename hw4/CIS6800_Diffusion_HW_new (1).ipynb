{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyqfM87ha-av"
      },
      "source": [
        "# CIS 6800 Project: VAE and Diffusion Transformer\n",
        "\n",
        "Instructions:\n",
        "\n",
        "*  In this HW you will implement a patch-based VAE and train a Diffusion model using the pre-trained VAE checkpoint.\n",
        "*  We provided you with a zipfile that contains a subset of 10,000 images from the CelebA dataset, along with an attribute file that includes binary labels for each image.\n",
        "*  **Please submit your ipynb notebook as well as a pdf version of it. Include all visualizations and answers to the questions.**\n",
        "*   Part A is due Wed 11/5, and Part B is due Wed 11/12.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Diffusion Transformer Paper: [https://arxiv.org/pdf/2212.09748](https://arxiv.org/pdf/2212.09748)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AGOFTvUDfZPg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch torchvision numpy tqdm datasets torch-ema pytorch-lightning timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Jk8k1y5JfGTM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
        "from torchvision import transforms as tf\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from torch_ema import ExponentialMovingAverage as EMA\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange, repeat  # You can learn about einops at https://einops.rocks\n",
        "from itertools import pairwise\n",
        "from accelerate import Accelerator\n",
        "from types import SimpleNamespace\n",
        "from typing import Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "\n",
        "import zipfile\n",
        "import io\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BfMhHvCsft4r"
      },
      "outputs": [],
      "source": [
        "# Set the seed for reproducibility\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj1sSnbJpMpj"
      },
      "source": [
        "# Part A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS6ZZ5oKU6mO"
      },
      "source": [
        "# Variational Autoencoder (VAE)\n",
        "\n",
        "\n",
        "In this part of the project you will train a Patch-based VAE to capture key facial features on on a subset of the CelebA dataset, consisting of 10000 face images. You can use the code below to load images from the given zipfile. We recommend resizing the images to (32, 32) for the following tasks but you're welcome to experiment with different sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aXd8vlpSgFA0"
      },
      "outputs": [],
      "source": [
        "class CelebADataset(Dataset):\n",
        "    def __init__(self, zip_file, attr_file, selected_attrs=None, transform=None):\n",
        "        self.zip_file = zip_file\n",
        "        self.attr_file = attr_file\n",
        "        self.transform = transform\n",
        "\n",
        "        # Open the zip file and get the list of images\n",
        "        self.attr_table, self.attr_names = self._load_attr_table(\n",
        "            attr_file=self.attr_file,\n",
        "            selected_attrs=selected_attrs\n",
        "        )\n",
        "\n",
        "        self.zip = zipfile.ZipFile(self.zip_file, 'r')\n",
        "        self.image_list = [file for file in self.zip.namelist() if file.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image name from the list\n",
        "        path_in_zip = self.image_list[idx]\n",
        "        basename = path_in_zip.split('/')[-1]\n",
        "        try:\n",
        "            # Read image data from the zip file\n",
        "            with self.zip.open(path_in_zip) as f:\n",
        "                img_bytes = f.read()\n",
        "                img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            # Read annotations from the attr_file\n",
        "            attrs = self.attr_table[basename]\n",
        "        except zipfile.BadZipFile:\n",
        "            return None\n",
        "        except Exception:\n",
        "            return None\n",
        "        return img, attrs\n",
        "\n",
        "    def _load_attr_table(self, attr_file, selected_attrs=None):\n",
        "        with open(attr_file, \"r\") as f:\n",
        "            lines = [line.rstrip(\"\\n\") for line in f if line.strip()]\n",
        "\n",
        "        # Format: line0: count, line1: 40 names, then <fname> v1 ... v40\n",
        "        all_attr_names = lines[1].split()\n",
        "        name_to_idx = {n: i for i, n in enumerate(all_attr_names)}\n",
        "\n",
        "        for a in selected_attrs:\n",
        "            if a not in name_to_idx:\n",
        "                raise ValueError(f\"Attribute '{a}' not found in CelebA attributes.\")\n",
        "\n",
        "        sel_idx = [name_to_idx[a] for a in selected_attrs]\n",
        "\n",
        "        table = {}\n",
        "        for line in lines[2:]:\n",
        "            parts = line.split()\n",
        "            fname = parts[0]\n",
        "            vals = np.array(list(map(int, parts[1:])), dtype=np.int16)\n",
        "            sel  = vals[sel_idx]\n",
        "            sel01 = ((sel + 1) // 2).astype(np.float32)\n",
        "            table[fname] = torch.from_numpy(sel01)\n",
        "\n",
        "        return table, selected_attrs\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Filter out None values (e.g., images that couldn't be loaded)\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    # If the batch is empty after filtering, return None (can be skipped by DataLoader)\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "# Usage of the dataset and dataloader\n",
        "def get_celeba_dataloader(zip_path, attr_file, batch_size=32, image_size=(32, 32), selected_attrs=None):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    train_dataset, valid_dataset = random_split(CelebADataset(zip_path, attr_file=attr_file, selected_attrs=selected_attrs, transform=transform), (0.8, 0.2))\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
        "    return train_dataloader, valid_dataloader\n",
        "\n",
        "\n",
        "# Function to display original and reconstructed images (4 images only)\n",
        "def show_original_reconstructed(orig_images, recon_images, epoch):\n",
        "    # Move the images back to CPU and denormalize\n",
        "    orig_images = orig_images.cpu().numpy()\n",
        "    recon_images = recon_images.cpu().numpy()\n",
        "\n",
        "    # Clip the values to the valid range [0, 1] for display\n",
        "    orig_images = np.clip(orig_images * 0.5 + 0.5, 0, 1)  # Denormalize and clip\n",
        "    recon_images = np.clip(recon_images * 0.5 + 0.5, 0, 1)  # Denormalize and clip\n",
        "\n",
        "    # Plot images side by side (4 images)\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(10, 4))\n",
        "    for i in range(4):\n",
        "        # Original image\n",
        "        axes[0, i].imshow(orig_images[i].transpose(1, 2, 0))  # Correct shape for imshow\n",
        "        axes[0, i].axis('off')\n",
        "        axes[0, i].set_title(\"Original\")\n",
        "\n",
        "        # Reconstructed image\n",
        "        axes[1, i].imshow(recon_images[i].transpose(1, 2, 0))  # Correct shape for imshow\n",
        "        axes[1, i].axis('off')\n",
        "        axes[1, i].set_title(\"Reconstructed\")\n",
        "\n",
        "    plt.suptitle(f'Epoch {epoch}: Original vs Reconstructed')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7tEj3hebptH_"
      },
      "outputs": [],
      "source": [
        "#Initialize your dataloaders here\n",
        "zip_file_path = \"celeba_10000_images.zip\"\n",
        "attr_file_path = \"list_attr_celeba.txt\"\n",
        "ATTR_ORDER = [\"Male\"]\n",
        "\n",
        "train_dataloader, valid_dataloader = get_celeba_dataloader(zip_file_path, attr_file_path, batch_size=64, image_size=(32, 32), selected_attrs=ATTR_ORDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GiyNPlynmVe"
      },
      "source": [
        "### Patch-based Variational Autoencoder\n",
        "\n",
        "PatchVAE divides an input image into smaller patches and operates on these patches rather than the entire image at once. By processing images in patches, the PatchVAE learns to capture fine-grained (local) and high-level (global) patterns, depending on the patch size. In this section you should:\n",
        "\n",
        "1.   Implement the PatchEmbed Class. This class is designed to create overlapping patches from an image, embed those patches into a latent space, and provide a method to reconstruct the image from the latent representations.\n",
        "\n",
        "2.   Implement the PatchVAE model.\n",
        "\n",
        "    *   The `encode` method convert patch embeddings into latent variables (mu and logvar) through a convolutional encoder. Rearrange patches to treat them as channels before passing them through the encoder.\n",
        "    *   The `reparameterize` method applied the reparameterization trick `z = mu + eps * std` where `eps` is sampled from a standard normal distribution.\n",
        "    *  The `decode` method convert the latent variable z back into patch embeddings, then reconstruct the original image from these patches\n",
        "    *  The `forward` method first patchifies the image, encode, reparameterize, decode, and finally reconstruct the image\n",
        "    *  The `compute_loss` calculates the reconstruction loss (Mean Squared Error) and the KL divergence loss to encourage the latent space to follow a normal distribution. Combine these losses to get the total VAE loss.\n",
        "    *  The `sample` method generates new random images from the learned latent space. Random latent vector `z` are sampled, decoded into patches, and reconstructed into full images.\n",
        "\n",
        "\n",
        "3.   Train the model on the faces dataset. **Experiment with different patch sizes and report the patch size with the best tradeoff between low-level details and high-level texture. Plot the training loss and visualize a couple reconstructed images during training.**\n",
        "\n",
        "4. After training, generate sample images from the latent space. Specifically, you need to sample latent vectors from a normal distribution, decode the latent varaibles, and reconstruct the images. **Visualize 4 generated examples. Do the generated images resemble realistic human faces? Explain in a couple sentences.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KFJY8FNIghTb"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=128, patch_size=32, stride=8, channels=3, embed_dim=128, bias=True):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Use Conv2D to create image patches of size (patch_size, patch_size) with overlapping regions.\n",
        "\n",
        "        Each patch should have embedding size embed_dim.\n",
        "        \"\"\"\n",
        "        self.patch_size = patch_size\n",
        "        self.stride = stride\n",
        "        self.embed_dim = embed_dim\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Conv2d to generate overlapping patches (from image to latent space)\n",
        "        self.proj = nn.Conv2d(channels, embed_dim, kernel_size=patch_size, stride=stride, bias=bias)\n",
        "\n",
        "        # Transposed Conv2d to reconstruct patches from latent space to RGB (from latent to image space)\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(embed_dim, 3, kernel_size=patch_size, stride=stride),\n",
        "            nn.Tanh())\n",
        "        H_out = (img_size - self.patch_size) // self.stride + 1\n",
        "        W_out = (img_size - self.patch_size) // self.stride + 1\n",
        "        self.num_patches = H_out * W_out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Input x is an image of size [B, C, img_size, img_size]\n",
        "\n",
        "        Return patches of size [B, num_patches, embed_dim]\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # Apply convolution to create patches: [B, C, H, W] -> [B, embed_dim, H_out, W_out]\n",
        "        x = self.proj(x)\n",
        "        # Flatten spatial dimensions: [B, embed_dim, H_out, W_out] -> [B, embed_dim, num_patches]\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.reshape(B, C, H * W)\n",
        "        # Transpose: [B, embed_dim, num_patches] -> [B, num_patches, embed_dim]\n",
        "        patches = x.transpose(1, 2)\n",
        "        ######## END TODO ########\n",
        "\n",
        "        return patches\n",
        "\n",
        "    def reconstruct(self, patches, img_size):\n",
        "        \"\"\"\n",
        "        Reconstruct the image from the patches by averaging overlapping regions.\n",
        "        Input patches: [B, num_patches, embed_dim]\n",
        "        img_size: (img_size, img_size)  # original size of the input image\n",
        "        Output images: [B, img_size, img_size]\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # Reshape patches: [B, num_patches, embed_dim] -> [B, embed_dim, H_out, W_out]\n",
        "        B, num_patches, embed_dim = patches.shape\n",
        "        H_out = W_out = int(num_patches ** 0.5)\n",
        "        patches = patches.transpose(1, 2).reshape(B, embed_dim, H_out, W_out)\n",
        "\n",
        "        # Apply transposed convolution to reconstruct image\n",
        "        reconstructed_image = self.deconv(patches)\n",
        "        ######## END TODO ########\n",
        "\n",
        "        return reconstructed_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d3Shm0MZGWji"
      },
      "outputs": [],
      "source": [
        "class PatchVAE(nn.Module):\n",
        "    def __init__(self, patch_size, img_channels, img_size,\n",
        "                  embed_dim=1024, latent_dim=512, stride=8):\n",
        "        super(PatchVAE, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.img_size = img_size\n",
        "        self.H_out = (self.img_size - self.patch_size) // stride + 1\n",
        "        self.W_out = (self.img_size - self.patch_size) // stride + 1\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Patch embedding layer (Patchify the image)\n",
        "        self.patch_embed = PatchEmbed(patch_size=patch_size, stride=stride, channels=img_channels, embed_dim=embed_dim)\n",
        "        self.num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(embed_dim, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.conv_mu = nn.Conv2d(128, latent_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv_logvar = nn.Conv2d(128, latent_dim, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_input = nn.Conv2d(latent_dim, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.ConvTranspose2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.ConvTranspose2d(256, embed_dim, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def encode(self, patches):\n",
        "        \"\"\"\n",
        "        Encode the patch embeddings into latent space (mu and logvar).\n",
        "        Args:\n",
        "            patches: Patch embeddings of shape [B, num_patches, embed_dim].\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # Rearrange patches to treat them as channels: [B, num_patches, embed_dim] -> [B, embed_dim, H_out, W_out]\n",
        "        B = patches.shape[0]\n",
        "        patches = patches.transpose(1, 2)  # [B, embed_dim, num_patches]\n",
        "        patches = patches.reshape(B, self.embed_dim, self.H_out, self.W_out)\n",
        "\n",
        "        # Pass through encoder\n",
        "        x = self.encoder(patches)\n",
        "        mu = self.conv_mu(x)\n",
        "        logvar = self.conv_logvar(x)\n",
        "        ######## END TODO ########\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"\n",
        "        Reparameterization trick to sample from N(mu, var) using N(0,1).\n",
        "        Args:\n",
        "            mu: Mean of the latent distribution.\n",
        "            logvar: Log variance of the latent distribution.\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # std = torch.exp(0.5 * logvar)\n",
        "        var = torch.clamp(logvar.exp(), min=1e-7, max=1e7)\n",
        "        std = var.pow(0.5)\n",
        "        eps = torch.randn_like(std)\n",
        "        ######## END TODO ########\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"\n",
        "        Decode the latent variable z back to patch embeddings.\n",
        "        Args:\n",
        "            z: Latent variable of shape [B, latent_dim, 1, num_patches].\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # Pass through decoder\n",
        "        x = self.decoder_input(z)\n",
        "        patch_recon = self.decoder(x)\n",
        "        ######## END TODO ########\n",
        "        return rearrange(patch_recon, 'b c h w -> b (h w) c')  # Back to (B, num_patches, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the VAE. Patchify the input, encode into latent space, reparameterize, and decode.\n",
        "        Args:\n",
        "            x: Input image of shape [B, C, img_size, img_size].\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # Patchify the input image\n",
        "        patches = self.patch_embed(x)\n",
        "\n",
        "        # Encode to latent space\n",
        "        mu, logvar = self.encode(patches)\n",
        "\n",
        "        # Reparameterize\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Decode\n",
        "        patch_recon = self.decode(z)\n",
        "\n",
        "        # Reconstruct image from patches\n",
        "        recon_image = self.patch_embed.reconstruct(patch_recon, self.img_size)\n",
        "        ######## END TODO ########\n",
        "\n",
        "        return recon_image, mu, logvar\n",
        "\n",
        "    def compute_loss(self, recon_image, original_image, mu, logvar):\n",
        "        \"\"\"\n",
        "        Compute the VAE loss, which consists of the reconstruction loss and KL divergence.\n",
        "        Args:\n",
        "            recon_image: Reconstructed image.\n",
        "            original_image: Original input image.\n",
        "            mu: Mean of the latent distribution.\n",
        "            logvar: Log variance of the latent distribution.\n",
        "        Returns:\n",
        "            loss (Tensor): Total loss (reconstruction loss + KL divergence).\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # Reconstruction loss (Mean Squared Error)\n",
        "        recon_loss = F.mse_loss(recon_image, original_image, reduction='sum') / original_image.size(0)\n",
        "        # KL divergence loss (mean over batch for stable scaling)\n",
        "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / mu.size(0)        ######## END TODO ########\n",
        "        return recon_loss, kl_loss\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        \"\"\"\n",
        "        Generate random samples from the learned distribution.\n",
        "        Args:\n",
        "            num_samples (int): Number of samples to generate.\n",
        "        Returns:\n",
        "            samples (Tensor): Generated\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # Sample random latent vectors from standard normal distribution\n",
        "        z = torch.randn(num_samples, self.latent_dim, self.H_out, self.W_out).to(self.device)\n",
        "\n",
        "        # Decode the latent vectors to patch embeddings\n",
        "        with torch.no_grad():\n",
        "            patch_samples = self.decode(z)\n",
        "\n",
        "            # Reconstruct images from patches\n",
        "            sample_images = self.patch_embed.reconstruct(patch_samples, self.img_size)\n",
        "        ######## END TODO ########\n",
        "        return sample_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EVwMoGte_bfc"
      },
      "outputs": [],
      "source": [
        "def train_patchvae(model, train_dataloader, valid_dataloader, optimizer, device, epochs=10, print_interval=100, checkpoint_path='best_model.pth'):\n",
        "    \"\"\"\n",
        "    Training loop for the PatchVAE model with visualization of reconstructed images and saving the best model checkpoint.\n",
        "\n",
        "    Args:\n",
        "        model: The PatchVAE model to be trained.\n",
        "        dataloader: Dataloader for the training data.\n",
        "        optimizer: Optimizer for updating the model parameters.\n",
        "        device: Device (CPU or GPU) on which the training will run.\n",
        "        epochs: Number of training epochs.\n",
        "        print_interval: Interval at which the loss will be printed during training.\n",
        "        checkpoint_path: Path to save the best model checkpoint.\n",
        "\n",
        "    Returns:\n",
        "        losses: List of training losses for each epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    losses = []\n",
        "    best_loss = float('inf')  # Initialize with a large value\n",
        "    initial_beta = 0.0005  # Start with a small KL weight\n",
        "    final_beta = 1.0    # Gradually increase to full KL weight\n",
        "\n",
        "    for epoch, beta in zip(range(epochs), torch.linspace(initial_beta, final_beta, epochs)):\n",
        "        for batch in train_dataloader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "            else:\n",
        "                ims, _ = batch\n",
        "    ######## BEGIN TODO ########\n",
        "                ims = ims.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                recon_image, mu, logvar = model(ims)\n",
        "\n",
        "                # Compute loss\n",
        "                recon_loss, kl_loss = model.compute_loss(recon_image, ims, mu, logvar)\n",
        "                loss = recon_loss + beta * kl_loss\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                losses.append(loss.item())\n",
        "\n",
        "        # Validation and visualization every epoch\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for batch in valid_dataloader:\n",
        "                    if batch is None:\n",
        "                        continue\n",
        "                    val_ims, _ = batch\n",
        "                    val_ims = val_ims.to(device)\n",
        "                    recon_images, _, _ = model(val_ims)\n",
        "\n",
        "                    # Denormalize for visualization\n",
        "                    original_vis = (val_ims * 0.5 + 0.5).clamp(0, 1)\n",
        "                    recon_vis = (recon_images * 0.5 + 0.5).clamp(0, 1)\n",
        "\n",
        "                    visualize_reconstruction(original_vis[:4], recon_vis[:4], epoch)\n",
        "                    break\n",
        "            model.train()\n",
        "\n",
        "        # Save best model\n",
        "        avg_loss = sum(losses[-len(train_dataloader):]) / len(train_dataloader)\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f} - Model saved!')\n",
        "        else:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n",
        "    ######## END TODO ########\n",
        "    return losses\n",
        "\n",
        "def visualize_reconstruction(original_images, recon_images, epoch):\n",
        "    \"\"\"\n",
        "    Visualize original and reconstructed images side by side.\n",
        "\n",
        "    Args:\n",
        "        original_images: Batch of original images (values between 0 and 1).\n",
        "        recon_images: Batch of reconstructed images (values between 0 and 1).\n",
        "        epoch: Current epoch number.\n",
        "    \"\"\"\n",
        "    num_images = min(4, original_images.size(0))  # Visualize at most 4 images\n",
        "    fig, axes = plt.subplots(2, num_images, figsize=(12, 4))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Original images (ensure values are between 0 and 1)\n",
        "        orig_img = original_images[i].permute(1, 2, 0).cpu().numpy()  # Convert to (H, W, C)\n",
        "        orig_img = np.clip(orig_img, 0, 1)  # Clip values to [0, 1] range\n",
        "\n",
        "        # Reconstructed images (ensure values are between 0 and 1)\n",
        "        recon_img = recon_images[i].permute(1, 2, 0).cpu().numpy()  # Convert to (H, W, C)\n",
        "        recon_img = np.clip(recon_img, 0, 1)  # Clip values to [0, 1] range\n",
        "\n",
        "        # Display original images\n",
        "        axes[0, i].imshow(orig_img)\n",
        "        axes[0, i].set_title(\"Original\")\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Display reconstructed images\n",
        "        axes[1, i].imshow(recon_img)\n",
        "        axes[1, i].set_title(\"Reconstructed\")\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.suptitle(f\"Reconstruction at Epoch {epoch+1}\")\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "collapsed": true,
        "id": "4Ninfhma_ht0",
        "outputId": "e538357e-5732-4387-aa80-35ff901ca458"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resume_training:\n\u001b[32m     11\u001b[39m     vae.load_state_dict(torch.load(checkpoint_path))\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mtrain_patchvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m90\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtrain_patchvae\u001b[39m\u001b[34m(model, train_dataloader, valid_dataloader, optimizer, device, epochs, print_interval, checkpoint_path)\u001b[39m\n\u001b[32m     32\u001b[39m optimizer.zero_grad()\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m recon_image, mu, logvar = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m     38\u001b[39m recon_loss, kl_loss = model.compute_loss(recon_image, ims, mu, logvar)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Github/cis6800hw/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Github/cis6800hw/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mPatchVAE.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     95\u001b[39m patches = \u001b[38;5;28mself\u001b[39m.patch_embed(x)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Encode to latent space\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m mu, logvar = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Reparameterize\u001b[39;00m\n\u001b[32m    101\u001b[39m z = \u001b[38;5;28mself\u001b[39m.reparameterize(mu, logvar)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mPatchVAE.encode\u001b[39m\u001b[34m(self, patches)\u001b[39m\n\u001b[32m     53\u001b[39m x = \u001b[38;5;28mself\u001b[39m.encoder(patches)\n\u001b[32m     54\u001b[39m mu = \u001b[38;5;28mself\u001b[39m.conv_mu(x)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m logvar = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv_logvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m######## END TODO ########\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mu, logvar\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Github/cis6800hw/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Github/cis6800hw/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Github/cis6800hw/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Github/cis6800hw/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Train PatchVAE\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resume_training = False\n",
        "checkpoint_path = 'best_vae_model.pth'\n",
        "\n",
        "\n",
        "vae = PatchVAE(patch_size=16, img_channels=3, img_size=32, embed_dim=64, latent_dim=128).to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "\n",
        "if resume_training:\n",
        "    vae.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "train_patchvae(vae, train_dataloader, valid_dataloader, optimizer, device, epochs=90, checkpoint_path=checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKWsxg1ZtmNX"
      },
      "outputs": [],
      "source": [
        "# Generate images by sampling from latent space\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "checkpoint_path = 'best_vae_model.pth'\n",
        "vae = PatchVAE(patch_size=16, img_channels=3, img_size=32, embed_dim=64, latent_dim=128).to(device)\n",
        "vae.load_state_dict(torch.load(checkpoint_path))\n",
        "vae.eval()\n",
        "\n",
        "samples = 0.5 + 0.5 * vae.sample(4)\n",
        "samples = samples.permute(0, 2, 3, 1).cpu().numpy()\n",
        "print(\"samples min:\", np.min(samples))\n",
        "print(\"samples max:\", np.max(samples))\n",
        "\n",
        "samples = np.clip(samples, 0, 1)\n",
        "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
        "for i in range(4):\n",
        "    axes[i].imshow(samples[i])\n",
        "    axes[i].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJjs9i0npQZg"
      },
      "source": [
        "# Part B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kS2hsDujYJg"
      },
      "source": [
        "# Diffusion Transformer (DiT)\n",
        "\n",
        "\n",
        "We will train a DiT that operates in the patch latent space on the same dataset. The input image is decomposed into patches and processed by several DiT blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP4y-UwHI0xU"
      },
      "source": [
        "### Implement Noise Scheduler\n",
        "\n",
        "\n",
        "You will implement parts of the [DDPM scheduler](https://huggingface.co/docs/diffusers/en/api/schedulers/ddpm) using the following steps.\n",
        "\n",
        "1. Define $\\beta$ as a tensor of $N$ linearly spaced values between $\\beta_{start}$ and $\\beta_{end}$.\n",
        "\n",
        "$$\\beta = \\left[\\beta_{\\text{start}}, \\beta_{\\text{start}} + \\frac{\\beta_{\\text{end}} - \\beta_{\\text{start}}}{N - 1}, \\ldots, \\beta_{\\text{end}}\\right]$$\n",
        "\n",
        "2. Calculate $\\sigma$ using the cumulative product of $1 - \\beta$.\n",
        "\n",
        "$$\\sigma = \\sqrt{\\frac{1}{\\prod_{i=0}^{N-1}(1 - \\beta_i)} - 1}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1B_PlLWyHTQk"
      },
      "outputs": [],
      "source": [
        "class DDPMScheduler:\n",
        "    def __init__(self, N: int=1000, beta_start: float=0.0001, beta_end: float=0.02):\n",
        "        ######## BEGIN TODO ########\n",
        "        # Define beta as linearly spaced values\n",
        "        beta = torch.linspace(beta_start, beta_end, N)\n",
        "\n",
        "        # Calculate alpha = 1 - beta\n",
        "        alpha = 1 - beta\n",
        "\n",
        "        # Calculate cumulative product of alpha\n",
        "        alpha_cumprod = torch.cumprod(alpha, dim=0)\n",
        "\n",
        "        # Calculate sigma = sqrt(1/alpha_cumprod - 1)\n",
        "        self.sigmas = torch.sqrt(1.0 / alpha_cumprod - 1)\n",
        "        ######## END TODO ########\n",
        "\n",
        "    def __getitem__(self, i) -> torch.FloatTensor:\n",
        "        return self.sigmas[i]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sigmas)\n",
        "\n",
        "    def sample_sigmas(self, steps: int) -> torch.FloatTensor:\n",
        "        indices = list((len(self) * (1 - np.arange(0, steps)/steps)).round().astype(np.int64) - 1)\n",
        "        return self[indices + [0]]\n",
        "\n",
        "    def sample_batch(self, x0: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        batchsize = x0.shape[0]\n",
        "        return self[torch.randint(len(self), (batchsize,))].to(x0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWBzy8OKJdhW"
      },
      "source": [
        "### Implement DiT Block\n",
        "\n",
        "You will implement the DiT Block with adaLN-Zero, as illustrated in Figure 3 of the Diffusion Transformer paper. Specifically, you will need to implement Multi-Head Self-Attention, Modulation, and Adaptive Layer Norm for the DiT Block, and we will provide you with the code structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PodOL2SCae6M"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, head_dim, num_heads=8, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            head_dim (int): Dimensionality of each attention head\n",
        "            num_heads (int): Number of attention heads\n",
        "            qkv_bias (bool): Whether to include bias in the QKV layer\n",
        "        \"\"\"\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        dim = head_dim * num_heads\n",
        "\n",
        "        ######## BEGIN TODO ########\n",
        "        # Linear layer to generate query, key, value (3 * dim for Q, K, V)\n",
        "        self.qkv = nn.Linear(dim, 3 * dim, bias=qkv_bias)\n",
        "        # Output projection layer\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        ######## END TODO #######\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x (Tensor): Input tensor of shape (B, N, D), where:\n",
        "                - B: Batch size\n",
        "                - N: Number of tokens\n",
        "                - D: Dimensionality (D = num_heads * head_dim)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (B, N, D) after applying attention and projection\n",
        "\n",
        "        You may use F.scaled_dot_product_attention\n",
        "        \"\"\"\n",
        "        # (B, N, D) -> (B, N, D)\n",
        "        # N = H * W / patch_size**2, D = num_heads * head_dim\n",
        "\n",
        "        ######## BEGIN TODO ########\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        # Generate Q, K, V from input: (B, N, D) -> (B, N, 3*D) -> (B, N, 3, num_heads, head_dim)\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "        # Permute to (3, B, num_heads, N, head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each: (B, num_heads, N, head_dim)\n",
        "\n",
        "        # Apply scaled dot-product attention\n",
        "        attn_output = F.scaled_dot_product_attention(q, k, v)  # (B, num_heads, N, head_dim)\n",
        "\n",
        "        # Reshape back: (B, num_heads, N, head_dim) -> (B, N, num_heads, head_dim) -> (B, N, D)\n",
        "        attn_output = attn_output.transpose(1, 2).reshape(B, N, D)\n",
        "\n",
        "        # Apply output projection\n",
        "        output = self.proj(attn_output)\n",
        "\n",
        "        return output\n",
        "        ######## END TODO #######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JetvO5PnMjmH"
      },
      "outputs": [],
      "source": [
        "class Modulation(nn.Module):\n",
        "    def __init__(self, dim, n):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        1. self.proj is constructed as a sequence of operations:\n",
        "            - A SiLU (Sigmoid-Weighted Linear Unit) activation is applied to the input\n",
        "            - A linear layer transforms the input from dim to n * dim\n",
        "        2. The last layer's weights and biases are initialized to zero\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        self.n = n\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim, n * dim)\n",
        "        )\n",
        "        # Initialize the last layer to zero\n",
        "        nn.init.constant_(self.proj[1].weight, 0)\n",
        "        nn.init.constant_(self.proj[1].bias, 0)\n",
        "        ######## END TODO #######\n",
        "\n",
        "\n",
        "    def forward(self, y):\n",
        "        \"\"\"\n",
        "        1. self.proj(y) applies the defined projection, resulting in n * dim\n",
        "        2. Split the output tensor into n equal parts along dimension 1\n",
        "        3. Each chunk `m` has the shape dim and represents a separate modulation component\n",
        "        4. m.unsqueeze(1) adds a new dimension at index 1, necessary for future computations\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # Apply projection: (B, dim) -> (B, n * dim)\n",
        "        projected = self.proj(y)\n",
        "\n",
        "        # Split into n chunks along dimension 1: (B, n * dim) -> n tensors of (B, dim)\n",
        "        chunks = torch.chunk(projected, self.n, dim=1)\n",
        "\n",
        "        # Add dimension at index 1 for broadcasting: (B, dim) -> (B, 1, dim)\n",
        "        return tuple(m.unsqueeze(1) for m in chunks)\n",
        "        ######## END TODO #######\n",
        "\n",
        "\n",
        "class AdaptiveLayerNorm(nn.LayerNorm):\n",
        "    def __init__(self, dim, **kwargs):\n",
        "        super().__init__(dim, **kwargs)\n",
        "        \"\"\"\n",
        "        Initialize an instance of Modulation with dim and n = 2\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        self.modulation = Modulation(dim, 2)\n",
        "        ######## END TODO #######\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        1. Obtain (scale, shift) tensors from applying modulation on input y\n",
        "        2. Apply LayerNorm on input x, which we will denote as LayerNorm(x)\n",
        "        3. Compute AdaptiveLayerNorm as `LayerNorm(x) * (1 + scale) + shift`\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # Get scale and shift from modulation\n",
        "        scale, shift = self.modulation(y)\n",
        "\n",
        "        # Apply LayerNorm\n",
        "        normalized = super().forward(x)\n",
        "\n",
        "        # Apply adaptive scaling and shifting\n",
        "        return normalized * (1 + scale) + shift\n",
        "        ######## END TODO #######"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wDwvEVAnJdBU"
      },
      "outputs": [],
      "source": [
        "class DiTBlock(nn.Module):\n",
        "    def __init__(self, head_dim, num_heads, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        dim = head_dim * num_heads\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "\n",
        "        self.norm1 = AdaptiveLayerNorm(dim, elementwise_affine=False, eps=1e-6)\n",
        "        self.attn = Attention(head_dim, num_heads=num_heads, qkv_bias=True)\n",
        "        self.norm2 = AdaptiveLayerNorm(dim, elementwise_affine=False, eps=1e-6)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim, bias=True),\n",
        "            nn.GELU(approximate=\"tanh\"),\n",
        "            nn.Linear(mlp_hidden_dim, dim, bias=True),\n",
        "        )\n",
        "        self.scale_modulation = Modulation(dim, 2)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # (B, N, D), (B, D) -> (B, N, D)\n",
        "        # N = H * W / patch_size**2, D = num_heads * head_dim\n",
        "        gate_msa, gate_mlp = self.scale_modulation(y)\n",
        "        x = x + gate_msa * self.attn(self.norm1(x, y))\n",
        "        x = x + gate_mlp * self.mlp(self.norm2(x, y))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D90m_RQxM3IG"
      },
      "outputs": [],
      "source": [
        "def sigma_log_scale(batches, sigma, scaling_factor):\n",
        "    # Handle scalar sigma\n",
        "    if sigma.shape == torch.Size([]):\n",
        "        sigma = sigma.unsqueeze(0).repeat(batches)\n",
        "    # Handle 1D sigma with wrong batch size or need reshaping\n",
        "    elif len(sigma.shape) == 1:\n",
        "        if sigma.shape[0] != batches:\n",
        "            # If sigma has only 1 element, repeat it\n",
        "            if sigma.shape[0] == 1:\n",
        "                sigma = sigma.repeat(batches)\n",
        "            else:\n",
        "                raise ValueError(f'sigma shape {sigma.shape} does not match batch size {batches}')\n",
        "    else:\n",
        "        raise ValueError(f'sigma must be scalar or 1D, got shape {sigma.shape}')\n",
        "    return torch.log(sigma)*scaling_factor\n",
        "\n",
        "\n",
        "def get_sigma_embeds(batches, sigma, scaling_factor=0.5):\n",
        "    s = sigma_log_scale(batches, sigma, scaling_factor).unsqueeze(1)\n",
        "    return torch.cat([torch.sin(s), torch.cos(s)], dim=1)\n",
        "\n",
        "\n",
        "class SigmaEmbedderSinCos(nn.Module):\n",
        "    def __init__(self, hidden_size, scaling_factor=0.5):\n",
        "        super().__init__()\n",
        "        ######## BEGIN TODO ########\n",
        "        self.scaling_factor = scaling_factor\n",
        "        # MLP to embed the sigma values\n",
        "        # Input: 2 (sin and cos), Output: hidden_size\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(2, hidden_size),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_size, hidden_size)\n",
        "        )\n",
        "        ######## END TODO #######\n",
        "\n",
        "\n",
        "    def forward(self, batches, sigma):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        ######## BEGIN TODO ########\n",
        "        # Get sin/cos embeddings of sigma\n",
        "        sigma_embeds = get_sigma_embeds(batches, sigma, self.scaling_factor)\n",
        "\n",
        "        # Pass through MLP\n",
        "        return self.mlp(sigma_embeds)\n",
        "        ######## END TODO #######"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVHfC59UqOhg"
      },
      "source": [
        "### Helper Functions\n",
        "\n",
        "We provide you with the following helper functions for training DiT. No implementation needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kITU_H73MvIo"
      },
      "outputs": [],
      "source": [
        "def get_pos_embed(in_dim, patch_size, dim, N=10000):\n",
        "    n = in_dim // patch_size                                          # Number of patches per side\n",
        "    assert dim % 4 == 0, 'Embedding dimension must be multiple of 4!'\n",
        "    omega = 1 / N ** np.linspace(0, 1, dim // 4, endpoint=False)      # [dim/4]\n",
        "    freqs = np.outer(np.arange(n), omega)                             # [n, dim/4]\n",
        "    embeds = repeat(np.stack([np.sin(freqs), np.cos(freqs)]),\n",
        "                       ' b n d -> b n k d', k=n)                      # [2, n, n, dim/4]\n",
        "    embeds_2d = np.concatenate([\n",
        "        rearrange(embeds, 'b n k d -> (k n) (b d)'),                  # [n*n, dim/2]\n",
        "        rearrange(embeds, 'b n k d -> (n k) (b d)'),                  # [n*n, dim/2]\n",
        "    ], axis=1)                                                        # [n*n, dim]\n",
        "    return nn.Parameter(torch.tensor(embeds_2d).float().unsqueeze(0), # [1, n*n, dim]\n",
        "                        requires_grad=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pk5_8SFWNNpP"
      },
      "outputs": [],
      "source": [
        "class ModelMixin:\n",
        "    def rand_input(self, batchsize):\n",
        "        assert hasattr(self, 'input_dims'), 'Model must have \"input_dims\" attribute!'\n",
        "        return torch.randn((batchsize,) + self.input_dims)\n",
        "\n",
        "    # Currently predicts eps, override following methods to predict, for example, x0\n",
        "    def get_loss(self, x0, sigma, eps):\n",
        "        # Ensure sigma is properly shaped for broadcasting\n",
        "        # sigma should be (B,), reshape to (B, 1, 1, 1) for broadcasting with (B, C, H, W)\n",
        "        sigma_reshaped = sigma.view(-1, 1, 1, 1)\n",
        "        noisy_x = x0 + sigma_reshaped * eps\n",
        "        eps_pred = self(noisy_x, sigma)\n",
        "        return nn.MSELoss()(eps, eps_pred)\n",
        "\n",
        "    def predict_eps(self, x, sigma):\n",
        "        return self(x, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Mlu5v6qVkx"
      },
      "source": [
        "### Implement Entire DiT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "e_QETY8XJY4A"
      },
      "outputs": [],
      "source": [
        "class DiT(nn.Module, ModelMixin):\n",
        "    def __init__(self, in_dim, channels, patch_size, depth,\n",
        "                 head_dim, num_heads, mlp_ratio, sig_embed_factor,\n",
        "                 sig_embed_class=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_dim = in_dim\n",
        "        self.channels = channels\n",
        "        self.patch_size = patch_size\n",
        "        self.input_dims = (channels, in_dim, in_dim)\n",
        "\n",
        "        dim = head_dim * num_heads\n",
        "\n",
        "        self.pos_embed = get_pos_embed(in_dim, patch_size, dim)\n",
        "        self.sig_embed = (sig_embed_class or SigmaEmbedderSinCos)(\n",
        "            dim, scaling_factor=sig_embed_factor\n",
        "        )\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DiTBlock(head_dim, num_heads, mlp_ratio=mlp_ratio) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.final_norm = AdaptiveLayerNorm(dim, elementwise_affine=False, eps=1e-6)\n",
        "        self.final_linear = nn.Linear(dim, patch_size**2 * channels)\n",
        "        self.init()\n",
        "\n",
        "\n",
        "    def init(self):\n",
        "        # Initialize transformer layers\n",
        "        def _basic_init(module):\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "        self.apply(_basic_init)\n",
        "\n",
        "        # Initialize sigma embedding MLP\n",
        "        nn.init.normal_(self.sig_embed.mlp[0].weight, std=0.02)\n",
        "        nn.init.normal_(self.sig_embed.mlp[2].weight, std=0.02)\n",
        "\n",
        "        # Zero-out output layers\n",
        "        nn.init.constant_(self.final_linear.weight, 0)\n",
        "        nn.init.constant_(self.final_linear.bias, 0)\n",
        "\n",
        "    def forward(self, x, sigma):\n",
        "\n",
        "        ######## BEGIN TODO ########\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Patchify: (B, C, H, W) -> (B, C, n, patch_size, n, patch_size) -> (B, n*n, patch_size*patch_size*C)\n",
        "        # For patch_size=1: (B, C, H, W) -> (B, H*W, C)\n",
        "        x = rearrange(x, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
        "                      p1=self.patch_size, p2=self.patch_size)\n",
        "\n",
        "        # Add positional embedding: (B, N, D) + (1, N, D) -> (B, N, D)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Get sigma embedding: (B,) -> (B, D)\n",
        "        y = self.sig_embed(B, sigma)\n",
        "\n",
        "        # Pass through DiT blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, y)\n",
        "\n",
        "        # Apply final normalization\n",
        "        x = self.final_norm(x, y)\n",
        "\n",
        "        # Apply final linear projection: (B, N, D) -> (B, N, patch_size^2 * C)\n",
        "        x = self.final_linear(x)\n",
        "\n",
        "        # Unpatchify: (B, N, patch_size^2 * C) -> (B, C, H, W)\n",
        "        n = self.in_dim // self.patch_size\n",
        "        x = rearrange(x, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)',\n",
        "                      h=n, w=n, p1=self.patch_size, p2=self.patch_size, c=self.channels)\n",
        "\n",
        "        return x\n",
        "        ######## END TODO ########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "307IzvkcNbFZ"
      },
      "source": [
        "### Train DiT\n",
        "\n",
        "Here, we provide the pl LightningModule class for you. You only need to implement the `sample_image` function to generate new images and `training_step` function. You should use PatchVAE to generate latent encodings and train diffusion model on the latent encodings. In `sample_image` function, you should use DiT to generate latent encoding and use PatchVAE decoder to decode. The PatchVAE model should be fixed during training.\n",
        "\n",
        "Hint: use `get_loss` and `generate_train_sample` functions !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422,
          "referenced_widgets": [
            "5cec0995745c485c8b45647c1fd70e99",
            "48eb0f2ebbe846b7806831a584c48bed",
            "ca1adb19e3b8483892eedd3db261175d",
            "021e19977cd044c5acfbe4e846650d1b",
            "972e004a36354c54aee697d7cc130771",
            "4d289ae61f39403da6ace2bb8458da2f",
            "fb65bf8339014e3595a5363c4520a6a7",
            "d366730c239e4e2589d2a595a5428d48",
            "4b31bf03b40e43f88a1f1b1c101981e5",
            "c570216d46b44d4e99d03286818ee0be",
            "0fa736684dc34d3293e96d391dc847f2"
          ]
        },
        "id": "BwEBKBdA3C2d",
        "outputId": "e905d385-3df4-4ebb-ef00-fde7fcd5b550"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " VAE loaded and frozen: 26/26 params frozen\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name      | Type     | Params | Mode \n",
            "-----------------------------------------------\n",
            "0 | patch_vae | PatchVAE | 1.4 M  | eval \n",
            "1 | model     | DiT      | 1.8 M  | train\n",
            "-----------------------------------------------\n",
            "1.8 M     Trainable params\n",
            "1.4 M     Non-trainable params\n",
            "3.3 M     Total params\n",
            "13.107    Total estimated model params size (MB)\n",
            "145       Modules in train mode\n",
            "23        Modules in eval mode\n",
            "/Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
            "/Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 23 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 199: 100%|| 125/125 [00:05<00:00, 20.91it/s, v_num=2, train_loss=0.062] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 199: 100%|| 125/125 [00:06<00:00, 20.45it/s, v_num=2, train_loss=0.062]\n"
          ]
        }
      ],
      "source": [
        "class DiffusionModel(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # patch_size=8, img_channels=3, img_size=32, embed_dim=64, latent_dim=128\n",
        "\n",
        "    self.img_shape = 32\n",
        "    self.vae_patch_size = 16\n",
        "    self.vae_stride = 8\n",
        "    self.latent_dim = 128\n",
        "    self.head_dim = 16\n",
        "    self.num_heads = self.latent_dim // self.head_dim\n",
        "\n",
        "    # VAE Encoder for latent space\n",
        "    self.patch_vae = PatchVAE(patch_size=self.vae_patch_size, img_channels=3, stride=self.vae_stride,\n",
        "                                  img_size=self.img_shape, embed_dim=64, latent_dim=self.latent_dim)\n",
        "\n",
        "    self.in_dim = (self.img_shape - self.vae_patch_size) // self.vae_stride + 1\n",
        "\n",
        "    # for VAE latent\n",
        "    self.model = DiT(in_dim=self.in_dim, channels=self.latent_dim,\n",
        "                    patch_size=1, depth=6,\n",
        "                    head_dim=self.head_dim, num_heads=self.num_heads, mlp_ratio=4.0, sig_embed_factor=0.5)\n",
        "\n",
        "    self.schedule = DDPMScheduler(beta_start=0.0001, beta_end=0.02, N=1000)\n",
        "\n",
        "    self.train_loss = []\n",
        "\n",
        "  def on_training_epoch_start(self):\n",
        "    \"\"\" Fix patch vae \"\"\"\n",
        "    for param in self.patch_vae.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "  def generate_train_sample(self, x0: torch.FloatTensor):\n",
        "    \"\"\" Generate train samples\n",
        "\n",
        "    Args:\n",
        "      x0: torch.Tensor in shape ()\n",
        "    \"\"\"\n",
        "    sigma = self.schedule.sample_batch(x0)\n",
        "    while len(sigma.shape) < len(x0.shape):\n",
        "        sigma = sigma.unsqueeze(-1)\n",
        "    eps = torch.randn_like(x0)\n",
        "    return sigma, eps\n",
        "\n",
        "  # provide here\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    \"\"\" Training Step \"\"\"\n",
        "    # Unpack batch: (images, labels)\n",
        "    if isinstance(batch, (list, tuple)):\n",
        "        x0, _ = batch  # Ignore labels\n",
        "    else:\n",
        "        x0 = batch\n",
        "\n",
        "    ### YOUR CODE BEGINS ###\n",
        "    # Encode images to latent space using PatchVAE (no gradient)\n",
        "    with torch.no_grad():\n",
        "        # Get patches from images\n",
        "        patches = self.patch_vae.patch_embed(x0)\n",
        "\n",
        "        # Encode to get mu and logvar\n",
        "        mu, logvar = self.patch_vae.encode(patches)\n",
        "\n",
        "        # Use mu as the latent representation (deterministic encoding for training stability)\n",
        "        # Shape: (B, latent_dim, H_out, W_out)\n",
        "        latent_x0 = mu\n",
        "\n",
        "    # Generate training samples (add noise)\n",
        "    sigma, eps = self.generate_train_sample(latent_x0)\n",
        "\n",
        "    # Compute loss using the DiT model\n",
        "    # Squeeze sigma to (B,) shape by removing all singleton dimensions\n",
        "    sigma_1d = sigma.squeeze()\n",
        "    while len(sigma_1d.shape) > 1:\n",
        "        sigma_1d = sigma_1d.squeeze(-1)\n",
        "    loss = self.model.get_loss(latent_x0, sigma_1d, eps)\n",
        "    ## YOUR CODE ENDS ###\n",
        "\n",
        "    self.log('train_loss', loss, on_step=True, prog_bar=True)\n",
        "    self.train_loss.append(loss.item())\n",
        "    return loss\n",
        "\n",
        "  def sample_image(self, gam = 1.6, mu = 0., xt = None, batchsize = 4):\n",
        "    \"\"\" Function to generate image samples\n",
        "\n",
        "    Args:\n",
        "      gam: float, suggested to use gam >= 1\n",
        "      mu: float, requires mu in [0, 1)\n",
        "      xt: torch.Tensor, optional, default None\n",
        "      batchsize: int, optional, default 4\n",
        "    Return:\n",
        "      torch.Tensor in shape (batchsize, 1, 28, 28)\n",
        "    \"\"\"\n",
        "    sigmas = self.schedule.sample_sigmas(50).to(self.device)\n",
        "\n",
        "    if xt is None:\n",
        "        xt = self.model.rand_input(batchsize).to(self.device) * sigmas[0]\n",
        "    else:\n",
        "        batchsize = xt.shape[0]\n",
        "\n",
        "    ######### BEGIN TODO ########\n",
        "    # DDPM sampling loop\n",
        "    for i in range(len(sigmas) - 1):\n",
        "        sigma_curr = sigmas[i].to(self.device)\n",
        "        sigma_next = sigmas[i + 1].to(self.device)\n",
        "\n",
        "        # Predict epsilon (noise) using the DiT model\n",
        "        eps_pred = self.model.predict_eps(xt, sigma_curr)\n",
        "\n",
        "        # Denoise: x_{t-1} = (x_t - sigma_t * eps_pred) / sqrt(1 + sigma_t^2) * sqrt(1 + sigma_{t-1}^2) + sigma_{t-1} * eps_pred\n",
        "        # Simplified DDPM update with gamma and mu for better sampling\n",
        "\n",
        "        # Estimate x0 from xt\n",
        "        x0_pred = (xt - sigma_curr * eps_pred) / (1 + sigma_curr ** 2).sqrt()\n",
        "\n",
        "        # Update to next step with gamma scheduling\n",
        "        if sigma_next > 0:\n",
        "            # Add noise for stochastic sampling\n",
        "            z = torch.randn_like(xt)\n",
        "            xt = (1 - mu) * (x0_pred + sigma_next * eps_pred) + mu * xt + (gam * sigma_next - sigma_next) * z\n",
        "        else:\n",
        "            # Final step, no noise\n",
        "            xt = x0_pred\n",
        "\n",
        "    # xt is now the denoised latent representation\n",
        "    # Decode using PatchVAE decoder\n",
        "    with torch.no_grad():\n",
        "        # Decode latent to patches: (B, latent_dim, H, W) -> (B, num_patches, embed_dim)\n",
        "        patches = self.patch_vae.decode(xt)\n",
        "\n",
        "        # Reconstruct image from patches\n",
        "        recon_image = self.patch_vae.patch_embed.reconstruct(patches, self.img_shape)\n",
        "\n",
        "        # Denormalize: [-1, 1] -> [0, 1]\n",
        "        recon_image = (recon_image * 0.5 + 0.5).clamp(0, 1)\n",
        "    ######## END TODO ########\n",
        "\n",
        "    return recon_image\n",
        "\n",
        "  def on_train_epoch_end(self) -> None:\n",
        "    if self.current_epoch % 10 == 0:\n",
        "      batchsize = 4\n",
        "      imgs = self.sample_image(batchsize=batchsize)\n",
        "\n",
        "      fig, axes = plt.subplots(1, batchsize, figsize=(batchsize*4, 2))\n",
        "      for i in range(batchsize):\n",
        "        axes[i].imshow(imgs[i].permute(1, 2, 0).detach().cpu().numpy())\n",
        "        axes[i].axis('off')\n",
        "\n",
        "      # add title\n",
        "      fig.suptitle(f'Epoch {self.current_epoch}')\n",
        "      fig.savefig(f\"epoch_{self.current_epoch}.png\")\n",
        "      plt.close()\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.Adam(self.model.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "\n",
        "model = DiffusionModel()\n",
        "# load patch vae\n",
        "model.patch_vae.load_state_dict(torch.load(\"./best_vae_model.pth\"))\n",
        "\n",
        "# Freeze VAE parameters (critical!)\n",
        "for param in model.patch_vae.parameters():\n",
        "    param.requires_grad = False\n",
        "model.patch_vae.eval()\n",
        "print(f\" VAE loaded and frozen: {sum(1 for p in model.patch_vae.parameters() if not p.requires_grad)}/{sum(1 for p in model.patch_vae.parameters())} params frozen\")\n",
        "trainer = pl.Trainer(max_epochs=200, devices=1, accelerator=\"gpu\")\n",
        "trainer.fit(model, train_dataloader)\n",
        "torch.save(model.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DCIb3DO0MeCL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " No metrics.csv found in lightning_logs/\n",
            "Please check if PyTorch Lightning has created the logs directory.\n",
            "\n",
            " Found training loss in model.train_loss, plotting...\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total steps: 25000\n",
            "Initial loss: 1.0023\n",
            "Final loss: 0.0620\n",
            "Min loss: 0.0312\n"
          ]
        }
      ],
      "source": [
        "# Plot the train_logs from PyTorch Lightning CSV\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# metrics.csv\n",
        "csv_files = glob.glob('lightning_logs/*/metrics.csv')\n",
        "\n",
        "if csv_files:\n",
        "    # version\n",
        "    latest_csv = sorted(csv_files)[-1]\n",
        "    print(f\" Loading metrics from: {latest_csv}\")\n",
        "\n",
        "    # CSV\n",
        "    df = pd.read_csv(latest_csv)\n",
        "    print(f\" Loaded {len(df)} training steps\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # \n",
        "    print(f\"\\nTraining summary:\")\n",
        "    print(f\"  Total epochs: {df['epoch'].max() + 1}\")\n",
        "    print(f\"  Total steps: {df['step'].max() + 1}\")\n",
        "    if 'train_loss' in df.columns:\n",
        "        df_loss = df.dropna(subset=['train_loss'])\n",
        "        print(f\"  Initial loss: {df_loss['train_loss'].iloc[0]:.4f}\")\n",
        "        print(f\"  Final loss: {df_loss['train_loss'].iloc[-1]:.4f}\")\n",
        "        print(f\"  Min loss: {df_loss['train_loss'].min():.4f}\")\n",
        "        print(f\"  Mean loss: {df_loss['train_loss'].mean():.4f}\")\n",
        "\n",
        "    # \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot 1: Loss over steps (raw)\n",
        "    if 'train_loss' in df.columns:\n",
        "        df_loss = df.dropna(subset=['train_loss'])\n",
        "        axes[0].plot(df_loss['step'], df_loss['train_loss'], alpha=0.6, linewidth=0.8, color='steelblue')\n",
        "        axes[0].set_xlabel('Training Step', fontsize=12)\n",
        "        axes[0].set_ylabel('Loss', fontsize=12)\n",
        "        axes[0].set_title('Training Loss over Steps (Raw)', fontsize=14, fontweight='bold')\n",
        "        axes[0].grid(True, alpha=0.3, linestyle='--')\n",
        "        axes[0].set_ylim(bottom=0)\n",
        "\n",
        "    # Plot 2: Loss over epochs (smoothed by epoch mean)\n",
        "    if 'train_loss' in df.columns and 'epoch' in df.columns:\n",
        "        df_loss = df.dropna(subset=['train_loss'])\n",
        "        epoch_loss = df_loss.groupby('epoch')['train_loss'].agg(['mean', 'min', 'max']).reset_index()\n",
        "\n",
        "        axes[1].plot(epoch_loss['epoch'], epoch_loss['mean'],\n",
        "                    linewidth=2.5, color='darkblue', label='Mean loss', marker='o', markersize=4)\n",
        "        axes[1].fill_between(epoch_loss['epoch'],\n",
        "                            epoch_loss['min'],\n",
        "                            epoch_loss['max'],\n",
        "                            alpha=0.2, color='steelblue', label='Min-Max range')\n",
        "\n",
        "        axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "        axes[1].set_ylabel('Loss', fontsize=12)\n",
        "        axes[1].set_title('Training Loss over Epochs (Smoothed)', fontsize=14, fontweight='bold')\n",
        "        axes[1].legend(loc='upper right', fontsize=10)\n",
        "        axes[1].grid(True, alpha=0.3, linestyle='--')\n",
        "        axes[1].set_ylim(bottom=0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_loss_from_csv.png', dpi=150, bbox_inches='tight')\n",
        "    print(f\"\\n Plot saved to: training_loss_from_csv.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # \n",
        "    if len(csv_files) > 1:\n",
        "        print(f\"\\n Note: Found {len(csv_files)} training runs:\")\n",
        "        for csv_file in sorted(csv_files):\n",
        "            df_temp = pd.read_csv(csv_file)\n",
        "            print(f\"  - {csv_file}: {len(df_temp)} steps, {df_temp['epoch'].max()+1} epochs\")\n",
        "\n",
        "else:\n",
        "    print(\" No metrics.csv found in lightning_logs/\")\n",
        "    print(\"Please check if PyTorch Lightning has created the logs directory.\")\n",
        "\n",
        "    # model.train_lossmodel\n",
        "    if 'model' in dir() and hasattr(model, 'train_loss') and len(model.train_loss) > 0:\n",
        "        print(\"\\n Found training loss in model.train_loss, plotting...\")\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Plot 1: Raw loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(model.train_loss, alpha=0.6, linewidth=0.5, color='steelblue')\n",
        "        plt.title('Training Loss over Steps')\n",
        "        plt.xlabel('Training Step')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: Smoothed loss\n",
        "        plt.subplot(1, 2, 2)\n",
        "        window_size = min(100, len(model.train_loss) // 10)\n",
        "        if window_size > 1:\n",
        "            smoothed = np.convolve(model.train_loss, np.ones(window_size)/window_size, mode='valid')\n",
        "            plt.plot(smoothed, linewidth=2, color='darkblue')\n",
        "            plt.title(f'Smoothed Training Loss (window={window_size})')\n",
        "        else:\n",
        "            plt.plot(model.train_loss, linewidth=2, color='darkblue')\n",
        "            plt.title('Training Loss')\n",
        "        plt.xlabel('Training Step')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_loss_from_memory.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"\\nTotal steps: {len(model.train_loss)}\")\n",
        "        print(f\"Initial loss: {model.train_loss[0]:.4f}\")\n",
        "        print(f\"Final loss: {model.train_loss[-1]:.4f}\")\n",
        "        print(f\"Min loss: {np.min(model.train_loss):.4f}\")\n",
        "    else:\n",
        "        print(\"Also no training loss found in memory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M_CfUWhqi4v"
      },
      "source": [
        "### Sample and Evaluate DiT\n",
        "\n",
        "In addition to implementing the code above, please complete the following:\n",
        "1. Plot loss over epochs. Show that loss decreases over epochs.\n",
        "2. Generate new samples using your DiT checkpoint. Show that samples look reasonable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5Ea2InK4Q6gq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 251/251 [01:19<00:00,  3.17it/s]\n",
            "100%|| 251/251 [00:00<00:00, 823.09it/s]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Discard labels and only output data\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, dataset, fn):\n",
        "        self.dataset = dataset\n",
        "        self.fn = fn\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    def __getitem__(self, i):\n",
        "        return self.fn(self.dataset[i])\n",
        "\n",
        "# generate 2k images and score in the tensor\n",
        "total_image = 2000\n",
        "batch_size = 8\n",
        "iterations = 2000 // batch_size + 1\n",
        "model.load_state_dict(torch.load(\"model.pt\"))\n",
        "model.eval()\n",
        "# model.cuda()\n",
        "\n",
        "sampled_images = np.zeros((batch_size * iterations, 3, 32, 32), dtype=np.float32)\n",
        "for i in tqdm(range(iterations)):\n",
        "    imgs = model.sample_image(batchsize=batch_size)\n",
        "    sampled_images[i*batch_size:(i+1)*batch_size] = imgs.detach().cpu().numpy()\n",
        "sampled_images = sampled_images[:total_image]\n",
        "mydataset = MyDataset(sampled_images, lambda x: x)\n",
        "\n",
        "patch_vae_images = np.zeros((batch_size * iterations, 3, 32, 32), dtype=np.float32)\n",
        "for i in tqdm(range(iterations)):\n",
        "    imgs = 0.5 + 0.5 * model.patch_vae.sample(batch_size)\n",
        "    patch_vae_images[i*batch_size:(i+1)*batch_size] = imgs.detach().cpu().numpy()\n",
        "patch_vae_images = patch_vae_images[:total_image]\n",
        "patch_vae_dataset = MyDataset(patch_vae_images, lambda x: x)\n",
        "\n",
        "celebdataset = dataset = Subset(valid_dataloader.dataset, torch.arange(2000))  # Limit dataset size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBD3AYfx6IsL"
      },
      "source": [
        "3. Compute FID between images in CelebA dataset and:\n",
        "    i. Images generated from PatchVAE.\n",
        "    ii. Images generated from Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oEqrcY7J42_n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-fid in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (0.3.0)\n",
            "Requirement already satisfied: scipy in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (1.16.2)\n",
            "Requirement already satisfied: numpy in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (from pytorch-fid) (2.3.3)\n",
            "Requirement already satisfied: pillow in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (from pytorch-fid) (11.3.0)\n",
            "Requirement already satisfied: torch>=1.0.1 in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (from pytorch-fid) (2.8.0)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (from pytorch-fid) (0.23.0)\n",
            "Requirement already satisfied: filelock in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (from torch>=1.0.1->pytorch-fid) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (from torch>=1.0.1->pytorch-fid) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (from torch>=1.0.1->pytorch-fid) (70.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (from torch>=1.0.1->pytorch-fid) (1.13.3)\n",
            "Requirement already satisfied: networkx in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (from torch>=1.0.1->pytorch-fid) (3.3)\n",
            "Requirement already satisfied: jinja2 in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (from torch>=1.0.1->pytorch-fid) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (from torch>=1.0.1->pytorch-fid) (2024.6.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.0.1->pytorch-fid) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages (from jinja2->torch>=1.0.1->pytorch-fid) (2.1.5)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-fid scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l2poV-8fMeCL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "32it [04:26,  8.33s/it]\n",
            "32it [04:31,  8.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FID_score between CelebA and itself: -9.643839007367205e-07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "32it [04:24,  8.26s/it]\n",
            "32it [04:01,  7.55s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FID_score between CelebA and images sampled from Diffusion Model: 174.74941425011116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "32it [26:10, 49.07s/it] \n",
            "32it [10:12:20, 1148.14s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FID_score between CelebA and images sampled from PatchVAE: 150.61674617828692\n"
          ]
        }
      ],
      "source": [
        "# ! pip install pytorch-fid scipy\n",
        "from pytorch_fid.inception import InceptionV3\n",
        "import scipy.linalg as linalg\n",
        "\n",
        "def build_feature_table(dataset, model, batch_size, dim, device):\n",
        "    '''\n",
        "    Argms:\n",
        "    Input:\n",
        "        dataset: pytorch dataset, you want to evaluate IS score on\n",
        "        model: Inception network v3\n",
        "        batch_size: int number\n",
        "        dim: for IS computation, dim should be 1000 as the final softmax out put dimension\n",
        "        device: device type torch.device(\"cuda:0\") or torch.device(\"cpu\")\n",
        "    Output:\n",
        "        feature_table: (n,dim) numpy matrix\n",
        "    '''\n",
        "    # model enter eval mode\n",
        "    model.eval()\n",
        "    # initalize the dataloader\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
        "    n = len(dataset)\n",
        "    idx_counter = 0\n",
        "    # feature table\n",
        "    feature_table = np.zeros((n, dim))\n",
        "\n",
        "    for i, data in tqdm(enumerate(dataloader, 0)):\n",
        "        # Handle both single tensor and (image, label) tuple\n",
        "        if isinstance(data, (list, tuple)):\n",
        "            image = data[0].to(device)\n",
        "        else:\n",
        "            image = data.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(image)[0]\n",
        "            pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
        "            feature_table[idx_counter:idx_counter+pred.shape[0]] = pred\n",
        "            idx_counter += len(pred)\n",
        "\n",
        "    return feature_table\n",
        "\n",
        "def compute_stat(feature_table):\n",
        "    '''\n",
        "    Argms:\n",
        "    Input:\n",
        "        feature_table: (n,dim) numpy matrix\n",
        "    Output:\n",
        "        mu: mean along row dimension\n",
        "        sigma: covarance matrix of dataset\n",
        "    '''\n",
        "    # compute mean and sigma based on activation table\n",
        "    mu = np.mean(feature_table, axis=0)\n",
        "    sigma = np.cov(feature_table, rowvar=False)\n",
        "\n",
        "    return mu, sigma\n",
        "\n",
        "def compute_FID(mu_1, sigma_1, mu_2, sigma_2, eps=1e-6):\n",
        "    '''\n",
        "    Argms:\n",
        "    Input:\n",
        "        mu_1: mean vector we get for dataset1\n",
        "        sigma_1: covariance matrix for dataset1\n",
        "        mu_2: mean vector we get for dataset2\n",
        "        sigma_2: covariance matrix for dataset1\n",
        "    Output:\n",
        "        FID score: float\n",
        "    '''\n",
        "\n",
        "    # compute mu difference\n",
        "\n",
        "    # compute square root of Sigma1*Sigma2 using \"linalg.sqrtm\" from scipy\n",
        "    # please name the resulting matrix as covmean\n",
        "\n",
        "    ######### BEGIN TODO ########\n",
        "    # Compute mean difference squared\n",
        "    diff = mu_1 - mu_2\n",
        "\n",
        "    # Compute square root of Sigma1*Sigma2\n",
        "    from scipy import linalg\n",
        "    covmean = linalg.sqrtm(sigma_1.dot(sigma_2))\n",
        "    ######## END TODO ########\n",
        "\n",
        "\n",
        "    # The following block take care of imagionary part of covmean\n",
        "    #################################################################\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma_1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma_1 + offset).dot(sigma_2 + offset))\n",
        "\n",
        "    # Numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "    #################################################################\n",
        "\n",
        "    ######### BEGIN TODO ########\n",
        "    # Compute FID: ||mu_1 - mu_2||^2 + Tr(sigma_1 + sigma_2 - 2*covmean)\n",
        "    tr_covmean = np.trace(covmean)\n",
        "    FID = diff.dot(diff) + np.trace(sigma_1) + np.trace(sigma_2) - 2 * tr_covmean\n",
        "    ######## END TODO ########\n",
        "\n",
        "\n",
        "\n",
        "    return FID\n",
        "\n",
        "def FID(dataset_1, dataset_2, device, batch_size=64, dim=2048, block_idx = 3):\n",
        "    '''\n",
        "    Argms:\n",
        "    Input:\n",
        "        dataset_1: pytorch dataset\n",
        "        dataset_2: pytorch dataset\n",
        "        device: device type torch.device(\"cuda:0\") or torch.device(\"cpu\")\n",
        "        batch_size: int number\n",
        "        dim: for IS computation, dim should be 1000 as the final softmax out put dimension\n",
        "        block_idx: the block stage index we want to use in inception module\n",
        "    Output:\n",
        "        FID_score: float\n",
        "    '''\n",
        "    # load InveptionV3 model\n",
        "    model = InceptionV3([block_idx]).to(device)\n",
        "\n",
        "    ## build up the feature table\n",
        "    feature_table_1 = build_feature_table(dataset_1, model, batch_size, dim, device)\n",
        "    feature_table_2 = build_feature_table(dataset_2, model, batch_size, dim, device)\n",
        "\n",
        "\n",
        "    ## compute mu, sigma for dataset 1&2\n",
        "    mu_1, sigma_1 = compute_stat(feature_table_1)\n",
        "    mu_2, sigma_2 = compute_stat(feature_table_2)\n",
        "\n",
        "    ## FID score computation\n",
        "    FID_score = compute_FID(mu_1, sigma_1, mu_2, sigma_2, eps=1e-6)\n",
        "\n",
        "    return FID_score\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "FID_score = FID(celebdataset, celebdataset, device)\n",
        "print('FID_score between CelebA and itself:', FID_score)\n",
        "\n",
        "FID_score = FID(celebdataset, mydataset, device)\n",
        "print('FID_score between CelebA and images sampled from Diffusion Model:', FID_score)\n",
        "\n",
        "FID_score = FID(celebdataset, patch_vae_dataset, device)\n",
        "print('FID_score between CelebA and images sampled from PatchVAE:', FID_score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jj1sSnbJpMpj",
        "5iLpnEI6H1Cd",
        "AxxFfD3rc2MG"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "021e19977cd044c5acfbe4e846650d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c570216d46b44d4e99d03286818ee0be",
            "placeholder": "",
            "style": "IPY_MODEL_0fa736684dc34d3293e96d391dc847f2",
            "value": "20/125[00:01&lt;00:08,12.61it/s,v_num=0,train_loss=0.546]"
          }
        },
        "0fa736684dc34d3293e96d391dc847f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48eb0f2ebbe846b7806831a584c48bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d289ae61f39403da6ace2bb8458da2f",
            "placeholder": "",
            "style": "IPY_MODEL_fb65bf8339014e3595a5363c4520a6a7",
            "value": "Epoch3:16%"
          }
        },
        "4b31bf03b40e43f88a1f1b1c101981e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d289ae61f39403da6ace2bb8458da2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cec0995745c485c8b45647c1fd70e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48eb0f2ebbe846b7806831a584c48bed",
              "IPY_MODEL_ca1adb19e3b8483892eedd3db261175d",
              "IPY_MODEL_021e19977cd044c5acfbe4e846650d1b"
            ],
            "layout": "IPY_MODEL_972e004a36354c54aee697d7cc130771"
          }
        },
        "972e004a36354c54aee697d7cc130771": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "c570216d46b44d4e99d03286818ee0be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca1adb19e3b8483892eedd3db261175d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d366730c239e4e2589d2a595a5428d48",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b31bf03b40e43f88a1f1b1c101981e5",
            "value": 1
          }
        },
        "d366730c239e4e2589d2a595a5428d48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb65bf8339014e3595a5363c4520a6a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
