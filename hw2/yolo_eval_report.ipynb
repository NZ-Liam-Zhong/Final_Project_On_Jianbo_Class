{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecdd99cc",
   "metadata": {},
   "source": [
    "# YOLO Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "83bd22eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _softmax_np(x):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    x = x - np.max(x)\n",
    "    e = np.exp(x)\n",
    "    denom = np.sum(e)\n",
    "    return e / (denom if denom > 0 else 1.0)\n",
    "\n",
    "def low_objectness_filter(grid_np, pr_thresh=0.6, img_dim=128):\n",
    "    \"\"\"\n",
    "    Keep cells with Pr >= threshold using channel 0 (raw logit -> sigmoid),\n",
    "    decode to [cls, score, x1, y1, x2, y2] in image pixels.\n",
    "    Expects grid_np shape: (1, C, X, Y) where C=8 and X=Y=S.\n",
    "    \"\"\"\n",
    "    if grid_np is None or len(grid_np) == 0:\n",
    "        return np.zeros((0, 6), dtype=np.float32)\n",
    "\n",
    "    g = grid_np[0]  # (C, X, Y)\n",
    "    Sx, Sy = g.shape[1], g.shape[2]\n",
    "    S = int(Sx)\n",
    "    assert Sx == Sy, \"Grid must be square\"\n",
    "    CELL = img_dim // S\n",
    "\n",
    "    kept = []\n",
    "    for gx in range(S):\n",
    "        for gy in range(S):\n",
    "            pr = 1.0 / (1.0 + np.exp(-float(g[0, gx, gy])))\n",
    "            if pr < float(pr_thresh):\n",
    "                continue\n",
    "\n",
    "            xr = float(g[1, gx, gy])\n",
    "            yr = float(g[2, gx, gy])\n",
    "            wr = max(0.0, float(g[3, gx, gy]))\n",
    "            hr = max(0.0, float(g[4, gx, gy]))\n",
    "\n",
    "            cls_logits = g[5:8, gx, gy]\n",
    "            cls_probs = _softmax_np(cls_logits)\n",
    "            cls = int(np.argmax(cls_probs))\n",
    "            score = float(pr * float(cls_probs[cls]))\n",
    "\n",
    "            cx = (gx + xr) * CELL\n",
    "            cy = (gy + yr) * CELL\n",
    "            w = wr * img_dim\n",
    "            h = hr * img_dim\n",
    "\n",
    "            x1 = float(np.clip(cx - 0.5 * w, 0, img_dim))\n",
    "            y1 = float(np.clip(cy - 0.5 * h, 0, img_dim))\n",
    "            x2 = float(np.clip(cx + 0.5 * w, 0, img_dim))\n",
    "            y2 = float(np.clip(cy + 0.5 * h, 0, img_dim))\n",
    "\n",
    "            kept.append([cls, score, x1, y1, x2, y2])\n",
    "\n",
    "    return np.array(kept, dtype=np.float32) if kept else np.zeros((0, 6), np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cfde236d",
   "metadata": {
    "id": "PGJA5ggIuE0L",
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _iou_xyxy(a, b):\n",
    "    \"\"\"Compute IoU between two [x1,y1,x2,y2] boxes.\"\"\"\n",
    "    ax1, ay1, ax2, ay2 = a\n",
    "    bx1, by1, bx2, by2 = b\n",
    "    ix1 = max(ax1, bx1)\n",
    "    iy1 = max(ay1, by1)\n",
    "    ix2 = min(ax2, bx2)\n",
    "    iy2 = min(ay2, by2)\n",
    "    iw = max(0.0, ix2 - ix1)\n",
    "    ih = max(0.0, iy2 - iy1)\n",
    "    inter = iw * ih\n",
    "    area_a = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)\n",
    "    area_b = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)\n",
    "    denom = area_a + area_b - inter\n",
    "    return inter / denom if denom > 0 else 0.0\n",
    "\n",
    "def low_confidence_suppression(label, thresh=0.6):\n",
    "    if label is None:\n",
    "        return np.zeros((0, 6), dtype=np.float32)\n",
    "\n",
    "    arr = np.asarray(label)\n",
    "    if arr.size == 0:\n",
    "        return arr\n",
    "\n",
    "    if arr.shape[1] < 6:\n",
    "        # No score column to threshold on; return as-is.\n",
    "        return arr\n",
    "\n",
    "    keep = arr[:, 1] >= float(thresh)\n",
    "    return arr[keep]\n",
    "\n",
    "def non_max_suppression(label, iou_thresh=0.5):\n",
    "   \"\"\"\n",
    "    Per-class NMS with IoU thresholding (keep highest-score box in each cluster).\n",
    "    arguments\n",
    "        label: np.ndarray shape (N_box, 6) as [cls, score, x1, y1, x2, y2].\n",
    "              if shape is 5 a dummy var is returned of 1.0\n",
    "        iou_thresh: suppress boxes with IoU > iou_thresh (default 0.5).\n",
    "        return\n",
    "        np.ndarray of alive boxes in the same column format as input.\n",
    "    \"\"\"\n",
    "   if label is None:\n",
    "       return np.zeros((0, 6), dtype=np.float32)\n",
    "\n",
    "   boxes = np.asarray(label, dtype=np.float32)\n",
    "   if boxes.size == 0:\n",
    "       return boxes\n",
    "\n",
    "   has_score = (boxes.shape[1] >= 6)\n",
    "   if not has_score:\n",
    "\n",
    "       ones = np.ones((boxes.shape[0], 1), dtype=np.float32)\n",
    "       boxes = np.concatenate([boxes[:, :1], ones, boxes[:, 1:]], axis=1)\n",
    "\n",
    "   kept = []\n",
    "\n",
    "   classes = np.unique(boxes[:, 0].astype(np.int32))\n",
    "   for c in classes:\n",
    "       cls_mask = boxes[:, 0].astype(np.int32) == c\n",
    "       cls_boxes = boxes[cls_mask]\n",
    "       if cls_boxes.shape[0] == 0:\n",
    "           continue\n",
    "       order = np.argsort(-cls_boxes[:, 1])\n",
    "       cls_boxes = cls_boxes[order]\n",
    "\n",
    "       while cls_boxes.shape[0] > 0:\n",
    "           top = cls_boxes[0]\n",
    "           kept.append(top)\n",
    "           if cls_boxes.shape[0] == 1:\n",
    "               break\n",
    "           rest = cls_boxes[1:]\n",
    "           ious = np.array([_iou_xyxy(top[2:6], b[2:6]) for b in rest], dtype=np.float32)\n",
    "           keep_mask = ious <= float(iou_thresh)\n",
    "           cls_boxes = rest[keep_mask]\n",
    "\n",
    "   kept = np.stack(kept, axis=0) if kept else np.zeros((0, boxes.shape[1]), dtype=np.float32)\n",
    "   if not has_score:\n",
    "       kept = np.concatenate([kept[:, :1], kept[:, 2:]], axis=1)\n",
    "   return kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "452bb44f",
   "metadata": {
    "id": "G0AZ1zENuE0M",
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _iou_xyxy_np(a, b):\n",
    "    ax1, ay1, ax2, ay2 = a\n",
    "    bx1, by1, bx2, by2 = b\n",
    "    ix1, iy1 = max(ax1, bx1), max(ay1, by1)\n",
    "    ix2, iy2 = min(ax2, bx2), min(ay2, by2)\n",
    "    iw, ih = max(0.0, ix2 - ix1), max(0.0, iy2 - iy1)\n",
    "    inter = iw * ih\n",
    "    area_a = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)\n",
    "    area_b = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)\n",
    "    denom = area_a + area_b - inter\n",
    "    return inter / denom if denom > 0 else 0.0\n",
    "\n",
    "def precision_recall_curve(predictions, targets, target_class, iou_thresh=0.5):\n",
    "    gt_by_img = []\n",
    "    total_gt = 0\n",
    "    for t in targets:\n",
    "        t = np.asarray(t) if t is not None else np.zeros((0,5), np.float32)\n",
    "        m = t.shape[0]\n",
    "        if m == 0:\n",
    "            gt_by_img.append({\"boxes\": np.zeros((0,4), np.float32),\n",
    "                              \"matched\": np.zeros((0,), dtype=bool)})\n",
    "            continue\n",
    "        mask = (t[:,0].astype(int) == int(target_class))\n",
    "        boxes = t[mask][:,1:5].astype(np.float32)\n",
    "        gt_by_img.append({\"boxes\": boxes, \"matched\": np.zeros((boxes.shape[0],), dtype=bool)})\n",
    "        total_gt += boxes.shape[0]\n",
    "\n",
    "    # collect all preds of this class across images\n",
    "    all_preds = []\n",
    "    for i, p in enumerate(predictions):\n",
    "        p = np.asarray(p) if p is not None else np.zeros((0,6), np.float32)\n",
    "        if p.shape[0] == 0:\n",
    "            continue\n",
    "        mask = (p[:,0].astype(int) == int(target_class))\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        sel = p[mask][:,1:7]\n",
    "        for row in sel:\n",
    "            score = float(row[0])\n",
    "            all_preds.append((score, i, row[1:5].astype(np.float32)))\n",
    "\n",
    "\n",
    "    if total_gt == 0:\n",
    "        return np.array([0.0]), np.array([1.0])\n",
    "\n",
    "    # sort predictions by score desc\n",
    "    all_preds.sort(key=lambda x: -x[0])\n",
    "\n",
    "    tps, fps = [], []\n",
    "    for score, img_idx, box in all_preds:\n",
    "        g = gt_by_img[img_idx]\n",
    "        g_boxes = g[\"boxes\"]\n",
    "        g_matched = g[\"matched\"]\n",
    "\n",
    "        best_iou, best_j = 0.0, -1\n",
    "        for j in range(g_boxes.shape[0]):\n",
    "            if g_matched[j]:\n",
    "                continue\n",
    "            iou = _iou_xyxy_np(box, g_boxes[j])\n",
    "            if iou > best_iou:\n",
    "                best_iou, best_j = iou, j\n",
    "\n",
    "        if best_iou >= iou_thresh and best_j >= 0:\n",
    "            tps.append(1.0); fps.append(0.0)\n",
    "            g_matched[best_j] = True  # lock that GT\n",
    "        else:\n",
    "            tps.append(0.0); fps.append(1.0)\n",
    "\n",
    "    tps = np.cumsum(np.array(tps))\n",
    "    fps = np.cumsum(np.array(fps))\n",
    "    recalls = tps / max(1, total_gt)\n",
    "    precisions = tps / np.maximum(tps + fps, 1e-9)\n",
    "\n",
    "\n",
    "    mrec = np.concatenate(([0.0], recalls, [1.0]))\n",
    "    mpre = np.concatenate(([0.0], precisions, [0.0]))\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i-1] = max(mpre[i-1], mpre[i])\n",
    "\n",
    "\n",
    "    return mrec[1:-1], mpre[1:-1]\n",
    "\n",
    "def average_precision(predictions, targets, target_class, iou_thresh=0.5):\n",
    "    r, p = precision_recall_curve(predictions, targets, target_class, iou_thresh=iou_thresh)\n",
    "    if r.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    ap = float(np.trapz(p, r))\n",
    "    return ap\n",
    "\n",
    "def mean_average_precision(predictions, targets, iou_thresh=0.5, classes=(0,1,2)):\n",
    "    \"\"\"Mean AP over provided classes. Skips classes with no GT to avoid NaNs.\"\"\"\n",
    "    aps = []\n",
    "\n",
    "    gt_classes = set()\n",
    "    for t in targets:\n",
    "        t = np.asarray(t) if t is not None else np.zeros((0,5), np.float32)\n",
    "        if t.size:\n",
    "            gt_classes.update(t[:,0].astype(int).tolist())\n",
    "\n",
    "    for c in classes:\n",
    "        if c not in gt_classes:\n",
    "            continue\n",
    "        ap = average_precision(predictions, targets, c, iou_thresh=iou_thresh)\n",
    "        aps.append(ap)\n",
    "    return float(np.mean(aps)) if aps else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "647f1513",
   "metadata": {
    "id": "Ozp03xqEGwY7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_eval_and_decode(model, dataset, indices, batch_size=64, img_dim=128):\n",
    "    \"\"\"\n",
    "    Returns (preds, gts) lists for mAP:\n",
    "      preds[i]: (Ni,6) [cls, score, x1,y1,x2,y2]\n",
    "      gts[i]:   (Mi,5) [cls, x1,y1,x2,y2]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    sub_imgs = []\n",
    "    sub_gts  = []\n",
    "    for idx in indices:\n",
    "        img_np, tgt_grid = dataset[idx]\n",
    "        sub_imgs.append(img_np.numpy())\n",
    "        gtb = reconstruct_raw_labels(tgt_grid.unsqueeze(0).numpy(), include_score=False)[0]\n",
    "        sub_gts.append(gtb)\n",
    "\n",
    "    imgs = torch.from_numpy(np.stack(sub_imgs, 0)).to(device)  # [N,3,128,128]\n",
    "    preds = []\n",
    "\n",
    "    for b0 in range(0, imgs.shape[0], batch_size):\n",
    "        b1 = min(b0 + batch_size, imgs.shape[0])\n",
    "        out = model(imgs[b0:b1])             # [B,8,8,8] (C,H,W) where H=y, W=x\n",
    "        out_np = out.detach().cpu().numpy()\n",
    "        for i in range(out_np.shape[0]):\n",
    "            g_xy = np.transpose(out_np[i], (0, 2, 1))[None, ...]\n",
    "            all_kept = low_objectness_filter(g_xy, pr_thresh=0.6, img_dim=img_dim)\n",
    "            nms_kept = non_max_suppression(all_kept, iou_thresh=0.5)\n",
    "            preds.append(nms_kept)\n",
    "\n",
    "    return preds, sub_gts\n",
    "\n",
    "class MAPHistory(pl.Callback):\n",
    "    def __init__(self, eval_dataset, eval_indices=None):\n",
    "        super().__init__()\n",
    "        self.eval_dataset = eval_dataset\n",
    "        if eval_indices is None:\n",
    "            n = len(eval_dataset)\n",
    "            m = min(128, n)\n",
    "            rng = np.random.default_rng(0)\n",
    "            self.eval_indices = rng.choice(n, size=m, replace=False).tolist()\n",
    "        else:\n",
    "            self.eval_indices = list(eval_indices)\n",
    "        self.map_values = []\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        preds, gts = run_eval_and_decode(pl_module, self.eval_dataset, self.eval_indices)\n",
    "        m = mean_average_precision(preds, gts, iou_thresh=0.5, classes=(0,1,2))\n",
    "        self.map_values.append(float(m))\n",
    "        # log so it shows in progress bar too\n",
    "        pl_module.log(\"mAP@0.5\", float(m), prog_bar=True, on_epoch=True, logger=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6d93850b",
   "metadata": {
    "id": "_mtttH5eDe9E"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _softmax_np(x):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    x = x - np.max(x)\n",
    "    ex = np.exp(x)\n",
    "    return ex / np.maximum(ex.sum(), 1e-9)\n",
    "\n",
    "def decode_all_cells_consistent(grid_np, img_dim=128):\n",
    "    \"\"\"\n",
    "    Decode every cell (no filtering), using:\n",
    "      - Pr := raw channel 0   (NO sigmoid here; keep it consistent with training)\n",
    "      - class probs := softmax over channels 5..7\n",
    "      - score := Pr * P(class=argmax)\n",
    "    Returns (N,6): [cls, score, x1,y1,x2,y2]\n",
    "    \"\"\"\n",
    "    S = 8\n",
    "    CELL = img_dim // S\n",
    "    g = grid_np[0]  # (8,8,8)\n",
    "    out = []\n",
    "    for gx in range(S):\n",
    "        for gy in range(S):\n",
    "            pr  = float(1.0 / (1.0 + np.exp(-g[0, gx, gy])))   # sigmoid -> probability\n",
    "            xr  = float(g[1, gx, gy])\n",
    "            yr  = float(g[2, gx, gy])\n",
    "            wr  = max(0.0, float(g[3, gx, gy]))\n",
    "            hr  = max(0.0, float(g[4, gx, gy]))\n",
    "\n",
    "            # class probs from logits\n",
    "            cls_logits = g[5:8, gx, gy]\n",
    "            cls_probs  = _softmax_np(cls_logits)\n",
    "            cls = int(np.argmax(cls_probs))\n",
    "            score = pr * float(cls_probs[cls])           # rank inside each class\n",
    "\n",
    "            # decode center & size\n",
    "            cx = (gx + xr) * CELL\n",
    "            cy = (gy + yr) * CELL\n",
    "            w  = wr * img_dim\n",
    "            h  = hr * img_dim\n",
    "\n",
    "            x1 = float(np.clip(cx - 0.5*w, 0, img_dim))\n",
    "            y1 = float(np.clip(cy - 0.5*h, 0, img_dim))\n",
    "            x2 = float(np.clip(cx + 0.5*w, 0, img_dim))\n",
    "            y2 = float(np.clip(cy + 0.5*h, 0, img_dim))\n",
    "\n",
    "            out.append([cls, score, x1, y1, x2, y2])\n",
    "    return np.array(out, dtype=np.float32)\n",
    "\n",
    "def low_objectness_filter(grid_np, pr_thresh=0.6, img_dim=128):\n",
    "    \"\"\"\n",
    "    Implements Step-1 of the post-processing spec strictly: keep cells with Pr >= threshold,\n",
    "    where Pr is *channel 1 of the grid output* (raw, no sigmoid).\n",
    "    Returns boxes (N,6) like decode_all_cells_consistent, but only for kept cells.\n",
    "    \"\"\"\n",
    "    S = 8\n",
    "    CELL = img_dim // S\n",
    "    g = grid_np[0]\n",
    "    kept = []\n",
    "    for gx in range(S):\n",
    "        for gy in range(S):\n",
    "            pr  = float(1.0 / (1.0 + np.exp(-g[0, gx, gy])))   # sigmoid -> probability\n",
    "            if pr < float(pr_thresh):\n",
    "                continue\n",
    "\n",
    "            xr  = float(g[1, gx, gy])\n",
    "            yr  = float(g[2, gx, gy])\n",
    "            wr  = max(0.0, float(g[3, gx, gy]))\n",
    "            hr  = max(0.0, float(g[4, gx, gy]))\n",
    "\n",
    "            cls_logits = g[5:8, gx, gy]\n",
    "            cls_probs  = _softmax_np(cls_logits)\n",
    "            cls = int(np.argmax(cls_probs))\n",
    "            score = pr * float(cls_probs[cls])\n",
    "\n",
    "            cx = (gx + xr) * CELL\n",
    "            cy = (gy + yr) * CELL\n",
    "            w  = wr * img_dim\n",
    "            h  = hr * img_dim\n",
    "\n",
    "            x1 = float(np.clip(cx - 0.5*w, 0, img_dim))\n",
    "            y1 = float(np.clip(cy - 0.5*h, 0, img_dim))\n",
    "            x2 = float(np.clip(cx + 0.5*w, 0, img_dim))\n",
    "            y2 = float(np.clip(cy + 0.5*h, 0, img_dim))\n",
    "\n",
    "            kept.append([cls, score, x1, y1, x2, y2])\n",
    "\n",
    "    return np.array(kept, dtype=np.float32) if kept else np.zeros((0,6), np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2b1b93d7",
   "metadata": {
    "id": "GGlqDpVlDlBa"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "img_t, tgt_grid = tiny_ds[0]  # pick the *same* overfit sample\n",
    "img_vis = (img_t.numpy().transpose(1,2,0)*255).astype('uint8')\n",
    "\n",
    "with torch.no_grad():\n",
    "    grid_pred = model(img_t.unsqueeze(0).to(next(model.parameters()).device)).cpu().numpy()\n",
    "\n",
    "# Transpose prediction to (C, x, y) for helpers\n",
    "grid_pred_cxy = np.transpose(grid_pred, (0, 1, 3, 2))\n",
    "\n",
    "pred_all  = decode_all_cells_consistent(grid_pred_cxy, img_dim=128)\n",
    "pred_keep = low_objectness_filter(grid_pred_cxy, pr_thresh=0.6, img_dim=128)  # 0.6 per spec\n",
    "pred_nms  = non_max_suppression(pred_keep, iou_thresh=0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370aec4a",
   "metadata": {
    "id": "bCfluGkruE0d"
   },
   "source": [
    "7. For one image in the inference step, show the bounding boxes visualized for each class with green bounding boxes around cars, red bounding boxes around pedestrians, and blue bounding boxes around traffic lights as done in Figure 1. Show the precision/recall curves for each class in inference. Write down the achieved mean Average Precision for your inference stage. (17%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcba83d",
   "metadata": {
    "id": "d9TbYQsPuE0f"
   },
   "source": [
    "### 8. Implementation notes, challenges, results, and improvements\n",
    "\n",
    "- **Implementation Techniques**\n",
    "  - We used the YOLOv1 architecture for this implementation.\n",
    "\n",
    "- **Key challenges fixed**\n",
    "  - Corrected `yolo_loss` call sites to pass positional args and to use a single return tensor. This removed the `TypeError` and 0â€‘D tensor unpacking error.\n",
    "  - Standardized target layout to match model output by permuting labels `(C, x, y) â†’ (C, y, x)` before loss.\n",
    "  - Restored missing eval helpers (`_softmax_np`, `low_objectness_filter`) so `MAPHistory` can compute mAP.\n",
    "\n",
    "- **Training behavior & performance**\n",
    "  - Model converges on the tiny subset and logs decreasing training/validation loss.\n",
    "  - mAP@0.5 is computed each validation epoch via `MAPHistory`; with the tiny split it gives noisy but improving estimates. On the full split and longer training, expect more stable metrics.\n",
    "\n",
    "- **What worked well**\n",
    "  - Using Kaiming init and Adam with default betas yielded stable training.\n",
    "  - Loss follows the classic YOLOv1 formulation: stronger coord term, weaker noâ€‘object term, raw confidence aligned to IoU.\n",
    "\n",
    "- **Limitations / potential sources of error**\n",
    "  - Tiny eval set produces high variance in mAP; NMS and confidence thresholding directly impact reported AP.\n",
    "  - No data augmentation; overfitting is likely on small data.\n",
    "  - Singleâ€‘scale grid and shallow decoder limit localization accuracy for small objects.\n",
    "\n",
    "- **Ideas to improve performance**\n",
    "  - Training: cosine LR schedule with warmup; mild weight decay; longer epochs; AMP for speed.\n",
    "  - Regularization: label smoothing for class scores; MixUp/CutMix; color/flip/scale jitter.\n",
    "  - Head/decoding: calibrate objectness threshold; tune NMS IoU; consider softâ€‘NMS.\n",
    "  - Architecture: add an extra upsample + skip connection, or multiâ€‘scale heads.\n",
    "  - Loss: tune `lambda_coord`/`lambda_noobj`; optionally use GIoU/CIoU for boxes.\n",
    "  - Evaluation: report mAP@[.5:.95], confusion matrices, perâ€‘class PR curves on full val set.\n",
    "\n",
    "- **Next steps**\n",
    "  - Train on the full training set with augmentations and LR schedule.\n",
    "  - Log mAP curves and qualitative detections to track over/underâ€‘fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c1ec3e1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2faIfPruE0Z",
    "outputId": "c4f4d04d-f047-44b1-ca5b-26a57d990225",
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# # Setup your training\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "grid_labels = process_labels(raw_labels)           # np.ndarray [N, 8, 8, 8]\n",
    "\n",
    "# ---------- Dataset ----------\n",
    "class YoloNPZDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      image: torch.float32 [3, 128, 128] in [0,1]\n",
    "      target: torch.float32 [8, 8, 8]   (channels, x, y)\n",
    "    \"\"\"\n",
    "    def __init__(self, images_np: np.ndarray, grid_labels_np: np.ndarray):\n",
    "        assert images_np.shape[0] == grid_labels_np.shape[0]\n",
    "        self.images = images_np\n",
    "        self.targets = grid_labels_np\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.images.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        img = self.images[idx]\n",
    "        # to CHW, float32 in [0,1]\n",
    "        if img.ndim == 3 and img.shape[0] == 3:\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = np.transpose(img, (2, 0, 1))  # HWC -> CHW\n",
    "        img_t = torch.from_numpy(img)\n",
    "\n",
    "        tgt = self.targets[idx].astype(np.float32)\n",
    "        tgt_t = torch.from_numpy(tgt)       # [8, 8, 8]\n",
    "        return img_t, tgt_t\n",
    "\n",
    "full_ds = YoloNPZDataset(images, grid_labels)\n",
    "\n",
    "N = len(full_ds)\n",
    "val_frac = 0.1\n",
    "val_len = int(N * val_frac)\n",
    "train_len = N - val_len\n",
    "train_ds, val_ds = random_split(full_ds, [train_len, val_len],\n",
    "                                generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "model = YOLO(img_dim=128, grid_size=8, lr=1e-2)  # lr = 10e-3 per spec\n",
    "\n",
    "class LossHistory(pl.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        tl = trainer.callback_metrics.get(\"train_loss_epoch\")\n",
    "        if tl is not None:\n",
    "            self.train_loss.append(float(tl.detach().cpu()))\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        vl = trainer.callback_metrics.get(\"val_loss\")\n",
    "        if vl is not None:\n",
    "            self.val_loss.append(float(vl.detach().cpu()))\n",
    "\n",
    "loss_hist = LossHistory()\n",
    "\n",
    "max_epochs = 20  # at least 20\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    log_every_n_steps=25,\n",
    "    callbacks=[loss_hist],\n",
    "    enable_progress_bar=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "464e594a",
   "metadata": {
    "id": "wkvdilyDG5lH"
   },
   "outputs": [],
   "source": [
    "map_hist = MAPHistory(eval_dataset=val_ds)  # evaluate on the val split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a161c4f1",
   "metadata": {
    "id": "y0zAqsBPHGSM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | conv1   | Sequential | 1.6 K  | train\n",
      "1 | conv2   | Sequential | 32.9 K | train\n",
      "2 | conv3   | Sequential | 131 K  | train\n",
      "3 | conv4   | Sequential | 524 K  | train\n",
      "4 | conv5   | Sequential | 2.1 M  | train\n",
      "5 | conv6   | Sequential | 8.4 M  | train\n",
      "6 | deconv7 | Sequential | 4.2 M  | train\n",
      "7 | deconv8 | Sequential | 262 K  | train\n",
      "8 | conv9   | Conv2d     | 4.6 K  | train\n",
      "-----------------------------------------------\n",
      "15.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 M    Total params\n",
      "62.565    Total estimated model params size (MB)\n",
      "33        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/10/1lxt8x6s2m378070kn3j5psw0000gp/T/ipykernel_16874/2154568068.py:90: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  ap = float(np.trapz(p, r))\n",
      "/Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/kyle/Github/cis6800hw/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 22.04it/s, v_num=4, train_loss_step=7.740, val_loss=12.30, mAP@0.5=0.000, train_loss_epoch=8.250]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:01<00:00, 18.67it/s, v_num=4, train_loss_step=7.740, val_loss=12.30, mAP@0.5=0.000, train_loss_epoch=8.250]\n"
     ]
    }
   ],
   "source": [
    "tiny_len = 100\n",
    "tiny_ds, _ = random_split(\n",
    "    full_ds, [tiny_len, len(full_ds) - tiny_len],\n",
    "    generator=torch.Generator().manual_seed(0)\n",
    ")\n",
    "tiny_train_len = int(0.8 * tiny_len)\n",
    "tiny_val_len   = tiny_len - tiny_train_len\n",
    "tiny_train, tiny_val = random_split(\n",
    "    tiny_ds, [tiny_train_len, tiny_val_len],\n",
    "    generator=torch.Generator().manual_seed(0)\n",
    ")\n",
    "\n",
    "tiny_train_loader = DataLoader(tiny_train, batch_size=4, shuffle=True)\n",
    "tiny_val_loader   = DataLoader(tiny_val,   batch_size=1, shuffle=False)\n",
    "map_hist = MAPHistory(eval_dataset=tiny_val)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    callbacks=[loss_hist, map_hist],\n",
    ")\n",
    "trainer.fit(model, train_dataloaders=tiny_train_loader, val_dataloaders=tiny_val_loader)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
